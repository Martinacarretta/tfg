{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829e2cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from gymnasium import spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d16832",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glioblastoma(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 4} \n",
    "    # The metadata of the environment, e.g. {“render_modes”: [“rgb_array”, “human”], “render_fps”: 30}. \n",
    "    # For Jax or Torch, this can be indicated to users with “jax”=True or “torch”=True.\n",
    "\n",
    "    def __init__(self, image_path, mask_path, grid_size=4, tumor_threshold=0.0001, rewards = [1.0, -2.0, -0.5], action_space=spaces.Discrete(3), render_mode=\"human\"): # cosntructor with the brain image, the mask and a size\n",
    "        super().__init__() # parent class\n",
    "        \n",
    "        self.image = np.load(image_path).astype(np.float32)\n",
    "        self.mask = np.load(mask_path).astype(np.uint8)\n",
    "        \n",
    "        img_min, img_max = self.image.min(), self.image.max()\n",
    "        if img_max > 1.0:  # only normalize if not already in [0, 1]\n",
    "            self.image = (self.image - img_min) / (img_max - img_min + 1e-8) #avoid division by 0\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.block_size = self.image.shape[0] // grid_size  # 240/4 = 60\n",
    "        \n",
    "        self.action_space = action_space\n",
    "        self.tumor_threshold = tumor_threshold # 15% of the patch must be tumor to consider that the agent is inside the tumor region\n",
    "        self.rewards = rewards  # [reward_on_tumor, reward_stay_no_tumor, reward_move_no_tumor]\n",
    "        \n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Observations: grayscale patch (normalized 0-1)\n",
    "        # apparently Neural networks train better when inputs are scaled to small, \n",
    "        # consistent ranges rather than raw 0–255 values.\n",
    "        # self.observation_space = spaces.Box( # Supports continuous (and discrete) vectors or matrices\n",
    "        #     low=0, high=1, # Data has been normalized\n",
    "        #     shape=(self.block_size, self.block_size), # shape of the observation\n",
    "        #     dtype=np.float32\n",
    "        # )\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'patch': spaces.Box(low=0, high=1, shape=(self.block_size, self.block_size), dtype=np.float32),\n",
    "            'position': spaces.Box(low=0, high=self.grid_size-1, shape=(2,), dtype=np.int32)\n",
    "        })\n",
    "\n",
    "\n",
    "        self.agent_pos = [0, 0] # INITIAL POSITION AT TOP LEFT\n",
    "        self.current_step = 0 # initialize counter\n",
    "        self.max_steps = 20  # like in the paper\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        obs, info = super().reset(seed=seed)\n",
    "        return self._get_obs(), info\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "\n",
    "        prev_pos = self.agent_pos.copy() # for reward computation taking into consideration the transition changes\n",
    "        \n",
    "        # Apply action (respect grid boundaries)\n",
    "        if self.action_space == spaces.Discrete(3):\n",
    "            if action == 1 and self.agent_pos[0] < self.grid_size - 1:\n",
    "                self.agent_pos[0] += 1  # move down\n",
    "            elif action == 2 and self.agent_pos[1] < self.grid_size - 1:\n",
    "                self.agent_pos[1] += 1  # move right\n",
    "            # else, the agent doesn't move so the observation \n",
    "            # and reward will be calculated from the same position\n",
    "            # no need to compute self.agent_pos\n",
    "        elif self.action_space == spaces.Discrete(5):\n",
    "            if action == 1 and self.agent_pos[0] < self.grid_size - 1:\n",
    "                self.agent_pos[0] += 1  # move down\n",
    "            elif action == 2 and self.agent_pos[1] < self.grid_size - 1:\n",
    "                self.agent_pos[1] += 1  # move right\n",
    "            elif action == 3 and self.agent_pos[0] > 0:\n",
    "                self.agent_pos[0] -= 1  # move up\n",
    "            elif action == 4 and self.agent_pos[1] > 0:\n",
    "                self.agent_pos[1] -= 1  # move left\n",
    "        elif self.action_space == spaces.Discrete(9):\n",
    "            if action == 1 and self.agent_pos[0] < self.grid_size - 1:\n",
    "                self.agent_pos[0] += 1  # move down\n",
    "            elif action == 2 and self.agent_pos[1] < self.grid_size - 1:\n",
    "                self.agent_pos[1] += 1  # move right\n",
    "            elif action == 3 and self.agent_pos[0] > 0:\n",
    "                self.agent_pos[0] -= 1  # move up\n",
    "            elif action == 4 and self.agent_pos[1] > 0:\n",
    "                self.agent_pos[1] -= 1  # move left\n",
    "            elif action == 5 and self.agent_pos[0] < self.grid_size - 1 and self.agent_pos[1] < self.grid_size - 1:\n",
    "                self.agent_pos[0] += 1  # move down-right\n",
    "                self.agent_pos[1] += 1\n",
    "            elif action == 6 and self.agent_pos[0] > 0 and self.agent_pos[1] < self.grid_size - 1:\n",
    "                self.agent_pos[0] -= 1  # move up-right\n",
    "                self.agent_pos[1] += 1\n",
    "            elif action == 7 and self.agent_pos[0] < self.grid_size - 1 and self.agent_pos[1] > 0:\n",
    "                self.agent_pos[0] += 1  # move down-left\n",
    "                self.agent_pos[1] -= 1\n",
    "            elif action == 8 and self.agent_pos[0] > 0 and self.agent_pos[1] > 0:\n",
    "                self.agent_pos[0] -= 1  # move up-left\n",
    "                self.agent_pos[1] -= 1\n",
    "        \n",
    "        reward = self._get_reward(action, prev_pos)\n",
    "                \n",
    "        obs = self._get_obs()\n",
    "\n",
    "        # Episode ends\n",
    "        terminated = self.current_step >= self.max_steps\n",
    "        truncated = False  # we don’t need truncation here\n",
    "        info = {}\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        r0 = self.agent_pos[0] * self.block_size # row start\n",
    "        c0 = self.agent_pos[1] * self.block_size # col start\n",
    "        \n",
    "        patch = self.image[r0:r0+self.block_size, c0:c0+self.block_size].astype(np.float32)\n",
    "    \n",
    "        return {\n",
    "                'patch': patch,\n",
    "                'position': np.array(self.agent_pos, dtype=np.int32)\n",
    "            }\n",
    "    \n",
    "\n",
    "    def _get_reward(self, action, prev_pos):        \n",
    "        # look position of the agent in the mask\n",
    "        r0 = self.agent_pos[0] * self.block_size\n",
    "        c0 = self.agent_pos[1] * self.block_size\n",
    "        patch_mask = self.mask[r0:r0+self.block_size, c0:c0+self.block_size]\n",
    "        \n",
    "        # Now that i have the patch where i was and the patch where i am, i can check if there is tumor in any of them\n",
    "        # tumor is labeled as 1 or 4 in the mask        \n",
    "        # label 2 is edema\n",
    "        \n",
    "        # first get a count of the tumor pixels in the patch. \n",
    "        tumor_count_curr = np.sum(np.isin(patch_mask, [1, 4]))\n",
    "        total = self.block_size * self.block_size # to compute the percentage\n",
    "        # Determine if patch has more than self.tumor_threshold of tumor\n",
    "        inside = (tumor_count_curr / total) >= self.tumor_threshold\n",
    "        \n",
    "        if inside:\n",
    "            return self.rewards[0]  # reward for being on tumor or staying on tumor\n",
    "        else:\n",
    "            if action == 0 or prev_pos == self.agent_pos:  # stayed in place but no tumor. we are also taking into consideration that if the action was to move but we are at the edge of the grid, we also stay in place\n",
    "                return self.rewards[1]\n",
    "            else:\n",
    "                return self.rewards[2]  # moved but no tumor\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode != \"human\": # would be rgb_array or ansi\n",
    "            return  # Only render in human mode\n",
    "\n",
    "        # Create RGB visualization image\n",
    "        # not necessary since it's grayscale, but i want to draw the mask and position\n",
    "        vis_img = np.stack([self.image] * 3, axis=-1).astype(np.float32)\n",
    "\n",
    "        # Overlay tumor mask in red [..., 0] \n",
    "        tumor_overlay = np.zeros_like(vis_img) # do all blank but here we have 3 channels, mask is 2D\n",
    "        tumor_overlay[..., 0] = (self.mask > 0).astype(float) # red channel. set to float to avoid issues when blending in vis_img\n",
    "\n",
    "        # transparency overlay (crec que es el mateix valor que tinc a l'altra notebook)\n",
    "        alpha = 0.4\n",
    "        vis_img = (1 - alpha) * vis_img + alpha * tumor_overlay\n",
    "\n",
    "        # Plotting\n",
    "        fig, ax = plt.subplots(figsize=(3, 3))\n",
    "        ax.imshow(vis_img, cmap='gray', origin='upper')\n",
    "\n",
    "        # Draw grid lines\n",
    "        # alpha for transparency again\n",
    "        for i in range(1, self.grid_size):\n",
    "            ax.axhline(i * self.block_size, color='white', lw=1, alpha=0.5)\n",
    "            ax.axvline(i * self.block_size, color='white', lw=1, alpha=0.5)\n",
    "\n",
    "        # Draw agent position\n",
    "        r0 = self.agent_pos[0] * self.block_size\n",
    "        c0 = self.agent_pos[1] * self.block_size\n",
    "        rect = patches.Rectangle(\n",
    "            (c0, r0), # (x,y) bottom left corner\n",
    "            self.block_size, # width\n",
    "            self.block_size, # height\n",
    "            linewidth=2,\n",
    "            edgecolor='yellow',\n",
    "            facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        ax.set_title(f\"Agent at {self.agent_pos} | Step {self.current_step}\")\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "    def current_patch_overlap_with_lesion(self): # FALTAAA chat\n",
    "        \"\"\" Returns the number of overlapping lesion pixels between the agent's current patch and the ground-truth mask. If > 0, the agent is correctly over the lesion (TP). \"\"\"\n",
    "        # get current agent patch boundaries\n",
    "        row, col = self.agent_pos\n",
    "        patch_h = self.block_size # not grid_size because grid_size is number of patches per side\n",
    "        patch_w = self.block_size\n",
    "        \n",
    "        y0 = row * patch_h\n",
    "        y1 = y0 + patch_h\n",
    "        x0 = col * patch_w\n",
    "        x1 = x0 + patch_w\n",
    "        # extract mask region under current patch\n",
    "        patch_mask = self.mask[y0:y1, x0:x1]\n",
    "        # count how many pixels of lesion (nonzero)\n",
    "        overlap = np.sum(patch_mask > 0)\n",
    "        return overlap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fd2208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(mode = \"train\"):\n",
    "    if mode == \"train\":\n",
    "        base_dir = \"/home/martina/codi2/4year/tfg/training_set_npy\"\n",
    "        csv_path = \"/home/martina/codi2/4year/tfg/set_training.csv\"\n",
    "    else:\n",
    "        base_dir = \"/home/martina/codi2/4year/tfg/testing_set_npy\"\n",
    "        csv_path = \"/home/martina/codi2/4year/tfg/set_testing.csv\"\n",
    "\n",
    "    # Load the CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Construct image and mask filenames\n",
    "    df[\"image_path\"] = df.apply(\n",
    "        lambda row: os.path.join(base_dir, f\"{row['Patient']:03d}_{row['SliceIndex']}.npy\"), axis=1\n",
    "    )\n",
    "    df[\"mask_path\"] = df.apply(\n",
    "        lambda row: os.path.join(base_dir, f\"{row['Patient']:03d}_{row['SliceIndex']}_mask.npy\"), axis=1\n",
    "    )\n",
    "\n",
    "    # Sanity check (optional)\n",
    "    pairs = [\n",
    "        (img, mask)\n",
    "        for img, mask in zip(df[\"image_path\"], df[\"mask_path\"])\n",
    "        if os.path.exists(img) and os.path.exists(mask)\n",
    "    ]\n",
    "\n",
    "    print(f\"✅ Found {len(pairs)} pairs out of {len(df)} listed in CSV.\")\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ecac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAwarePPOActorCritic(nn.Module):\n",
    "    def __init__(self, env, learning_rate=3e-4, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.n_outputs = env.action_space.n\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # CNN for patch processing\n",
    "        input_channels = 1\n",
    "        patch_shape = env.observation_space['patch'].shape  # (60, 60)\n",
    "        \n",
    "        self.patch_features = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        \n",
    "        # Calculate flattened patch features size\n",
    "        with torch.no_grad():\n",
    "            dummy_patch = torch.zeros(1, input_channels, *patch_shape)\n",
    "            patch_features_out = self.patch_features(dummy_patch)\n",
    "            patch_flatten = patch_features_out.view(1, -1).size(1)\n",
    "        \n",
    "        # Position embedding\n",
    "        position_size = env.observation_space['position'].shape[0]  # 2\n",
    "        self.position_embedding = nn.Sequential(\n",
    "            nn.Linear(position_size, 16),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        \n",
    "        # Combined features\n",
    "        combined_features_size = patch_flatten + 32\n",
    "        \n",
    "        # Actor and Critic\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(combined_features_size, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, self.n_outputs),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(combined_features_size, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "        if self.device == 'cuda':\n",
    "            self.to(self.device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if isinstance(x, dict):\n",
    "            # Single observation - convert to batch of size 1\n",
    "            patch = x['patch']\n",
    "            position = x['position']\n",
    "            \n",
    "            if isinstance(patch, np.ndarray):\n",
    "                if patch.ndim == 2:\n",
    "                    patch = patch[np.newaxis, np.newaxis, :, :]  # (1, 1, 60, 60)\n",
    "                patch = torch.FloatTensor(patch).to(self.device)\n",
    "            \n",
    "            if isinstance(position, np.ndarray):\n",
    "                position = torch.FloatTensor(position).to(self.device).unsqueeze(0)  # Add batch dim\n",
    "            \n",
    "        elif isinstance(x, list):\n",
    "            # Batch of observations\n",
    "            patch_batch = []\n",
    "            position_batch = []\n",
    "            \n",
    "            for obs in x:\n",
    "                patch_batch.append(obs['patch'])\n",
    "                position_batch.append(obs['position'])\n",
    "            \n",
    "            patch_array = np.array(patch_batch)\n",
    "            if patch_array.ndim == 3:\n",
    "                patch_array = patch_array[:, np.newaxis, :, :]\n",
    "            \n",
    "            patch = torch.FloatTensor(patch_array).to(self.device)\n",
    "            position = torch.FloatTensor(np.array(position_batch)).to(self.device)\n",
    "        \n",
    "        # Process through networks (both paths now have batch dimension)\n",
    "        patch_features = self.patch_features(patch)\n",
    "        patch_flat = patch_features.view(patch.size(0), -1)\n",
    "        position_embedded = self.position_embedding(position)\n",
    "        combined = torch.cat([patch_flat, position_embedded], dim=-1)\n",
    "        \n",
    "        action_probs = self.actor(combined)\n",
    "        state_values = self.critic(combined)\n",
    "        \n",
    "        return action_probs, state_values\n",
    "\n",
    "\n",
    "\n",
    "# Fixed environment with global awareness\n",
    "class GlobalAwareGlioblastoma(Glioblastoma):\n",
    "    def __init__(self, image_path, mask_path, grid_size=4, tumor_threshold=0.0001, rewards=[1.0, -2.0, -0.5], action_space=spaces.Discrete(3), render_mode=\"human\"):\n",
    "        super().__init__(image_path, mask_path, grid_size, tumor_threshold, rewards, action_space, render_mode)\n",
    "        \n",
    "        self.image = np.load(image_path).astype(np.float32)\n",
    "        self.mask = np.load(mask_path).astype(np.uint8)\n",
    "        \n",
    "        img_min, img_max = self.image.min(), self.image.max()\n",
    "        if img_max > 1.0:\n",
    "            self.image = (self.image - img_min) / (img_max - img_min + 1e-8)\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.block_size = self.image.shape[0] // grid_size\n",
    "        \n",
    "        self.action_space = action_space\n",
    "        self.tumor_threshold = tumor_threshold\n",
    "        self.rewards = rewards\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Dict observation space with position info\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'patch': spaces.Box(low=0, high=1, shape=(self.block_size, self.block_size), dtype=np.float32),\n",
    "            'position': spaces.Box(low=0, high=grid_size-1, shape=(2,), dtype=np.int32)\n",
    "        })\n",
    "\n",
    "        self.agent_pos = [0, 0]\n",
    "        self.current_step = 0\n",
    "        self.max_steps = 20\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.agent_pos = [0, 0]\n",
    "        self.current_step = 0\n",
    "        obs = self._get_obs()\n",
    "        info = {}\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        prev_pos = self.agent_pos.copy()\n",
    "        \n",
    "        # Movement logic (same as before)\n",
    "        if self.action_space.n == 3:\n",
    "            if action == 1 and self.agent_pos[0] < self.grid_size - 1:\n",
    "                self.agent_pos[0] += 1\n",
    "            elif action == 2 and self.agent_pos[1] < self.grid_size - 1:\n",
    "                self.agent_pos[1] += 1\n",
    "        elif self.action_space.n == 5:\n",
    "            if action == 1 and self.agent_pos[0] < self.grid_size - 1:\n",
    "                self.agent_pos[0] += 1\n",
    "            elif action == 2 and self.agent_pos[1] < self.grid_size - 1:\n",
    "                self.agent_pos[1] += 1\n",
    "            elif action == 3 and self.agent_pos[0] > 0:\n",
    "                self.agent_pos[0] -= 1\n",
    "            elif action == 4 and self.agent_pos[1] > 0:\n",
    "                self.agent_pos[1] -= 1\n",
    "        \n",
    "        reward = self._get_reward(action, prev_pos)\n",
    "        obs = self._get_obs()\n",
    "        terminated = self.current_step >= self.max_steps\n",
    "        truncated = False\n",
    "        info = {}\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        r0 = self.agent_pos[0] * self.block_size\n",
    "        c0 = self.agent_pos[1] * self.block_size\n",
    "        patch = self.image[r0:r0+self.block_size, c0:c0+self.block_size].astype(np.float32)\n",
    "        \n",
    "        return {\n",
    "            'patch': patch,\n",
    "            'position': np.array(self.agent_pos, dtype=np.int32)\n",
    "        }\n",
    "\n",
    "    def _get_reward(self, action, prev_pos):\n",
    "        r0 = self.agent_pos[0] * self.block_size\n",
    "        c0 = self.agent_pos[1] * self.block_size\n",
    "        patch_mask = self.mask[r0:r0+self.block_size, c0:c0+self.block_size]\n",
    "        \n",
    "        tumor_count_curr = np.sum(np.isin(patch_mask, [1, 4]))\n",
    "        total = self.block_size * self.block_size\n",
    "        inside = (tumor_count_curr / total) >= self.tumor_threshold\n",
    "        \n",
    "        if inside:\n",
    "            return self.rewards[0]\n",
    "        else:\n",
    "            if action == 0 or prev_pos == self.agent_pos:\n",
    "                return self.rewards[1]\n",
    "            else:\n",
    "                return self.rewards[2]\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode != \"human\":\n",
    "            return\n",
    "\n",
    "        vis_img = np.stack([self.image] * 3, axis=-1).astype(np.float32)\n",
    "        tumor_overlay = np.zeros_like(vis_img)\n",
    "        tumor_overlay[..., 0] = (self.mask > 0).astype(float)\n",
    "        \n",
    "        alpha = 0.4\n",
    "        vis_img = (1 - alpha) * vis_img + alpha * tumor_overlay\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "        import matplotlib.patches as patches\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(3, 3))\n",
    "        ax.imshow(vis_img, cmap='gray', origin='upper')\n",
    "\n",
    "        for i in range(1, self.grid_size):\n",
    "            ax.axhline(i * self.block_size, color='white', lw=1, alpha=0.5)\n",
    "            ax.axvline(i * self.block_size, color='white', lw=1, alpha=0.5)\n",
    "\n",
    "        r0 = self.agent_pos[0] * self.block_size\n",
    "        c0 = self.agent_pos[1] * self.block_size\n",
    "        rect = patches.Rectangle(\n",
    "            (c0, r0), self.block_size, self.block_size,\n",
    "            linewidth=2, edgecolor='yellow', facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        ax.set_title(f\"Agent at {self.agent_pos} | Step {self.current_step}\")\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "    def current_patch_overlap_with_lesion(self):\n",
    "        row, col = self.agent_pos\n",
    "        patch_h = self.block_size\n",
    "        patch_w = self.block_size\n",
    "        \n",
    "        y0 = row * patch_h\n",
    "        y1 = y0 + patch_h\n",
    "        x0 = col * patch_w\n",
    "        x1 = x0 + patch_w\n",
    "        patch_mask = self.mask[y0:y1, x0:x1]\n",
    "        overlap = np.sum(patch_mask > 0)\n",
    "        return overlap\n",
    "\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, env_config, model, train_pairs, env_class,\n",
    "                 gamma=0.99, gae_lambda=0.95,\n",
    "                 clip_epsilon=0.2, ppo_epochs=4, batch_size=64,\n",
    "                 save_name=\"PPO_Agent\"):\n",
    "        \n",
    "        self.env_config = env_config\n",
    "        self.env_class = env_class\n",
    "        self.model = model\n",
    "        self.device = model.device\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.save_name = save_name\n",
    "        \n",
    "        self.training_rewards = []\n",
    "        self.mean_training_rewards = []\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "        self.entropies = []\n",
    "        \n",
    "        self.train_pairs = train_pairs\n",
    "\n",
    "\n",
    "# Also need to fix the PPOAgent to handle dict observations\n",
    "class GlobalAwarePPOAgent(PPOAgent):\n",
    "    def __init__(self, env_config, model, train_pairs, env_class,\n",
    "                 gamma=0.99, gae_lambda=0.95,\n",
    "                 clip_epsilon=0.2, ppo_epochs=4, batch_size=64,\n",
    "                 save_name=\"GlobalAware_PPO\"):\n",
    "        \n",
    "        self.env_config = env_config\n",
    "        self.env_class = env_class\n",
    "        self.model = model\n",
    "        self.device = model.device\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.save_name = save_name\n",
    "        \n",
    "        self.training_rewards = []\n",
    "        self.mean_training_rewards = []\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "        self.entropies = []\n",
    "        \n",
    "        self.train_pairs = train_pairs\n",
    "        \n",
    "    def compute_gae(self, rewards, values, dones, next_value):\n",
    "        gae = 0\n",
    "        returns = []\n",
    "        advantages = []\n",
    "        \n",
    "        values = values + [next_value]\n",
    "        \n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + self.gamma * values[step + 1] * (1 - dones[step]) - values[step]\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1 - dones[step]) * gae\n",
    "            advantages.insert(0, gae)\n",
    "            returns.insert(0, gae + values[step])\n",
    "            \n",
    "        return returns, advantages\n",
    "    \n",
    "    def collect_trajectories(self, num_steps=2048):\n",
    "        all_states = []\n",
    "        all_actions = []\n",
    "        all_rewards = []\n",
    "        all_dones = []\n",
    "        all_values = []\n",
    "        all_log_probs = []\n",
    "        \n",
    "        img_path, mask_path = random.choice(self.train_pairs)\n",
    "        env = self.env_class(img_path, mask_path, **self.env_config)\n",
    "        state, _ = env.reset()\n",
    "        \n",
    "        episode_reward = 0\n",
    "        episode_rewards = []\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            with torch.no_grad():\n",
    "                action_probs, value = self.model(state)\n",
    "                dist = Categorical(action_probs)\n",
    "                action = dist.sample()\n",
    "                log_prob = dist.log_prob(action)\n",
    "                value = value.squeeze()\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Store the state dict properly\n",
    "            all_states.append({\n",
    "                'patch': state['patch'].copy(),\n",
    "                'position': state['position'].copy()\n",
    "            })\n",
    "            all_actions.append(action.item())\n",
    "            all_rewards.append(reward)\n",
    "            all_dones.append(done)\n",
    "            all_values.append(value.item())\n",
    "            all_log_probs.append(log_prob.item())\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                episode_rewards.append(episode_reward)\n",
    "                img_path, mask_path = random.choice(self.train_pairs)\n",
    "                env = self.env_class(img_path, mask_path, **self.env_config)\n",
    "                state, _ = env.reset()\n",
    "                episode_reward = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _, next_value = self.model(state)\n",
    "            next_value = next_value.squeeze().item()\n",
    "        \n",
    "        return (all_states, all_actions, all_rewards, all_dones, \n",
    "                all_values, all_log_probs, next_value, episode_rewards)\n",
    "    \n",
    "    def update(self, states, actions, returns, advantages, old_log_probs):\n",
    "        # Prepare batch data\n",
    "        patch_batch = []\n",
    "        position_batch = []\n",
    "        \n",
    "        for state in states:\n",
    "            patch_batch.append(state['patch'])\n",
    "            position_batch.append(state['position'])\n",
    "        \n",
    "        # Convert to tensors\n",
    "        patch_array = np.array(patch_batch)\n",
    "        if patch_array.ndim == 3:\n",
    "            patch_array = patch_array[:, np.newaxis, :, :]\n",
    "        \n",
    "        batch_data = {\n",
    "            'patch': torch.FloatTensor(patch_array).to(self.device),\n",
    "            'position': torch.FloatTensor(np.array(position_batch)).to(self.device)\n",
    "        }\n",
    "        \n",
    "        actions_tensor = torch.LongTensor(actions).to(self.device)\n",
    "        returns_tensor = torch.FloatTensor(returns).to(self.device)\n",
    "        advantages_tensor = torch.FloatTensor(advantages).to(self.device)\n",
    "        old_log_probs_tensor = torch.FloatTensor(old_log_probs).to(self.device)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages_tensor = (advantages_tensor - advantages_tensor.mean()) / (advantages_tensor.std() + 1e-8)\n",
    "        \n",
    "        batch_size = len(states)\n",
    "        indices = np.arange(batch_size)\n",
    "        \n",
    "        for _ in range(self.ppo_epochs):\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            for start in range(0, batch_size, self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                batch_indices = indices[start:end]\n",
    "                \n",
    "                batch_states = {\n",
    "                    'patch': batch_data['patch'][batch_indices],\n",
    "                    'position': batch_data['position'][batch_indices]\n",
    "                }\n",
    "                batch_actions = actions_tensor[batch_indices]\n",
    "                batch_returns = returns_tensor[batch_indices]\n",
    "                batch_advantages = advantages_tensor[batch_indices]\n",
    "                batch_old_log_probs = old_log_probs_tensor[batch_indices]\n",
    "                \n",
    "                # Get current policy and value\n",
    "                action_probs, values = self.model(batch_states)\n",
    "                dist = Categorical(action_probs)\n",
    "                new_log_probs = dist.log_prob(batch_actions)\n",
    "                entropy = dist.entropy().mean()\n",
    "                \n",
    "                values = values.squeeze()\n",
    "                \n",
    "                # Calculate ratios\n",
    "                ratios = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "                \n",
    "                # Policy loss\n",
    "                surr1 = ratios * batch_advantages\n",
    "                surr2 = torch.clamp(ratios, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                # Value loss\n",
    "                value_loss = 0.5 * (values - batch_returns).pow(2).mean()\n",
    "                \n",
    "                # Total loss\n",
    "                loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
    "                \n",
    "                # Backpropagate\n",
    "                self.model.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)\n",
    "                self.model.optimizer.step()\n",
    "                \n",
    "                self.actor_losses.append(policy_loss.item())\n",
    "                self.critic_losses.append(value_loss.item())\n",
    "                self.entropies.append(entropy.item())\n",
    "    \n",
    "    def train(self, max_episodes=1000, num_steps=512):\n",
    "        print(\"Starting Global-Aware PPO training...\")\n",
    "        \n",
    "        episode = 0\n",
    "        best_mean_reward = -float('inf')\n",
    "        \n",
    "        while episode < max_episodes:\n",
    "            (states, actions, rewards, dones, values, \n",
    "             old_log_probs, next_value, episode_rewards) = self.collect_trajectories(num_steps)\n",
    "            \n",
    "            if not states:\n",
    "                continue\n",
    "            \n",
    "            returns, advantages = self.compute_gae(rewards, values, dones, next_value)\n",
    "            self.update(states, actions, returns, advantages, old_log_probs)\n",
    "            \n",
    "            self.training_rewards.extend(episode_rewards)\n",
    "            \n",
    "            if len(self.training_rewards) >= 100:\n",
    "                mean_reward = np.mean(self.training_rewards[-100:])\n",
    "            else:\n",
    "                mean_reward = np.mean(self.training_rewards)\n",
    "                \n",
    "            self.mean_training_rewards.append(mean_reward)\n",
    "            \n",
    "            if episode_rewards:\n",
    "                avg_episode_reward = np.mean(episode_rewards)\n",
    "                print(f\"Episode {episode} | \"\n",
    "                      f\"Avg Reward: {avg_episode_reward:.2f} | \"\n",
    "                      f\"Mean Reward (100): {mean_reward:.2f} | \"\n",
    "                      f\"Actor Loss: {np.mean(self.actor_losses[-10:] or [0]):.4f}\")\n",
    "            \n",
    "            episode += len(episode_rewards)\n",
    "            \n",
    "            if mean_reward > best_mean_reward:\n",
    "                best_mean_reward = mean_reward\n",
    "                torch.save(self.model.state_dict(), f\"{self.save_name}_best.pth\")\n",
    "                print(\"New best model saved!\")\n",
    "            \n",
    "            if episode % 100 == 0:\n",
    "                torch.save(self.model.state_dict(), f\"{self.save_name}_checkpoint.pth\")\n",
    "        \n",
    "        torch.save(self.model.state_dict(), f\"{self.save_name}_final.pth\")\n",
    "        print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d97e78",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b353a6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "CURRENT_CONFIG = {\n",
    "    'grid_size': 4,\n",
    "    'rewards': [5.0, -0.01, 0.0], \n",
    "    'action_space': gym.spaces.Discrete(5)\n",
    "}\n",
    "\n",
    "LR = 3e-4\n",
    "MAX_EPISODES = 1000\n",
    "NUM_STEPS = 512  # Start with smaller rollout for testing\n",
    "\n",
    "train_pairs = prepare()\n",
    "\n",
    "env = GlobalAwareGlioblastoma(*train_pairs[0], **CURRENT_CONFIG)\n",
    "model = GlobalAwarePPOActorCritic(env, learning_rate=LR, device='cpu')\n",
    "agent = GlobalAwarePPOAgent(\n",
    "    env_config=CURRENT_CONFIG,\n",
    "    model=model,\n",
    "    train_pairs=train_pairs,\n",
    "    env_class=GlobalAwareGlioblastoma,  # Use the new environment class\n",
    "    gamma=0.99,\n",
    "    clip_epsilon=0.2,\n",
    "    ppo_epochs=4,\n",
    "    batch_size=128,\n",
    "    save_name=\"GlobalAware_PPO_batch128\"\n",
    ")\n",
    "\n",
    "# Start training\n",
    "agent.train(max_episodes=MAX_EPISODES, num_steps=NUM_STEPS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f706c9",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcbc350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ppo_agent(agent, test_pairs):\n",
    "    \"\"\"Test the trained PPO agent on test data\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for img_path, mask_path in test_pairs:\n",
    "        env = agent.env_class(img_path, mask_path, **agent.env_config)\n",
    "        state, _ = env.reset()\n",
    "        \n",
    "        tumor_hits = 0\n",
    "        final_on_tumor = False\n",
    "        action_distribution = np.zeros(env.action_space.n)\n",
    "        \n",
    "        for step in range(20):  # Fixed 20 steps\n",
    "            with torch.no_grad():\n",
    "                action_probs, _ = agent.model(state)\n",
    "                dist = Categorical(action_probs)\n",
    "                action = dist.sample()\n",
    "                \n",
    "            action_distribution[action.item()] += 1\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Check if current patch overlaps with lesion\n",
    "            if env.current_patch_overlap_with_lesion() > 0:\n",
    "                tumor_hits += 1\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Check if final position is on tumor\n",
    "        if env.current_patch_overlap_with_lesion() > 0:\n",
    "            final_on_tumor = True\n",
    "        \n",
    "        results.append({\n",
    "            'image_path': img_path,\n",
    "            'tumor_hits': tumor_hits,\n",
    "            'final_on_tumor': final_on_tumor,\n",
    "            'action_distribution': action_distribution / np.sum(action_distribution)  # Normalize\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "test_pairs = prepare(mode=\"test\")\n",
    "test_results = test_ppo_agent(agent, test_pairs)\n",
    "\n",
    "count = 0\n",
    "for i, result in enumerate(test_results):\n",
    "    print(f\"#{i} Image: {result['image_path']}, Tumor Hits: {result['tumor_hits']}, \"\n",
    "          f\"Final on Tumor: {result['final_on_tumor']}, \"\n",
    "          f\"Action Distribution: {result['action_distribution']}\")\n",
    "    if result['final_on_tumor']:\n",
    "        count += 1\n",
    "        \n",
    "print(f\"Total images where agent ended on tumor: {count} out of {len(test_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62af4dae",
   "metadata": {},
   "source": [
    "[48 - batch64](GlobalAware_PPO_batch64_best.pth)\n",
    "\n",
    "[49 - batch128](GlobalAware_PPO_batch128_best.pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfe2218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_agent_unified(agent, test_pairs, agent_type, num_episodes=None, env_config=None, save_gifs=True, gif_folder=\"TEST_GIFS\"):\n",
    "    \"\"\"\n",
    "    Unified testing function for both DQN and PPO agents\n",
    "    \n",
    "    Args:\n",
    "        agent: The trained agent (DQN or PPO)\n",
    "        test_pairs: List of (image_path, mask_path) tuples\n",
    "        agent_type: Either \"dqn\" or \"ppo\"\n",
    "        num_episodes: Number of episodes to test (default: all test pairs)\n",
    "        env_config: Environment configuration dictionary\n",
    "        save_gifs: Whether to save GIFs of episodes\n",
    "        gif_folder: Folder to save GIFs\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with test results including success rate and action distributions\n",
    "    \"\"\"\n",
    "    if num_episodes is None:\n",
    "        num_episodes = len(test_pairs)\n",
    "    \n",
    "    # Create GIF folder if needed\n",
    "    if save_gifs and not os.path.exists(gif_folder):\n",
    "        os.makedirs(gif_folder)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    if agent_type.lower() == \"dqn\":\n",
    "        agent.dnnetwork.eval()\n",
    "    elif agent_type.lower() == \"ppo\":\n",
    "        agent.model.eval()\n",
    "    \n",
    "    results = {\n",
    "        'success_rate': [],\n",
    "        'final_position_accuracy': [],\n",
    "        'average_reward': [],\n",
    "        'steps_to_find_tumor': [],\n",
    "        'tumor_coverage': [],\n",
    "        'total_tumor_reward': [],\n",
    "        'episode_details': []\n",
    "    }\n",
    "    \n",
    "    grid_size = env_config.get('grid_size', 4)\n",
    "    rewards = env_config.get('rewards', [5.0, -1.0, -0.2])\n",
    "    action_space = env_config.get('action_space', None)\n",
    "    \n",
    "    for i in range(min(num_episodes, len(test_pairs))):\n",
    "        img_path, mask_path = test_pairs[i]\n",
    "        \n",
    "        # Create environment\n",
    "        if hasattr(agent, 'env_class'):\n",
    "            env = agent.env_class(img_path, mask_path, grid_size=grid_size, rewards=rewards, action_space=action_space)\n",
    "        else:\n",
    "            env = Glioblastoma(img_path, mask_path, grid_size=grid_size, rewards=rewards, action_space=action_space)\n",
    "        \n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        found_tumor = False\n",
    "        tumor_positions_visited = set()\n",
    "        steps_to_find = env.max_steps\n",
    "        tumor_rewards = 0\n",
    "        \n",
    "        # For action distribution tracking\n",
    "        action_counts = np.zeros(env.action_space.n)\n",
    "        \n",
    "        # For GIF creation\n",
    "        frames = []\n",
    "        \n",
    "        for step in range(env.max_steps):\n",
    "            with torch.no_grad():\n",
    "                if agent_type.lower() == \"dqn\":\n",
    "                    action = agent.dnnetwork.get_action(state, epsilon=0.00)\n",
    "                    action_idx = action\n",
    "                elif agent_type.lower() == \"ppo\":\n",
    "                    action_probs, _ = agent.model(state)\n",
    "                    dist = Categorical(action_probs)\n",
    "                    action = dist.sample()\n",
    "                    action_idx = action.item()\n",
    "            \n",
    "            action_counts[action_idx] += 1\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action_idx)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Track tumor-related metrics\n",
    "            current_overlap = env.current_patch_overlap_with_lesion()\n",
    "            if current_overlap > 0:\n",
    "                tumor_positions_visited.add(tuple(env.agent_pos))\n",
    "                if not found_tumor:\n",
    "                    found_tumor = True\n",
    "                    steps_to_find = step + 1\n",
    "                \n",
    "                # Count positive rewards (when on tumor)\n",
    "                if reward > 0:\n",
    "                    tumor_rewards += 1\n",
    "            \n",
    "            # Capture frame for GIF\n",
    "            if save_gifs:\n",
    "                frame = env.render(mode='rgb_array')\n",
    "                if frame is not None:\n",
    "                    frames.append(frame)\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        # Save GIF\n",
    "        gif_path = None\n",
    "        if save_gifs and frames:\n",
    "            gif_path = os.path.join(gif_folder, f\"episode_{i}_{os.path.basename(img_path).split('.')[0]}.gif\")\n",
    "            # Convert frames to PIL Images and save as GIF\n",
    "            pil_frames = [Image.fromarray(frame) for frame in frames]\n",
    "            pil_frames[0].save(\n",
    "                gif_path,\n",
    "                save_all=True,\n",
    "                append_images=pil_frames[1:],\n",
    "                duration=500,  # milliseconds per frame\n",
    "                loop=0\n",
    "            )\n",
    "        \n",
    "        # Calculate metrics for this episode\n",
    "        final_overlap = env.current_patch_overlap_with_lesion()\n",
    "        \n",
    "        # Success: ended on tumor region\n",
    "        success = final_overlap > 0\n",
    "        results['success_rate'].append(success)\n",
    "        \n",
    "        # Final position accuracy\n",
    "        results['final_position_accuracy'].append(final_overlap > 0)\n",
    "        \n",
    "        # Average reward\n",
    "        results['average_reward'].append(total_reward)\n",
    "        \n",
    "        # Steps to find tumor\n",
    "        results['steps_to_find_tumor'].append(steps_to_find)\n",
    "        \n",
    "        # Tumor coverage (percentage of tumor patches visited)\n",
    "        total_tumor_patches = count_tumor_patches(env)\n",
    "        coverage = len(tumor_positions_visited) / total_tumor_patches if total_tumor_patches > 0 else 0\n",
    "        results['tumor_coverage'].append(coverage)\n",
    "        \n",
    "        # Total positive rewards from tumor\n",
    "        results['total_tumor_reward'].append(tumor_rewards)\n",
    "        \n",
    "        # Store detailed episode information\n",
    "        episode_detail = {\n",
    "            'image_path': img_path,\n",
    "            'success': success,\n",
    "            'final_on_tumor': final_overlap > 0,\n",
    "            'total_reward': total_reward,\n",
    "            'steps_to_find_tumor': steps_to_find,\n",
    "            'tumor_coverage': coverage,\n",
    "            'tumor_rewards': tumor_rewards,\n",
    "            'action_distribution': action_counts / np.sum(action_counts),  # Normalized\n",
    "            'gif_path': gif_path\n",
    "        }\n",
    "        results['episode_details'].append(episode_detail)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    overall_results = {\n",
    "        'success_rate': np.mean(results['success_rate']),\n",
    "        'final_position_accuracy': np.mean(results['final_position_accuracy']),\n",
    "        'average_reward': np.mean(results['average_reward']),\n",
    "        'avg_steps_to_find_tumor': np.mean(results['steps_to_find_tumor']),\n",
    "        'avg_tumor_coverage': np.mean(results['tumor_coverage']),\n",
    "        'avg_tumor_rewards': np.mean(results['total_tumor_reward']),\n",
    "        'episode_details': results['episode_details']\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"TEST RESULTS ({agent_type.upper()} Agent)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Success Rate: {overall_results['success_rate']*100:.2f}%\")\n",
    "    print(f\"Final Position Accuracy: {overall_results['final_position_accuracy']*100:.2f}%\")\n",
    "    print(f\"Average Episode Reward: {overall_results['average_reward']:.2f}\")\n",
    "    print(f\"Average Steps to Find Tumor: {overall_results['avg_steps_to_find_tumor']:.2f}\")\n",
    "    print(f\"Average Tumor Coverage: {overall_results['avg_tumor_coverage']*100:.2f}%\")\n",
    "    print(f\"Average Tumor Rewards per Episode: {overall_results['avg_tumor_rewards']:.2f}\")\n",
    "    \n",
    "    # Print individual episode results\n",
    "    print(f\"\\nDetailed Results for {len(results['episode_details'])} episodes:\")\n",
    "    print(\"-\" * 80)\n",
    "    for i, detail in enumerate(results['episode_details']):\n",
    "        print(f\"Episode {i}: {os.path.basename(detail['image_path'])}\")\n",
    "        print(f\"  Success: {detail['success']}, Final on Tumor: {detail['final_on_tumor']}\")\n",
    "        print(f\"  Total Reward: {detail['total_reward']:.2f}, Steps to Find: {detail['steps_to_find_tumor']}\")\n",
    "        print(f\"  Tumor Coverage: {detail['tumor_coverage']*100:.2f}%, Tumor Rewards: {detail['tumor_rewards']}\")\n",
    "        print(f\"  Action Distribution: {detail['action_distribution']}\")\n",
    "        if detail['gif_path']:\n",
    "            print(f\"  GIF saved: {detail['gif_path']}\")\n",
    "        print()\n",
    "    \n",
    "    return overall_results\n",
    "\n",
    "def count_tumor_patches(env):\n",
    "    \"\"\"Count total number of patches that contain tumor\"\"\"\n",
    "    tumor_patches = 0\n",
    "    original_pos = env.agent_pos.copy()  # Save original position\n",
    "    \n",
    "    for i in range(env.grid_size):\n",
    "        for j in range(env.grid_size):\n",
    "            env.agent_pos = [i, j]\n",
    "            if env.current_patch_overlap_with_lesion() > 0:\n",
    "                tumor_patches += 1\n",
    "    \n",
    "    env.agent_pos = original_pos  # Restore original position\n",
    "    return tumor_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5ac9bf",
   "metadata": {},
   "source": [
    "# Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d980d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# render on human mode the last test image\n",
    "env = agent.env_class(test_pairs[-1][0], test_pairs[-1][1], **agent.env_config)\n",
    "state, _ = env.reset()\n",
    "for step in range(20):\n",
    "    with torch.no_grad():\n",
    "        action_probs, _ = agent.model(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "    next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "    env.render()\n",
    "    \n",
    "    state = next_state\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
