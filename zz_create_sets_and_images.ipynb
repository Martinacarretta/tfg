{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e3b1cb9",
   "metadata": {},
   "source": [
    "# To create new data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38c4491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import random\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b76fbb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mount succeeded via bash script!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "try:\n",
    "    subprocess.run(['bash', 'z_mount_usb.sh'], check=True)\n",
    "    print(\"Mount succeeded via bash script!\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Mount failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c6ff1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths(patient_id):\n",
    "    pat = f\"{patient_id:03d}\"  # ensure 3 digits\n",
    "    root = f\"{BRA_TS_ROOT}{pat}\"\n",
    "    img_path = os.path.join(root, f\"BraTS20_Training_{pat}_{IMAGE_TYPE}{EXT}\")\n",
    "    mask_path = os.path.join(root, f\"BraTS20_Training_{pat}_{MASK_TYPE}{EXT}\")\n",
    "    return img_path, mask_path\n",
    "\n",
    "\n",
    "def load_volume(image_path, mask_path):\n",
    "    img = nib.load(image_path).get_fdata()      # (240,240,155)\n",
    "    mask = nib.load(mask_path).get_fdata()\n",
    "    # transpose to (240,240,depth) like your pipeline\n",
    "    return img.transpose(1, 0, 2), mask.transpose(1, 0, 2)\n",
    "\n",
    "\n",
    "def compute_tumor_stats(mask_slice):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - percentage of tumor core\n",
    "      - centroid (x,y) of tumor core (if exists)\n",
    "    \"\"\"\n",
    "    h, w = mask_slice.shape\n",
    "    tumor_mask = np.isin(mask_slice, TUMOR_LABELS)\n",
    "\n",
    "    tumor_pixels = np.sum(tumor_mask)\n",
    "    total_pixels = h * w\n",
    "    percent = tumor_pixels / total_pixels\n",
    "\n",
    "    if tumor_pixels == 0:\n",
    "        return percent, None\n",
    "\n",
    "    ys, xs = np.where(tumor_mask)\n",
    "    cx = float(np.mean(xs))\n",
    "    cy = float(np.mean(ys))\n",
    "    return percent, (cx, cy)\n",
    "\n",
    "\n",
    "def classify_position_loose(centroid):\n",
    "    if centroid is None:\n",
    "        return None\n",
    "\n",
    "    cx, cy = centroid\n",
    "    H, W = 240, 240\n",
    "\n",
    "    # 20% margins instead of 25% → larger variety\n",
    "    margin = 0.20 * W   # = 48px\n",
    "\n",
    "    if cx < margin:\n",
    "        return \"left\"\n",
    "    if cx > (W - margin):\n",
    "        return \"right\"\n",
    "    if cy < margin:\n",
    "        return \"top\"\n",
    "    if cy > (H - margin):\n",
    "        return \"bottom\"\n",
    "\n",
    "    return \"center\"\n",
    "\n",
    "\n",
    "\n",
    "def save_slice(patient_id, slice_idx, brain_slice, mask_slice):\n",
    "    \"\"\"\n",
    "    Saves .npy files exactly like your current pipeline\n",
    "    \"\"\"\n",
    "    # ensure dirs\n",
    "    os.makedirs(OUTPUT_NPY_DIR, exist_ok=True)\n",
    "    os.makedirs(OUTPUT_PNG_DIR, exist_ok=True)\n",
    "\n",
    "    base = f\"{patient_id:03d}_{slice_idx}\"\n",
    "\n",
    "    np.save(os.path.join(OUTPUT_NPY_DIR, base + \".npy\"), brain_slice)\n",
    "    np.save(os.path.join(OUTPUT_NPY_DIR, base + \"_mask.npy\"), mask_slice)\n",
    "\n",
    "    # Optional PNGs (comment out if not needed)\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.imsave(os.path.join(OUTPUT_PNG_DIR, base + \".png\"), brain_slice, cmap=\"gray\")\n",
    "    plt.imsave(os.path.join(OUTPUT_PNG_DIR, base + \"_mask.png\"), mask_slice, cmap=\"hot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b65db0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_csv_simple(num_slices_per_patient=3, patient_range=range(1, 370)):\n",
    "    \"\"\"\n",
    "    For each patient, pick slices based on tumor size:\n",
    "        - Largest tumor slice\n",
    "        - 65th percentile\n",
    "        - 25th percentile\n",
    "    Writes ONLY a CSV (no npy, no png).\n",
    "    \"\"\"\n",
    "\n",
    "    # Start fresh\n",
    "    with open(OUTPUT_CSV, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Patient\", \"SliceIndex\"])\n",
    "\n",
    "    print(\"Building simple, size-based dataset...\")\n",
    "\n",
    "    for pid in patient_range:\n",
    "        img_path, mask_path = get_paths(pid)\n",
    "        print(img_path, mask_path)\n",
    "        if not os.path.exists(img_path) or not os.path.exists(mask_path):\n",
    "            continue\n",
    "        try:\n",
    "            brain, mask = load_volume(img_path, mask_path)\n",
    "        except:\n",
    "            print(f\"Skipping patient {pid:03d} (load error)\")\n",
    "            continue\n",
    "\n",
    "        depth = brain.shape[2]\n",
    "\n",
    "        # Compute tumor size for each slice\n",
    "        sizes = []\n",
    "        for s in range(depth):\n",
    "            mask_slice = mask[:, :, s]\n",
    "            tumor_pixels = int(np.sum(np.isin(mask_slice, TUMOR_LABELS)))\n",
    "            if tumor_pixels > 0:       # Ensure there's tumor\n",
    "                sizes.append((s, tumor_pixels))\n",
    "\n",
    "        if len(sizes) < 3:\n",
    "            continue  # not enough tumor slices → skip patient\n",
    "\n",
    "        # Sort by tumor size descending\n",
    "        sizes.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        indexes = [sizes[0][0]]  # largest slice\n",
    "\n",
    "        if len(sizes) >= 3:\n",
    "            idx_65 = sizes[int(0.65 * (len(sizes) - 1))][0]\n",
    "            idx_25 = sizes[int(0.25 * (len(sizes) - 1))][0]\n",
    "            indexes.extend([idx_65, idx_25])\n",
    "\n",
    "        # Optional: Add a location-diversity slice\n",
    "        if num_slices_per_patient == 4:\n",
    "            # Find slice with centroid far from largest slice's centroid\n",
    "            def centroid(mask_slice):\n",
    "                ys, xs = np.where(np.isin(mask_slice, TUMOR_LABELS))\n",
    "                return (np.mean(xs), np.mean(ys))\n",
    "\n",
    "            first_centroid = centroid(mask[:, :, sizes[0][0]])\n",
    "\n",
    "            distances = []\n",
    "            for s, _ in sizes:\n",
    "                c = centroid(mask[:, :, s])\n",
    "                d = np.linalg.norm(np.array(c) - np.array(first_centroid))\n",
    "                distances.append((d, s))\n",
    "\n",
    "            # pick slice with farthest centroid\n",
    "            farthest_slice = max(distances)[1]\n",
    "            indexes.append(farthest_slice)\n",
    "\n",
    "        # Write to CSV\n",
    "        with open(OUTPUT_CSV, \"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            for s in indexes[:num_slices_per_patient]:\n",
    "                writer.writerow([f\"{pid:03d}\", s])\n",
    "\n",
    "        del brain, mask\n",
    "\n",
    "    print(f\"\\nDone. CSV written to {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a958fce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_from_csv(csv_path, out_npy=\"training_set_npy\", out_png=\"training_set_png\"):\n",
    "    os.makedirs(out_npy, exist_ok=True)\n",
    "    os.makedirs(out_png, exist_ok=True)\n",
    "\n",
    "    with open(csv_path, \"r\") as f:\n",
    "        rows = list(csv.DictReader(f))\n",
    "\n",
    "    patient_slices = {}\n",
    "    for r in rows:\n",
    "        pid = int(r[\"Patient\"])\n",
    "        s = int(r[\"SliceIndex\"])\n",
    "        patient_slices.setdefault(pid, []).append(s)\n",
    "\n",
    "    print(\"Exporting slices...\")\n",
    "\n",
    "    for pid, slices in patient_slices.items():\n",
    "        img_path, mask_path = get_paths(pid)\n",
    "        brain, mask = load_volume(img_path, mask_path)\n",
    "\n",
    "        for s in slices:\n",
    "            base = f\"{pid:03d}_{s}\"\n",
    "            np.save(os.path.join(out_npy, base + \".npy\"), brain[:, :, s])\n",
    "            np.save(os.path.join(out_npy, base + \"_mask.npy\"), mask[:, :, s])\n",
    "\n",
    "            plt.imsave(os.path.join(out_png, base + \".png\"), brain[:, :, s], cmap=\"gray\")\n",
    "            plt.imsave(os.path.join(out_png, base + \"_mask.png\"), mask[:, :, s], cmap=\"hot\")\n",
    "\n",
    "        del brain, mask\n",
    "\n",
    "    print(\"Finished exporting.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b65702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRA_TS_ROOT = \"/mnt/d/MICCAI_BraTS_2020/BraTS20_Training/BraTS20_Training_\"\n",
    "IMAGE_TYPE = \"t1ce\"\n",
    "MASK_TYPE = \"seg\"\n",
    "EXT = \".nii\"\n",
    "\n",
    "OUTPUT_NPY_DIR = \"training_set_npy\"\n",
    "OUTPUT_PNG_DIR = \"training_set_png\"\n",
    "OUTPUT_CSV = \"SET.csv\"\n",
    "\n",
    "# Tumor core = labels 1 and 4\n",
    "TUMOR_LABELS = [1, 4]\n",
    "\n",
    "MIN_TUMOR_PERCENT = 0.005   # 0.5%\n",
    "build_csv_simple(num_slices_per_patient=3, patient_range=range(1, 370))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90958be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after CSV is built:\n",
    "df = pd.read_csv(\"SET.csv\", dtype=str)\n",
    "# sample slices THAT ARE NOT IN testing_set.csv but keep in order\n",
    "# ENSURE THAT PATIENTS HAVE IDs OF 3 DIGITS USE :03d\n",
    "\n",
    "df_testing = pd.read_csv(\"testing_set.csv\", dtype=str)\n",
    "\n",
    "pad = lambda x: f\"{int(x):03d}\"\n",
    "\n",
    "for d in [df, df_testing]:\n",
    "    d[\"Patient\"] = d[\"Patient\"].map(pad)\n",
    "\n",
    "df = df[~df.set_index([\"Patient\", \"SliceIndex\"]).index.isin(df_testing.set_index([\"Patient\", \"SliceIndex\"]).index)]\n",
    "df.to_csv(f\"SET_no_testing_overlap.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051ece16",
   "metadata": {},
   "source": [
    "## Run this to create a new dataset without testing overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06860457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling done. 200 slices saved to training_set_200.csv\n"
     ]
    }
   ],
   "source": [
    "n = 200\n",
    "\n",
    "df = pd.read_csv(\"SET_no_testing_overlap.csv\", dtype=str)\n",
    "df = df.sample(n=n, random_state=42).sort_values(by=[\"Patient\", \"SliceIndex\"])\n",
    "\n",
    "df.to_csv(f\"training_set_{n}.csv\", index=False)\n",
    "print(f\"Sampling done. {n} slices saved to training_set_{n}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ac6033",
   "metadata": {},
   "source": [
    "## Run to create npy and pgn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "761f86e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global min/max: 0.0 17114.0\n",
      "After normalization: 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "folder = \"/home/martina/codi2/4year/tfg/training_set_200_npy\"\n",
    "\n",
    "# Compute global min/max using only IMAGE files (exclude masks)\n",
    "image_paths = [p for p in glob.glob(f\"{folder}/*.npy\") if \"_mask\" not in p]\n",
    "\n",
    "all_min = []\n",
    "all_max = []\n",
    "\n",
    "for path in image_paths:\n",
    "    img = np.load(path)\n",
    "    all_min.append(img.min())\n",
    "    all_max.append(img.max())\n",
    "\n",
    "global_min = min(all_min)\n",
    "global_max = max(all_max)\n",
    "\n",
    "print(\"Global min/max:\", global_min, global_max)\n",
    "\n",
    "# Normalize all images\n",
    "for path in image_paths:\n",
    "    img = np.load(path).astype(np.float32)\n",
    "    \n",
    "    img_norm = (img - global_min) / (global_max - global_min + 1e-8)\n",
    "    img_norm = np.clip(img_norm, 0, 1)\n",
    "    \n",
    "    np.save(path, img_norm)\n",
    "\n",
    "# Verify\n",
    "all_min_after = []\n",
    "all_max_after = []\n",
    "\n",
    "for path in image_paths:\n",
    "    img = np.load(path)\n",
    "    all_min_after.append(img.min())\n",
    "    all_max_after.append(img.max())\n",
    "\n",
    "print(\"After normalization:\", min(all_min_after), max(all_max_after))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef613754",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a39245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 17114.0\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc3cd80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
