{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cb7e8c3",
   "metadata": {},
   "source": [
    "# Notebook to experiment with training:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bc7211",
   "metadata": {},
   "source": [
    "## Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13a2d983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import torch\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "import wandb\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import torch.nn as nn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a06597a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glioblastoma(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 4} \n",
    "    # The metadata of the environment, e.g. {“render_modes”: [“rgb_array”, “human”], “render_fps”: 30}. \n",
    "    # For Jax or Torch, this can be indicated to users with “jax”=True or “torch”=True.\n",
    "\n",
    "    def __init__(self, image_path, mask_path, grid_size=4, tumor_threshold=0.0001, render_mode=\"human\"): # cosntructor with the brain image, the mask and a size\n",
    "        super().__init__() # parent class\n",
    "        \n",
    "        self.image = np.load(image_path).astype(np.float32)\n",
    "        self.mask = np.load(mask_path).astype(np.uint8)\n",
    "        \n",
    "        img_min, img_max = self.image.min(), self.image.max()\n",
    "        if img_max > 1.0:  # only normalize if not already in [0, 1]\n",
    "            self.image = (self.image - img_min) / (img_max - img_min + 1e-8) #avoid division by 0\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.block_size = self.image.shape[0] // grid_size  # 240/4 = 60\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Define action and observation spaces\n",
    "        # Actions: 0 = stay, 1 = move down, 2 = move right\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        # Observations: grayscale patch (normalized 0-1)\n",
    "        # apparently Neural networks train better when inputs are scaled to small, \n",
    "        # consistent ranges rather than raw 0–255 values.\n",
    "        self.observation_space = spaces.Box( # Supports continuous (and discrete) vectors or matrices\n",
    "            low=0, high=1, # Data has been normalized\n",
    "            shape=(self.block_size, self.block_size), # shape of the observation\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.agent_pos = [0, 0] # INITIAL POSITION AT TOP LEFT\n",
    "        self.current_step = 0 # initialize counter\n",
    "        self.max_steps = 20  # like in the paper\n",
    "\n",
    "        self.tumor_threshold = tumor_threshold # 15% of the patch must be tumor to consider that the agent is inside the tumor region\n",
    "\n",
    "    def reset(self, seed=None, options=None): # new episode where we initialize the state. \n",
    "        super().reset(seed=seed) # parent\n",
    "        \n",
    "        # reset\n",
    "        self.agent_pos = [0, 0]  # top-left corner\n",
    "        self.current_step = 0\n",
    "        obs = self._get_obs()\n",
    "        info = {}\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "\n",
    "        prev_pos = self.agent_pos.copy() # for reward computation taking into consideration the transition changes\n",
    "        \n",
    "        # Apply action (respect grid boundaries)\n",
    "        if action == 1 and self.agent_pos[0] < self.grid_size - 1:\n",
    "            self.agent_pos[0] += 1  # move down\n",
    "        elif action == 2 and self.agent_pos[1] < self.grid_size - 1:\n",
    "            self.agent_pos[1] += 1  # move right\n",
    "        # else, the agent doesn't move so the observation \n",
    "        # and reward will be calculated from the same position\n",
    "        # no need to compute self.agent_po\n",
    "\n",
    "        reward = self._get_reward(action, prev_pos)\n",
    "                \n",
    "        obs = self._get_obs()\n",
    "\n",
    "        # Episode ends\n",
    "        terminated = self.current_step >= self.max_steps\n",
    "        truncated = False  # we don’t need truncation here\n",
    "        info = {}\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        r0 = self.agent_pos[0] * self.block_size # row start\n",
    "        c0 = self.agent_pos[1] * self.block_size # col start\n",
    "        \n",
    "        patch = self.image[r0:r0+self.block_size, c0:c0+self.block_size].astype(np.float32)\n",
    "\n",
    "        # if patch.max() == 0: # DEBUGGING\n",
    "        #     print(\"Warning: extracted patch has max value 0 at position:\", self.agent_pos)\n",
    "        # else:\n",
    "        #     print(\"Brain\")\n",
    "        return patch\n",
    "\n",
    "    def _get_reward(self, action, prev_pos):        \n",
    "        # look position of the agent in the mask\n",
    "        r0 = self.agent_pos[0] * self.block_size\n",
    "        c0 = self.agent_pos[1] * self.block_size\n",
    "        patch_mask = self.mask[r0:r0+self.block_size, c0:c0+self.block_size]\n",
    "        \n",
    "        # Now that i have the patch where i was and the patch where i am, i can check if there is tumor in any of them\n",
    "        # tumor is labeled as 1 or 4 in the mask        \n",
    "        # label 2 is edema\n",
    "        \n",
    "        # first get a count of the tumor pixels in the patch. \n",
    "        tumor_count_curr = np.sum(np.isin(patch_mask, [1, 4]))\n",
    "        total = self.block_size * self.block_size # to compute the percentage\n",
    "        # Determine if patch has more than self.tumor_threshold of tumor\n",
    "        inside = (tumor_count_curr / total) >= self.tumor_threshold\n",
    "        \n",
    "        if inside:\n",
    "            return 10.0  # reward for being on tumor or staying on tumor\n",
    "        else:\n",
    "            if action == 0 or prev_pos == self.agent_pos:  # stayed in place but no tumor. we are also taking into consideration that if the action was to move but we are at the edge of the grid, we also stay in place\n",
    "                return -2.0\n",
    "            else:\n",
    "                return -0.5  # moved but no tumor\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode != \"human\": # would be rgb_array or ansi\n",
    "            return  # Only render in human mode\n",
    "\n",
    "        # Create RGB visualization image\n",
    "        # not necessary since it's grayscale, but i want to draw the mask and position\n",
    "        vis_img = np.stack([self.image] * 3, axis=-1).astype(np.float32)\n",
    "\n",
    "        # Overlay tumor mask in red [..., 0] \n",
    "        tumor_overlay = np.zeros_like(vis_img) # do all blank but here we have 3 channels, mask is 2D\n",
    "        tumor_overlay[..., 0] = (self.mask > 0).astype(float) # red channel. set to float to avoid issues when blending in vis_img\n",
    "\n",
    "        # transparency overlay (crec que es el mateix valor que tinc a l'altra notebook)\n",
    "        alpha = 0.4\n",
    "        vis_img = (1 - alpha) * vis_img + alpha * tumor_overlay\n",
    "\n",
    "        # Plotting\n",
    "        fig, ax = plt.subplots(figsize=(3, 3))\n",
    "        ax.imshow(vis_img, cmap='gray', origin='upper')\n",
    "\n",
    "        # Draw grid lines\n",
    "        # alpha for transparency again\n",
    "        for i in range(1, self.grid_size):\n",
    "            ax.axhline(i * self.block_size, color='white', lw=1, alpha=0.5)\n",
    "            ax.axvline(i * self.block_size, color='white', lw=1, alpha=0.5)\n",
    "\n",
    "        # Draw agent position\n",
    "        r0 = self.agent_pos[0] * self.block_size\n",
    "        c0 = self.agent_pos[1] * self.block_size\n",
    "        rect = patches.Rectangle(\n",
    "            (c0, r0), # (x,y) bottom left corner\n",
    "            self.block_size, # width\n",
    "            self.block_size, # height\n",
    "            linewidth=2,\n",
    "            edgecolor='yellow',\n",
    "            facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        ax.set_title(f\"Agent at {self.agent_pos} | Step {self.current_step}\")\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def current_patch_overlap_with_lesion(self): # FALTAAA chat\n",
    "        \"\"\" Returns the number of overlapping lesion pixels between the agent's current patch and the ground-truth mask. If > 0, the agent is correctly over the lesion (TP). \"\"\"\n",
    "        # get current agent patch boundaries\n",
    "        row, col = self.agent_pos\n",
    "        patch_h = self.block_size # not grid_size because grid_size is number of patches per side\n",
    "        patch_w = self.block_size\n",
    "        \n",
    "        y0 = row * patch_h\n",
    "        y1 = y0 + patch_h\n",
    "        x0 = col * patch_w\n",
    "        x1 = x0 + patch_w\n",
    "        # extract mask region under current patch\n",
    "        patch_mask = self.mask[y0:y1, x0:x1]\n",
    "        # count how many pixels of lesion (nonzero)\n",
    "        overlap = np.sum(patch_mask > 0)\n",
    "        return overlap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8d671b",
   "metadata": {},
   "source": [
    "# DQN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90c34bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, env, learning_rate=1e-3, device='cpu'):\n",
    "        super(DQN, self).__init__()\n",
    "        self.device = device\n",
    "        self.n_inputs = env.observation_space.shape[0] # 60\n",
    "        self.n_outputs = env.action_space.n # 3\n",
    "        self.actions = np.arange(env.action_space.n) # np.array([0, 1, 2])\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        input_channels = 1\n",
    "        height, width = env.observation_space.shape  # 60, 60   \n",
    "        \n",
    "        ### Construction of the neural network\n",
    "        ## features first and then fully connected layers\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        \n",
    "        # flatten \n",
    "        with  torch.no_grad(): # FALTA MIRAR Q ES AIXO\n",
    "            dummy_input = torch.zeros(1, input_channels, height, width) # batch size 1\n",
    "            n_flatten = self.features(dummy_input).view(1, -1).size(1)\n",
    "            \n",
    "        # nn.Linear (in features, out features)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_flatten, 512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, self.n_outputs)\n",
    "        )\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        ### Work with CUDA is allowed\n",
    "        if self.device == 'cuda':\n",
    "            self.to(self.device).cuda()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, 1, 60, 60)\n",
    "        features = self.features(x)\n",
    "        features_flat = features.view(x.size(0), -1)\n",
    "        q_values = self.fc(features_flat)\n",
    "        return q_values\n",
    "    \n",
    "    # e-greedy method\n",
    "    def get_action(self, state, epsilon=0.05):\n",
    "        if np.random.random() < epsilon:\n",
    "            # random action -- Exploration\n",
    "            action = np.random.choice(self.actions)  \n",
    "        else:\n",
    "            # Q-value based action -- Exploitation\n",
    "            qvals = self.get_qvals(state)  \n",
    "            if qvals.dim() == 2 and qvals.size(0) == 1:\n",
    "                action = torch.argmax(qvals, dim=-1).item()\n",
    "            else:\n",
    "                action = torch.argmax(qvals, dim=-1)[0].item()\n",
    "\n",
    "        return int(action)\n",
    "    \n",
    "    # forward pass through conv and fc layers\n",
    "    def get_qvals(self, state):\n",
    "        # Convert (60,60) → (1,1,60,60)\n",
    "        if isinstance(state, np.ndarray):\n",
    "            if state.ndim == 2:  # grayscale single image (60x60)\n",
    "                state = np.expand_dims(np.expand_dims(state, 0), 0) # (1,1,60,60)\n",
    "                \n",
    "            elif state.ndim == 3:  # batch or stacked images (batch, 60, 60)\n",
    "                if state.shape[0] != 1: #batch\n",
    "                    state = np.expand_dims(state, 1)\n",
    "                    \n",
    "        state_t = torch.FloatTensor(state).to(self.device)\n",
    "        qvals = self.forward(state_t)  # Use the forward method instead\n",
    "        return qvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3283f892",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self, env, dnnetwork, ReplayBuffer, train_pairs, epsilon=0.1, eps_decay=0.99, epsilon_min=0.01, batch_size=32, gamma=0.99):\n",
    "        self.env = env\n",
    "        self.dnnetwork = dnnetwork # main network\n",
    "        self.target_network = deepcopy(dnnetwork) # prevents the target Q-values from changing with every single update\n",
    "        self.target_network.optimizer = None # paper said target net is only  weights, no optimizer\n",
    "        # self.buffer = buffer # store experiences\n",
    "        self.epsilon = epsilon # initial epsilon for e-greedy\n",
    "        self.eps_decay = eps_decay # decay of epsilon after each episode to balance exploration and exploitation\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size # size of the mini-batch for training\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # block of the last X episodes to calculate the average reward \n",
    "        self.nblock = 100 \n",
    "        \n",
    "        #create buffers for each training image\n",
    "        self.buffers = {\n",
    "            img_path: ReplayBuffer(capacity=MEMORY_SIZE)\n",
    "            for img_path, _ in train_pairs\n",
    "        }\n",
    "\n",
    "        \n",
    "        # average reward used to determine if the agent has learned to play\n",
    "        #self.reward_threshold = self.env.spec.reward_threshold \n",
    "        self.initialize()\n",
    "    \n",
    "    def initialize(self): # reset variables at the beginning of training\n",
    "        self.update_loss = []\n",
    "        self.training_rewards = []\n",
    "        self.mean_training_rewards = []\n",
    "        self.sync_eps = []\n",
    "        self.total_reward = 0\n",
    "        self.step_count = 0\n",
    "        self.state0 = self.env.reset()[0]\n",
    "        \n",
    "    ## Take new action\n",
    "    def take_step(self, eps, mode='train'):\n",
    "        if mode == 'explore':\n",
    "            # random action in burn-in and in the exploration phase (epsilon)\n",
    "            action = self.env.action_space.sample() \n",
    "        else:\n",
    "            # Action based on the Q-value (max Q-value)\n",
    "            action = self.dnnetwork.get_action(self.state0, eps)\n",
    "            self.step_count += 1\n",
    "            \n",
    "        # Execute action and get reward and new state\n",
    "        new_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "        done = terminated or truncated\n",
    "        self.total_reward += reward\n",
    "        # save experience in the buffer\n",
    "        self.buffers[self.current_img].append(self.state0, action, reward, done, new_state)\n",
    "        self.state0 = new_state.copy()\n",
    "        \n",
    "        if done:\n",
    "            self.state0 = self.env.reset()[0]\n",
    "        return done, reward # THE REWARD RETURN IS FOR DEBUGGING \n",
    "\n",
    "            \n",
    "    ## Training\n",
    "    def train(self, train_pairs, gamma=0.99, max_episodes=50000, \n",
    "              dnn_update_frequency=4,\n",
    "              dnn_sync_frequency=200):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Fill the buffer with N random experiences\n",
    "        print(\"Filling replay buffer...\")\n",
    "        for img_path, mask_path in train_pairs: # fill each buffer\n",
    "            self.current_img = img_path\n",
    "            self.env = Glioblastoma(img_path, mask_path, \n",
    "                                    grid_size=self.env.grid_size, \n",
    "                                    tumor_threshold=self.env.tumor_threshold)\n",
    "            self.state0, _ = self.env.reset()\n",
    "\n",
    "            #run short episode of 20 random steps on this image\n",
    "            for _ in range (500): # trial 500 x 30 = 15000 experiences per buffer\n",
    "                self.take_step(self.epsilon, mode='explore')\n",
    "\n",
    "            \n",
    "        # Store metrics locally to plot\n",
    "        self.episode_rewards = []\n",
    "        self.mean_rewards = []\n",
    "        self.epsilon_values = []\n",
    "        self.loss_values = []\n",
    " \n",
    "        episode = 0\n",
    "        training = True\n",
    "        print(\"Training...\")\n",
    "        while training and episode < max_episodes:\n",
    "            img_path, mask_path = random.choice(train_pairs)\n",
    "            self.current_img = img_path\n",
    "            print(f\"[Episode {episode}] Using image: {os.path.basename(img_path)}\") # debugging\n",
    "\n",
    "            self.env = Glioblastoma(img_path, mask_path, grid_size=self.env.grid_size, tumor_threshold=self.env.tumor_threshold)\n",
    "            self.state0, _ = self.env.reset()\n",
    "            self.total_reward = 0\n",
    "            \n",
    "            # DEBUGGING\n",
    "            pos_rewards = 0\n",
    "            neg_rewards = 0\n",
    "            \n",
    "            for step in range(self.env.max_steps):\n",
    "                gamedone, reward = self.take_step(self.epsilon, mode='train') # THE REWARD RETURN IS FOR DEBUGGING\n",
    "\n",
    "                # DEBUGGING\n",
    "                if reward > 0: pos_rewards += 1\n",
    "                else: neg_rewards += 1\n",
    "                # END OF DEBUGGING\n",
    "                \n",
    "                \n",
    "                # Upgrade main network\n",
    "                if self.step_count % dnn_update_frequency == 0:\n",
    "                    self.update()\n",
    "                # Synchronize the main network and the target network\n",
    "                if self.step_count % dnn_sync_frequency == 0:\n",
    "                    self.target_network.load_state_dict(\n",
    "                        self.dnnetwork.state_dict())\n",
    "                    self.sync_eps.append(episode)\n",
    "                    \n",
    "                if gamedone:\n",
    "                    episode += 1                   \n",
    "                    # Save the rewards\n",
    "                    self.training_rewards.append(self.total_reward)\n",
    "                    # Calculate the average reward for the last X episodes\n",
    "                    if len(self.training_rewards) >= self.nblock:\n",
    "                        mean_rewards = np.mean(self.training_rewards[-self.nblock:])\n",
    "                    else:\n",
    "                        mean_rewards = np.mean(self.training_rewards)  # Use all rewards if less than nblock\n",
    "                    \n",
    "                    self.mean_training_rewards.append(mean_rewards)\n",
    "\n",
    "                    print(\"Episode {:d} Mean Rewards {:.2f} Epsilon {} Loss {}\".format(episode, mean_rewards, self.epsilon, np.mean(self.update_loss)))\n",
    "                    print(f\"   Positive rewards: {pos_rewards}, Negative rewards: {neg_rewards}\") # DEBUGGING\n",
    "                    \n",
    "                    wandb.log({\n",
    "                        'episode': episode,\n",
    "                        'mean_rewards': mean_rewards,\n",
    "                        'episode reward': self.total_reward,\n",
    "                        'epsilon': self.epsilon,\n",
    "                        'loss': np.mean(self.update_loss)\n",
    "                    }, step=episode)\n",
    "                    \n",
    "                    # Append metrics to lists for plotting\n",
    "                    self.episode_rewards.append(self.total_reward)\n",
    "                    self.mean_rewards.append(mean_rewards)\n",
    "                    self.epsilon_values.append(self.epsilon)\n",
    "                    self.loss_values.append(np.mean(self.update_loss))\n",
    "                    \n",
    "                    self.update_loss = []\n",
    "\n",
    "                    # Check if there are still episodes left\n",
    "                    if episode >= max_episodes:\n",
    "                        training = False\n",
    "                        print('\\nEpisode limit reached.')\n",
    "                        break\n",
    "                    \n",
    "                    self.epsilon = max(self.epsilon * self.eps_decay, self.epsilon_min)\n",
    "                    # Update epsilon according to the fixed decay rate # SUBTRACTION FROM PPAPER\n",
    "                    # self.epsilon = max(self.epsilon - self.eps_decay, self.epsilon_min) \n",
    "                    torch.save(self.dnnetwork.state_dict(), \"Glioblastoma\" + \".dat\")\n",
    "                    \n",
    "        # PLOTTING\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(10, 8))  # Create a 2x2 grid of subplots\n",
    "        axes = axes.ravel()  # Flatten the axes array for easier indexing\n",
    "\n",
    "        # Plot episode rewards\n",
    "        axes[0].plot(self.episode_rewards)\n",
    "        axes[0].set_xlabel('Episode')\n",
    "        axes[0].set_ylabel('Episode Reward')\n",
    "        axes[0].set_title('Episode Rewards Over Time')\n",
    "\n",
    "        # Plot mean rewards\n",
    "        axes[1].plot(self.mean_rewards)\n",
    "        axes[1].set_xlabel('Episode')\n",
    "        axes[1].set_ylabel('Mean Reward')\n",
    "        axes[1].set_title('Mean Rewards Over Time')\n",
    "\n",
    "        # Plot epsilon values\n",
    "        axes[2].plot(self.epsilon_values)\n",
    "        axes[2].set_xlabel('Episode')\n",
    "        axes[2].set_ylabel('Epsilon Values')\n",
    "        axes[2].set_title('Epsilon Values Over Time')\n",
    "\n",
    "        # Plot loss values\n",
    "        axes[3].plot(self.loss_values)\n",
    "        axes[3].set_xlabel('Episode')\n",
    "        axes[3].set_ylabel('Loss')\n",
    "        axes[3].set_title('Loss Over Time')\n",
    "\n",
    "        # Adjust layout (optional)\n",
    "        # fig.suptitle('Training Performance', fontsize=16)  # Add a main title\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust spacing between subplots and title\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # Loss calculation           \n",
    "    def calculate_loss(self, batch):\n",
    "        # Separate the variables of the experience and convert them to tensors\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "        device = self.dnnetwork.device\n",
    "\n",
    "        # Add channel dimension # FALTAA\n",
    "        states = torch.FloatTensor(states).unsqueeze(1).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).unsqueeze(1).to(device)\n",
    "\n",
    "        rewards_vals = torch.FloatTensor(rewards).to(device=device) \n",
    "        actions_vals = torch.LongTensor(np.array(actions)).reshape(-1,1).to(device=device)\n",
    "        dones_t = torch.BoolTensor(dones).to(device=device)\n",
    "        \n",
    "        # Obtain the Q values of the main network\n",
    "        qvals = torch.gather(self.dnnetwork.get_qvals(states), 1, actions_vals)\n",
    "        \n",
    "        # Obtain the target Q values.\n",
    "        # The detach() parameter prevents these values from updating the target network\n",
    "        qvals_next_all = self.target_network.get_qvals(next_states)  # Shape: [batch_size, n_actions]\n",
    "        qvals_next = torch.max(qvals_next_all, dim=1)[0].detach()    # Shape: [batch_size]\n",
    "\n",
    "        # 0 in terminal states\n",
    "        qvals_next[dones_t] = 0.0 \n",
    "        \n",
    "        # print(\"qvals_next.shape\", qvals_next.shape, \"dones_t.shape\", dones_t.shape) # debugging\n",
    "        \n",
    "        # Calculate the Bellman equation\n",
    "        expected_qvals = (self.gamma * qvals_next) + rewards_vals\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = torch.nn.MSELoss()(qvals, expected_qvals.reshape(-1,1))\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def update(self, num_buffers=4):        \n",
    "        # Remove any gradient\n",
    "        self.dnnetwork.optimizer.zero_grad()  \n",
    "        # Select a subset from the buffer\n",
    "        all_imgs = list(self.buffers.keys())\n",
    "        k = min(num_buffers, len(all_imgs))\n",
    "        selected_imgs = random.sample(all_imgs, k)\n",
    "\n",
    "        per_buf = max(1, self.batch_size // k)\n",
    "        combined = []\n",
    "        for img in selected_imgs:\n",
    "            buf = self.buffers[img]\n",
    "            if len(buf.buffer) < per_buf:\n",
    "                # fallback: sample whatever available or skip\n",
    "                continue\n",
    "            states, actions, rewards, dones, next_states = buf.sample_batch(per_buf)\n",
    "            for s,a,r,d,ns in zip(states, actions, rewards, dones, next_states):\n",
    "                combined.append((s,a,r,d,ns))\n",
    "\n",
    "        if len(combined) < self.batch_size:\n",
    "            # if not enough from selected buffers, sample rest from random buffers\n",
    "            while len(combined) < self.batch_size:\n",
    "                img = random.choice(all_imgs)\n",
    "                buf = self.buffers[img]\n",
    "                if len(buf.buffer) < 1:\n",
    "                    continue\n",
    "                s,a,r,d,ns = random.choice(buf.buffer)\n",
    "                combined.append((s,a,r,d,ns))\n",
    "\n",
    "        # build batch arrays\n",
    "        states, actions, rewards, dones, next_states = zip(*combined[:self.batch_size])\n",
    "        batch = (\n",
    "            np.array(states, dtype=np.float32),\n",
    "            np.array(actions, dtype=np.int64),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.bool_),\n",
    "            np.array(next_states, dtype=np.float32),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = self.calculate_loss(batch) \n",
    "        # Difference to get the gradients\n",
    "        loss.backward() \n",
    "        # add gradient clipping (FALTAAA)\n",
    "        torch.nn.utils.clip_grad_norm_(self.dnnetwork.parameters(), max_norm=1.0)\n",
    "        # Apply the gradients to the neural network\n",
    "        self.dnnetwork.optimizer.step() \n",
    "        \n",
    "        # Save loss values\n",
    "        if self.dnnetwork.device == 'cuda':\n",
    "            self.update_loss.append(loss.detach().cpu().numpy())\n",
    "        else:\n",
    "            self.update_loss.append(loss.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89a78476",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        self.buffer.append((np.array(state, dtype=np.float32), int(action), float(reward), bool(done), np.array(next_state, dtype=np.float32)))\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, dones, next_states = zip(*batch)\n",
    "        return (\n",
    "            np.array(states, dtype=np.float32),\n",
    "            np.array(actions, dtype=np.int64),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.bool_),\n",
    "            np.array(next_states, dtype=np.float32),\n",
    "        )\n",
    "\n",
    "    def burn_in_capacity(self):\n",
    "        return len(self.buffer) / self.capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18b21fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 30 testing pairs out of 30 listed in CSV.\n",
      "test_pairs example: [('/home/martina/codi2/4year/tfg/testing_set_npy/002_58.npy', '/home/martina/codi2/4year/tfg/testing_set_npy/002_58_mask.npy')]\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"/home/martina/codi2/4year/tfg/testing_set_npy\"\n",
    "csv_path = \"/home/martina/codi2/4year/tfg/testing_dataset_slices.csv\"\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Construct image and mask filenames\n",
    "df[\"image_path\"] = df.apply(\n",
    "    lambda row: os.path.join(base_dir, f\"{row['Patient']:03d}_{row['SliceIndex']}.npy\"), axis=1\n",
    ")\n",
    "df[\"mask_path\"] = df.apply(\n",
    "    lambda row: os.path.join(base_dir, f\"{row['Patient']:03d}_{row['SliceIndex']}_mask.npy\"), axis=1\n",
    ")\n",
    "\n",
    "# Sanity check (optional)\n",
    "test_pairs = [\n",
    "    (img, mask)\n",
    "    for img, mask in zip(df[\"image_path\"], df[\"mask_path\"])\n",
    "    if os.path.exists(img) and os.path.exists(mask)\n",
    "]\n",
    "\n",
    "print(f\"✅ Found {len(test_pairs)} testing pairs out of {len(df)} listed in CSV.\")\n",
    "print(f\"test_pairs example:\", test_pairs[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89efd4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4 #From paper\n",
    "MEMORY_SIZE = 15000 #From paper\n",
    "MAX_EPISODES = 100 #From paper\n",
    "\n",
    "\n",
    "EPSILON = 1.0 #From paper\n",
    "EPSILON_MIN = 1e-4 #From paper\n",
    "EPSILON_DECAY = 0.6 #Let's try exponential decay\n",
    "# EPSILON_DECAY = (EPSILON - EPSILON_MIN) / MAX_EPISODES \n",
    "# print(f\"Starting at {EPSILON}, decaying {EPSILON_DECAY}, will reach {EPSILON_MIN} after {MAX_EPISODES} episodes\")\n",
    "\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 128 #From paper\n",
    "BURN_IN = 1000\n",
    "DNN_UPD = 1\n",
    "DNN_SYNC = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4461e489",
   "metadata": {},
   "source": [
    "# TESTING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdeda815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent_current(agent, test_pairs, num_episodes=10):\n",
    "    \"\"\"\n",
    "    Test the trained agent using the current environment setup\n",
    "    without any modifications to reward system or early termination\n",
    "    \"\"\"\n",
    "    agent.dnnetwork.eval()  # Set to evaluation mode\n",
    "    \n",
    "    metrics = {\n",
    "        'success_rate': [],\n",
    "        'final_position_accuracy': [],\n",
    "        'average_reward': [],\n",
    "        'steps_to_find_tumor': [],\n",
    "        'tumor_coverage': [],\n",
    "        'total_tumor_reward': []\n",
    "    }\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        img_path, mask_path = test_pairs[i]\n",
    "        env = Glioblastoma(img_path, mask_path, grid_size=4)\n",
    "        \n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        found_tumor = False\n",
    "        tumor_positions_visited = set()\n",
    "        steps_to_find = env.max_steps  # Default: didn't find\n",
    "        tumor_rewards = 0\n",
    "        \n",
    "        for step in range(env.max_steps):\n",
    "            with torch.no_grad():\n",
    "                action = agent.dnnetwork.get_action(state, epsilon=0.00)\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Track tumor-related metrics\n",
    "            current_overlap = env.current_patch_overlap_with_lesion()\n",
    "            if current_overlap > 0:\n",
    "                tumor_positions_visited.add(tuple(env.agent_pos))\n",
    "                if not found_tumor:\n",
    "                    found_tumor = True\n",
    "                    steps_to_find = step + 1\n",
    "                \n",
    "                # Count positive rewards (when on tumor)\n",
    "                if reward > 0:\n",
    "                    tumor_rewards += 1\n",
    "        \n",
    "        # Calculate metrics for this episode\n",
    "        final_overlap = env.current_patch_overlap_with_lesion()\n",
    "        \n",
    "        # Success: ended on tumor region\n",
    "        success = final_overlap > 0\n",
    "        metrics['success_rate'].append(success)\n",
    "        \n",
    "        # Final position accuracy\n",
    "        metrics['final_position_accuracy'].append(final_overlap > 0)\n",
    "        \n",
    "        # Average reward\n",
    "        metrics['average_reward'].append(total_reward)\n",
    "        \n",
    "        # Steps to find tumor\n",
    "        metrics['steps_to_find_tumor'].append(steps_to_find)\n",
    "        \n",
    "        # Tumor coverage (percentage of tumor patches visited)\n",
    "        total_tumor_patches = count_tumor_patches(env)\n",
    "        coverage = len(tumor_positions_visited) / total_tumor_patches if total_tumor_patches > 0 else 0\n",
    "        metrics['tumor_coverage'].append(coverage)\n",
    "        \n",
    "        # Total positive rewards from tumor\n",
    "        metrics['total_tumor_reward'].append(tumor_rewards)\n",
    "    \n",
    "    # Calculate and print final results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TEST RESULTS (Current Model)\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Success Rate: {np.mean(metrics['success_rate'])*100:.2f}%\")\n",
    "    print(f\"Final Position Accuracy: {np.mean(metrics['final_position_accuracy'])*100:.2f}%\")\n",
    "    print(f\"Average Episode Reward: {np.mean(metrics['average_reward']):.2f}\")\n",
    "    print(f\"Average Steps to Find Tumor: {np.mean(metrics['steps_to_find_tumor']):.2f}\")\n",
    "    print(f\"Average Tumor Coverage: {np.mean(metrics['tumor_coverage'])*100:.2f}%\")\n",
    "    print(f\"Average Tumor Rewards per Episode: {np.mean(metrics['total_tumor_reward']):.2f}\")\n",
    "    \n",
    "    # Additional detailed statistics\n",
    "    print(\"\\nDetailed Statistics:\")\n",
    "    print(f\"Best Episode Reward: {np.max(metrics['average_reward']):.2f}\")\n",
    "    print(f\"Worst Episode Reward: {np.min(metrics['average_reward']):.2f}\")\n",
    "    print(f\"Median Steps to Find Tumor: {np.median(metrics['steps_to_find_tumor']):.2f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def count_tumor_patches(env):\n",
    "    \"\"\"Count total number of patches that contain tumor\"\"\"\n",
    "    tumor_patches = 0\n",
    "    original_pos = env.agent_pos.copy()  # Save original position\n",
    "    \n",
    "    for i in range(env.grid_size):\n",
    "        for j in range(env.grid_size):\n",
    "            env.agent_pos = [i, j]\n",
    "            if env.current_patch_overlap_with_lesion() > 0:\n",
    "                tumor_patches += 1\n",
    "    \n",
    "    env.agent_pos = original_pos  # Restore original position\n",
    "    return tumor_patches\n",
    "\n",
    "def visualize_test_episode(agent, img_path, mask_path, episode_num=0):\n",
    "    \"\"\"Visualize a single test episode\"\"\"\n",
    "    env = Glioblastoma(img_path, mask_path, grid_size=4)\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    positions = []\n",
    "    rewards = []\n",
    "    episode_reward = 0\n",
    "    tumor_found = False\n",
    "    \n",
    "    print(f\"\\nVisualizing Test Episode {episode_num}\")\n",
    "    print(\"Image:\", os.path.basename(img_path))\n",
    "    \n",
    "    for step in range(env.max_steps):\n",
    "        with torch.no_grad():\n",
    "            action = agent.dnnetwork.get_action(state, epsilon=0.01)\n",
    "        \n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        \n",
    "        positions.append(env.agent_pos.copy())\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        # Check tumor status\n",
    "        current_overlap = env.current_patch_overlap_with_lesion()\n",
    "        if current_overlap > 0 and not tumor_found:\n",
    "            tumor_found = True\n",
    "            print(f\"  Step {step+1}: Found tumor at position {env.agent_pos}\")\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        # Render every step or at important moments\n",
    "        if step == 0 or tumor_found or step == env.max_steps - 1:\n",
    "            env.render()\n",
    "    \n",
    "    final_overlap = env.current_patch_overlap_with_lesion()\n",
    "    print(f\"Final position: {env.agent_pos}, On tumor: {final_overlap > 0}\")\n",
    "    print(f\"Total reward: {sum(rewards):.2f}\")\n",
    "    \n",
    "    print(\"\\nStep-by-step rewards:\")\n",
    "    for idx, (pos, rew) in enumerate(zip(positions, rewards)):\n",
    "        print(f\"  Step {idx+1}: Position {pos}, Reward {rew}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6597962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights of layer fc.0.weight:\n",
      "tensor([[-0.0103,  0.0046, -0.0161,  ...,  0.0086,  0.0136,  0.0106],\n",
      "        [ 0.0048, -0.0053,  0.0384,  ..., -0.0400, -0.0133, -0.0099],\n",
      "        [-0.0083,  0.0072, -0.0432,  ...,  0.0360, -0.0099, -0.0465],\n",
      "        ...,\n",
      "        [-0.0022,  0.0027, -0.0081,  ..., -0.0125,  0.0228,  0.0104],\n",
      "        [-0.0238, -0.0209,  0.0188,  ..., -0.0117, -0.0254, -0.0529],\n",
      "        [-0.0292,  0.0022,  0.0155,  ..., -0.0205, -0.0261,  0.0468]])\n",
      "Weights of layer fc.0.weight:\n",
      "tensor([[ 0.0457, -0.0255, -0.0067,  ..., -0.0145, -0.0194,  0.0553],\n",
      "        [-0.0144,  0.0111, -0.0237,  ...,  0.0092,  0.0182,  0.0530],\n",
      "        [ 0.0291, -0.0288, -0.0541,  ..., -0.0471, -0.0503, -0.0414],\n",
      "        ...,\n",
      "        [ 0.0490,  0.0193, -0.0301,  ..., -0.0445,  0.0345, -0.0869],\n",
      "        [-0.0245, -0.0035, -0.0140,  ..., -0.0262,  0.0071, -0.0459],\n",
      "        [-0.0112,  0.0424, -0.0333,  ..., -0.0602,  0.0015, -0.0723]])\n"
     ]
    }
   ],
   "source": [
    "# load model to test:\n",
    "model = DQN(Glioblastoma(*test_pairs[0], grid_size=4), learning_rate=LR, device='cpu')\n",
    "model.load_state_dict(torch.load(\"Glioblastoma008_53.dat\"))\n",
    "model2 = DQN(Glioblastoma(*test_pairs[0], grid_size=4), learning_rate=LR, device='cpu')\n",
    "model2.load_state_dict(torch.load(\"Glioblastoma020_56.dat\"))\n",
    "# model2.load_state_dict(torch.load(\"Glioblastoma.dat\"))\n",
    "\n",
    "#print some weights:\n",
    "for name, param in model.named_parameters():\n",
    "    if 'fc' in name and 'weight' in name:\n",
    "        print(f\"Weights of layer {name}:\")\n",
    "        print(param.data)\n",
    "        break  # print only the first fc layer weights\n",
    "\n",
    "for name, param in model2.named_parameters():\n",
    "    if 'fc' in name and 'weight' in name:\n",
    "        print(f\"Weights of layer {name}:\")\n",
    "        print(param.data)\n",
    "        break  # print only the first fc layer weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "698e9f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TEST RESULTS (Current Model)\n",
      "==================================================\n",
      "Success Rate: 53.33%\n",
      "Final Position Accuracy: 53.33%\n",
      "Average Episode Reward: 52.90\n",
      "Average Steps to Find Tumor: 10.50\n",
      "Average Tumor Coverage: 18.44%\n",
      "Average Tumor Rewards per Episode: 7.50\n",
      "\n",
      "Detailed Statistics:\n",
      "Best Episode Reward: 189.50\n",
      "Worst Episode Reward: -37.00\n",
      "Median Steps to Find Tumor: 3.00\n",
      "\n",
      "==================================================\n",
      "TEST RESULTS (Current Model)\n",
      "==================================================\n",
      "Success Rate: 56.67%\n",
      "Final Position Accuracy: 56.67%\n",
      "Average Episode Reward: 65.30\n",
      "Average Steps to Find Tumor: 10.23\n",
      "Average Tumor Coverage: 23.00%\n",
      "Average Tumor Rewards per Episode: 8.47\n",
      "\n",
      "Detailed Statistics:\n",
      "Best Episode Reward: 189.50\n",
      "Worst Episode Reward: -35.50\n",
      "Median Steps to Find Tumor: 3.00\n"
     ]
    }
   ],
   "source": [
    "metrics1 = test_agent_current(DQNAgent(\n",
    "    env=Glioblastoma(*test_pairs[0], grid_size=4),\n",
    "    dnnetwork=model,\n",
    "    ReplayBuffer=ReplayBuffer,\n",
    "    train_pairs=test_pairs,\n",
    "    epsilon=0.00  # very low epsilon for testing\n",
    "), test_pairs, num_episodes=30)\n",
    "\n",
    "metrics2 = test_agent_current(DQNAgent(\n",
    "    env=Glioblastoma(*test_pairs[0], grid_size=4),\n",
    "    dnnetwork=model2,\n",
    "    ReplayBuffer=ReplayBuffer,\n",
    "    train_pairs=test_pairs,\n",
    "    epsilon=0.00  # very low epsilon for testing\n",
    "), test_pairs, num_episodes=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29f84e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(3):  # visualize 3 test episodes\n",
    "#     visualize_test_episode(DQNAgent(\n",
    "#         env=Glioblastoma(*test_pairs[i], grid_size=4),\n",
    "#         dnnetwork=model,\n",
    "#         ReplayBuffer=ReplayBuffer,\n",
    "#         train_pairs=test_pairs,\n",
    "#     epsilon=0.00  # very low epsilon for testing\n",
    "#     ), test_pairs[i][0], test_pairs[i][1], episode_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7365cb6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
