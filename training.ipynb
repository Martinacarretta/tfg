{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cb7e8c3",
   "metadata": {},
   "source": [
    "# Notebook to experiment with training:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bc7211",
   "metadata": {},
   "source": [
    "## Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a2d983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import torch\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "import wandb\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import torch.nn as nn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06597a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glioblastoma(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 4} \n",
    "    # The metadata of the environment, e.g. {“render_modes”: [“rgb_array”, “human”], “render_fps”: 30}. \n",
    "    # For Jax or Torch, this can be indicated to users with “jax”=True or “torch”=True.\n",
    "\n",
    "    def __init__(self, image_path, mask_path, grid_size=4, tumor_threshold=0.15, render_mode=\"human\"): # cosntructor with the brain image, the mask and a size\n",
    "        super().__init__() # parent class\n",
    "        \n",
    "        self.image = np.load(image_path).astype(np.float32)\n",
    "        self.mask = np.load(mask_path).astype(np.uint8)\n",
    "        \n",
    "        img_min, img_max = self.image.min(), self.image.max()\n",
    "        if img_max > 1.0:  # only normalize if not already in [0, 1]\n",
    "            self.image = (self.image - img_min) / (img_max - img_min + 1e-8) #avoid division by 0\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.block_size = self.image.shape[0] // grid_size  # 240/4 = 60\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Define action and observation spaces\n",
    "        # Actions: 0 = stay, 1 = move down, 2 = move right\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        # Observations: grayscale patch (normalized 0-1)\n",
    "        # apparently Neural networks train better when inputs are scaled to small, \n",
    "        # consistent ranges rather than raw 0–255 values.\n",
    "        self.observation_space = spaces.Box( # Supports continuous (and discrete) vectors or matrices\n",
    "            low=0, high=1, # Data has been normalized\n",
    "            shape=(self.block_size, self.block_size), # shape of the observation\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.agent_pos = [0, 0] # INITIAL POSITION AT TOP LEFT\n",
    "        self.current_step = 0 # initialize counter\n",
    "        self.max_steps = 20  # like in the paper\n",
    "\n",
    "        self.tumor_threshold = tumor_threshold # 15% of the patch must be tumor to consider that the agent is inside the tumor region\n",
    "\n",
    "    def reset(self, seed=None, options=None): # new episode where we initialize the state. \n",
    "        super().reset(seed=seed) # parent\n",
    "        \n",
    "        # reset\n",
    "        self.agent_pos = [0, 0]  # top-left corner\n",
    "        self.current_step = 0\n",
    "        obs = self._get_obs()\n",
    "        info = {}\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "\n",
    "        prev_pos = self.agent_pos.copy() # for reward computation taking into consideration the transition changes\n",
    "        \n",
    "        # Apply action (respect grid boundaries)\n",
    "        if action == 1 and self.agent_pos[0] < self.grid_size - 1:\n",
    "            self.agent_pos[0] += 1  # move down\n",
    "        elif action == 2 and self.agent_pos[1] < self.grid_size - 1:\n",
    "            self.agent_pos[1] += 1  # move right\n",
    "        # else, the agent doesn't move so the observation \n",
    "        # and reward will be calculated from the same position\n",
    "        # no need to compute self.agent_po\n",
    "\n",
    "        reward = self._get_reward(action, prev_pos)\n",
    "                \n",
    "        obs = self._get_obs()\n",
    "\n",
    "        # Episode ends\n",
    "        terminated = self.current_step >= self.max_steps\n",
    "        truncated = False  # we don’t need truncation here\n",
    "        info = {}\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        r0 = self.agent_pos[0] * self.block_size # row start\n",
    "        c0 = self.agent_pos[1] * self.block_size # col start\n",
    "        \n",
    "        patch = self.image[r0:r0+self.block_size, c0:c0+self.block_size].astype(np.float32)\n",
    "        \n",
    "        #return patch FALTAAA\n",
    "        mean = patch.mean()\n",
    "        std = patch.std()\n",
    "        patch = (patch - mean) / (std + 1e-8)  # avoid division by zero\n",
    "        return patch.astype(np.float32)\n",
    "\n",
    "    def _get_reward(self, action, prev_pos):\n",
    "        # look for previous position in the mask\n",
    "        r0_prev = prev_pos[0] * self.block_size\n",
    "        c0_prev = prev_pos[1] * self.block_size\n",
    "        patch_mask_prev = self.mask[r0_prev:r0_prev+self.block_size, c0_prev:c0_prev+self.block_size]\n",
    "        \n",
    "        # look position of the agent in the mask\n",
    "        r0 = self.agent_pos[0] * self.block_size\n",
    "        c0 = self.agent_pos[1] * self.block_size\n",
    "        patch_mask = self.mask[r0:r0+self.block_size, c0:c0+self.block_size]\n",
    "        \n",
    "        # Now that i have the patch where i was and the patch where i am, i can check if there is tumor in any of them\n",
    "        # tumor is labeled as 1 or 4 in the mask        \n",
    "        # label 2 is edema\n",
    "        \n",
    "        # first get a count of the tumor pixels in the patch. \n",
    "        tumor_count_prev = np.sum(np.isin(patch_mask_prev, [1, 4]))\n",
    "        tumor_count_curr = np.sum(np.isin(patch_mask, [1, 4]))\n",
    "        total = self.block_size * self.block_size # to compute the percentage\n",
    "        # Determine if patch has more than self.tumor_threshold of tumor\n",
    "        was_inside = (tumor_count_prev / total) >= self.tumor_threshold\n",
    "        inside = (tumor_count_curr / total) >= self.tumor_threshold\n",
    "\n",
    "        if inside: # regardless of previous position\n",
    "            print(\"Entered tumor region!\") # DEBUGGING ONLY!\n",
    "            return 1.0  # entered tumor region\n",
    "        if not was_inside and not inside: # still outside\n",
    "            if action == 0: # didn't move\n",
    "                return -2.0\n",
    "            else:\n",
    "                return -0.5 # moved but still outside\n",
    "        if was_inside and not inside: # exited tumor region\n",
    "            print(\"Exited tumor region!\") # DEBUGGING ONLY!\n",
    "            return -0.5  # exited tumor region #maybe should be -1.0? FALTAA!!!!!!!!!!!!!!!!!!!\n",
    "        return 0.0\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode != \"human\": # would be rgb_array or ansi\n",
    "            return  # Only render in human mode\n",
    "\n",
    "        # Create RGB visualization image\n",
    "        # not necessary since it's grayscale, but i want to draw the mask and position\n",
    "        vis_img = np.stack([self.image] * 3, axis=-1).astype(np.float32)\n",
    "\n",
    "        # Overlay tumor mask in red [..., 0] \n",
    "        tumor_overlay = np.zeros_like(vis_img) # do all blank but here we have 3 channels, mask is 2D\n",
    "        tumor_overlay[..., 0] = (self.mask > 0).astype(float) # red channel. set to float to avoid issues when blending in vis_img\n",
    "\n",
    "        # transparency overlay (crec que es el mateix valor que tinc a l'altra notebook)\n",
    "        alpha = 0.4\n",
    "        vis_img = (1 - alpha) * vis_img + alpha * tumor_overlay\n",
    "\n",
    "        # Plotting\n",
    "        fig, ax = plt.subplots(figsize=(3, 3))\n",
    "        ax.imshow(vis_img, cmap='gray', origin='upper')\n",
    "\n",
    "        # Draw grid lines\n",
    "        # alpha for transparency again\n",
    "        for i in range(1, self.grid_size):\n",
    "            ax.axhline(i * self.block_size, color='white', lw=1, alpha=0.5)\n",
    "            ax.axvline(i * self.block_size, color='white', lw=1, alpha=0.5)\n",
    "\n",
    "        # Draw agent position\n",
    "        r0 = self.agent_pos[0] * self.block_size\n",
    "        c0 = self.agent_pos[1] * self.block_size\n",
    "        rect = patches.Rectangle(\n",
    "            (c0, r0), # (x,y) bottom left corner\n",
    "            self.block_size, # width\n",
    "            self.block_size, # height\n",
    "            linewidth=2,\n",
    "            edgecolor='yellow',\n",
    "            facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        ax.set_title(f\"Agent at {self.agent_pos} | Step {self.current_step}\")\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def current_patch_overlap_with_lesion(self): # FALTAAA chat\n",
    "        \"\"\"\n",
    "        Returns the number of overlapping lesion pixels between the agent's\n",
    "        current patch and the ground-truth mask.\n",
    "        If > 0, the agent is correctly over the lesion (TP).\n",
    "        \"\"\"\n",
    "        # get current agent patch boundaries\n",
    "        row, col = self.agent_pos\n",
    "        patch_h = self.grid_size\n",
    "        patch_w = self.grid_size\n",
    "\n",
    "        y0 = row * patch_h\n",
    "        y1 = y0 + patch_h\n",
    "        x0 = col * patch_w\n",
    "        x1 = x0 + patch_w\n",
    "\n",
    "        # extract mask region under current patch\n",
    "        patch_mask = self.mask[y0:y1, x0:x1]\n",
    "\n",
    "        # count how many pixels of lesion (nonzero)\n",
    "        overlap = np.sum(patch_mask > 0)\n",
    "        return overlap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8d671b",
   "metadata": {},
   "source": [
    "# DQN:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956c9099",
   "metadata": {},
   "source": [
    "https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6b41db",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_image_path = f\"/home/martina/codi2/4year/tfg/training_set_npy/001_49.npy\"\n",
    "trial_mask_path = f\"/home/martina/codi2/4year/tfg/training_set_npy/001_49_mask.npy\"\n",
    "\n",
    "env = Glioblastoma(trial_image_path, trial_mask_path, grid_size=4)\n",
    "print(env.observation_space.shape)\n",
    "print(env.action_space.n)\n",
    "print(np.arange(env.action_space.n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b53a716",
   "metadata": {},
   "source": [
    "neural network that approximates the Q-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c34bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, env, learning_rate=1e-3, device='cpu'):\n",
    "        super(DQN, self).__init__()\n",
    "        self.device = device\n",
    "        self.n_inputs = env.observation_space.shape[0] # 60\n",
    "        self.n_outputs = env.action_space.n # 3\n",
    "        self.actions = np.arange(env.action_space.n) # np.array([0, 1, 2])\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        input_channels = 1\n",
    "        height, width = env.observation_space.shape  # 60, 60   \n",
    "        \n",
    "        ### Construction of the neural network\n",
    "        ## features first and then fully connected layers\n",
    "        \n",
    "        # conv2d(in channels, out channels, kernel size, stride, padding)\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        \n",
    "        # flatten \n",
    "        with  torch.no_grad(): # FALTA MIRAR Q ES AIXO\n",
    "            dummy_input = torch.zeros(1, input_channels, height, width) # batch size 1\n",
    "            n_flatten = self.features(dummy_input).view(1, -1).size(1)\n",
    "            \n",
    "        # nn.Linear (in features, out features)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_flatten, 512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, self.n_outputs)\n",
    "        )\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        ### Work with CUDA is allowed\n",
    "        if self.device == 'cuda':\n",
    "            self.to(self.device).cuda()\n",
    "            \n",
    "    \n",
    "    # e-greedy method\n",
    "    def get_action(self, state, epsilon=0.05):\n",
    "        if np.random.random() < epsilon:\n",
    "            # random action -- Exploration\n",
    "            action = np.random.choice(self.actions)  \n",
    "        else:\n",
    "            # Q-value based action -- Exploitation\n",
    "            qvals = self.get_qvals(state)  \n",
    "            if qvals.dim() == 2 and qvals.size(0) == 1:\n",
    "                action = torch.argmax(qvals, dim=-1).item()\n",
    "            else:\n",
    "                action = torch.argmax(qvals, dim=-1)[0].item()\n",
    "\n",
    "        return int(action)\n",
    "    \n",
    "    # forward pass through conv and fc layers\n",
    "    def get_qvals(self, state): # FALTAAAAAA\n",
    "        # Convert (60,60) → (1,1,60,60)\n",
    "        if isinstance(state, np.ndarray):\n",
    "            if state.ndim == 2:  # grayscale single image (60x60)\n",
    "                state = np.expand_dims(np.expand_dims(state, 0), 0) # (1,1,60,60)\n",
    "                \n",
    "            elif state.ndim == 3:  # batch or stacked images (barch, 60, 60)\n",
    "                if state.shape[0] != 1: #batch\n",
    "                    state = np.expand_dims(state, 1)\n",
    "                    \n",
    "        # state_t = torch.FloatTensor(state).to(self.device)\n",
    "        state_t = torch.FloatTensor(state / 255.0).to(self.device) # Normalization [0,1]\n",
    "        qvals = self.fc(self.features(state_t).view(state_t.size(0), -1))\n",
    "        return qvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3283f892",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self, env, dnnetwork, buffer, epsilon=0.1, eps_decay=0.99, epsilon_min=0.01, batch_size=32, gamma=0.99):\n",
    "        self.env = env\n",
    "        self.dnnetwork = dnnetwork # main network\n",
    "        self.target_network = deepcopy(dnnetwork) # prevents the target Q-values from changing with every single update\n",
    "        self.target_network.optimizer = None # paper said target net is only  weights, no optimizer\n",
    "        self.buffer = buffer # store experiences\n",
    "        self.epsilon = epsilon # initial epsilon for e-greedy\n",
    "        self.eps_decay = eps_decay # decay of epsilon after each episode to balance exploration and exploitation\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size # size of the mini-batch for training\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # block of the last X episodes to calculate the average reward \n",
    "        self.nblock = 100 \n",
    "        # average reward used to determine if the agent has learned to play\n",
    "        #self.reward_threshold = self.env.spec.reward_threshold \n",
    "        self.initialize()\n",
    "    \n",
    "    def initialize(self): # reset variables at the beginning of training\n",
    "        self.update_loss = []\n",
    "        self.training_rewards = []\n",
    "        self.mean_training_rewards = []\n",
    "        self.sync_eps = []\n",
    "        self.total_reward = 0\n",
    "        self.step_count = 0\n",
    "        self.state0 = self.env.reset()[0]\n",
    "        \n",
    "    ## Take new action\n",
    "    def take_step(self, eps, mode='train'):\n",
    "        if mode == 'explore':\n",
    "            # random action in burn-in and in the exploration phase (epsilon)\n",
    "            action = self.env.action_space.sample() \n",
    "        else:\n",
    "            # Action based on the Q-value (max Q-value)\n",
    "            action = self.dnnetwork.get_action(self.state0, eps)\n",
    "            self.step_count += 1\n",
    "            \n",
    "        # Execute action and get reward and new state\n",
    "        new_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "        done = terminated or truncated\n",
    "        self.total_reward += reward\n",
    "        # save experience in the buffer\n",
    "        self.buffer.append(self.state0, action, reward, done, new_state) \n",
    "        self.state0 = new_state.copy()\n",
    "        \n",
    "        if done:\n",
    "            self.state0 = self.env.reset()[0]\n",
    "        return done\n",
    "\n",
    "            \n",
    "    ## Training\n",
    "    def train(self, train_pairs, gamma=0.99, max_episodes=50000, \n",
    "              dnn_update_frequency=4,\n",
    "              dnn_sync_frequency=200):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Fill the buffer with N random experiences\n",
    "        print(\"Filling replay buffer...\")\n",
    "        #while self.buffer.burn_in_capacity() < 1:\n",
    "        while len(self.buffer.buffer) < self.batch_size:\n",
    "            # pick random actions to fill the buffer\n",
    "            img_path, mask_path = random.choice(train_pairs)\n",
    "            self.env = Glioblastoma(img_path, mask_path, grid_size=self.env.grid_size, tumor_threshold=self.env.tumor_threshold)\n",
    "            obs, _ = self.env.reset()\n",
    "            self.state0 = obs\n",
    "            self.take_step(self.epsilon, mode='explore')\n",
    "\n",
    "            \n",
    "        # Store metrics locally to plot\n",
    "        self.episode_rewards = []\n",
    "        self.mean_rewards = []\n",
    "        self.epsilon_values = []\n",
    "        self.loss_values = []\n",
    " \n",
    "        episode = 0\n",
    "        training = True\n",
    "        print(\"Training...\")\n",
    "        while training and episode < max_episodes:\n",
    "            img_path, mask_path = random.choice(train_pairs)\n",
    "            self.env = Glioblastoma(img_path, mask_path, grid_size=self.env.grid_size, tumor_threshold=self.env.tumor_threshold)\n",
    "            self.state0, _ = self.env.reset()\n",
    "            self.total_reward = 0\n",
    "            gamedone = False\n",
    "            \n",
    "            while gamedone == False:\n",
    "                # The agent takes an action\n",
    "                gamedone = self.take_step(self.epsilon, mode='train')\n",
    "               \n",
    "                # Upgrade main network\n",
    "                if self.step_count % dnn_update_frequency == 0:\n",
    "                    self.update()\n",
    "                # Synchronize the main network and the target network\n",
    "                if self.step_count % dnn_sync_frequency == 0:\n",
    "                    self.target_network.load_state_dict(\n",
    "                        self.dnnetwork.state_dict())\n",
    "                    self.sync_eps.append(episode)\n",
    "                    \n",
    "                if gamedone:                   \n",
    "                    episode += 1\n",
    "                    # Save the rewards\n",
    "                    self.training_rewards.append(self.total_reward)\n",
    "                    # Calculate the average reward for the last X episodes\n",
    "                    if len(self.training_rewards) >= self.nblock:\n",
    "                        mean_rewards = np.mean(self.training_rewards[-self.nblock:])\n",
    "                    else:\n",
    "                        mean_rewards = np.mean(self.training_rewards)  # Use all rewards if less than nblock\n",
    "                    \n",
    "                    self.mean_training_rewards.append(mean_rewards)\n",
    "\n",
    "                    print(\"Episode {:d} Mean Rewards {:.2f} Epsilon {}\".format(episode, mean_rewards, self.epsilon))\n",
    "                    \n",
    "                    wandb.log({\n",
    "                        'episode': episode,\n",
    "                        'mean_rewards': mean_rewards,\n",
    "                        'episode reward': self.total_reward,\n",
    "                        'epsilon': self.epsilon,\n",
    "                        'loss': np.mean(self.update_loss)\n",
    "                    }, step=episode)\n",
    "                    \n",
    "                    # Append metrics to lists for plotting\n",
    "                    self.episode_rewards.append(self.total_reward)\n",
    "                    self.mean_rewards.append(mean_rewards)\n",
    "                    self.epsilon_values.append(self.epsilon)\n",
    "                    self.loss_values.append(np.mean(self.update_loss))\n",
    "                    \n",
    "                    self.update_loss = []\n",
    "\n",
    "                    # Check if there are still episodes left\n",
    "                    if episode >= max_episodes:\n",
    "                        training = False\n",
    "                        print('\\nEpisode limit reached.')\n",
    "                        break\n",
    "                    \n",
    "                    # # The game ends if the average reward has reached the threshold\n",
    "                    # if mean_rewards >= self.reward_threshold:\n",
    "                    #     training = False\n",
    "                    #     print('\\nEnvironment solved in {} episodes!'.format(episode))\n",
    "                    #     break\n",
    "                    \n",
    "                    # Update epsilon according to the fixed decay rate # SUBTRACTION FROM PPAPER\n",
    "                    self.epsilon = max(self.epsilon - self.eps_decay, self.epsilon_min) # at each  episode, not step\n",
    "                    torch.save(self.dnnetwork.state_dict(), \"Glioblastoma\" + \".dat\")\n",
    "                    \n",
    "        # PLOTTING\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(10, 8))  # Create a 2x2 grid of subplots\n",
    "        axes = axes.ravel()  # Flatten the axes array for easier indexing\n",
    "\n",
    "        # Plot episode rewards\n",
    "        axes[0].plot(self.episode_rewards)\n",
    "        axes[0].set_xlabel('Episode')\n",
    "        axes[0].set_ylabel('Episode Reward')\n",
    "        axes[0].set_title('Episode Rewards Over Time')\n",
    "\n",
    "        # Plot mean rewards\n",
    "        axes[1].plot(self.mean_rewards)\n",
    "        axes[1].set_xlabel('Episode')\n",
    "        axes[1].set_ylabel('Mean Reward')\n",
    "        axes[1].set_title('Mean Rewards Over Time')\n",
    "\n",
    "        # Plot epsilon values\n",
    "        axes[2].plot(self.epsilon_values)\n",
    "        axes[2].set_xlabel('Episode')\n",
    "        axes[2].set_ylabel('Epsilon Values')\n",
    "        axes[2].set_title('Epsilon Values Over Time')\n",
    "\n",
    "        # Plot loss values\n",
    "        axes[3].plot(self.loss_values)\n",
    "        axes[3].set_xlabel('Episode')\n",
    "        axes[3].set_ylabel('Loss')\n",
    "        axes[3].set_title('Loss Over Time')\n",
    "\n",
    "        # Adjust layout (optional)\n",
    "        # fig.suptitle('Training Performance', fontsize=16)  # Add a main title\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust spacing between subplots and title\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # Loss calculation           \n",
    "    def calculate_loss(self, batch):\n",
    "        # Separate the variables of the experience and convert them to tensors\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "        device = self.dnnetwork.device\n",
    "\n",
    "        # Add channel dimension and normalize to [0, 1] # FALTAA\n",
    "        states = torch.FloatTensor(states / 255.0).unsqueeze(1).to(device)\n",
    "        next_states = torch.FloatTensor(next_states / 255.0).unsqueeze(1).to(device)\n",
    "\n",
    "        rewards_vals = torch.FloatTensor(rewards).to(device=device) \n",
    "        actions_vals = torch.LongTensor(np.array(actions)).reshape(-1,1).to(device=device)\n",
    "        dones_t = torch.BoolTensor(dones).to(device=device)\n",
    "        \n",
    "        # Obtain the Q values of the main network\n",
    "        qvals = torch.gather(self.dnnetwork.get_qvals(states), 1, actions_vals)\n",
    "        \n",
    "        # Obtain the target Q values.\n",
    "        # The detach() parameter prevents these values from updating the target network\n",
    "        qvals_next = torch.max(self.target_network.get_qvals(next_states), dim=-1)[0].detach()\n",
    "        # 0 in terminal states\n",
    "        qvals_next[dones_t] = 0.0 \n",
    "        \n",
    "        # Calculate the Bellman equation\n",
    "        expected_qvals = (self.gamma * qvals_next) + rewards_vals\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = torch.nn.MSELoss()(qvals, expected_qvals.reshape(-1,1))\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def update(self):\n",
    "        if len(self.buffer.buffer) < self.batch_size:\n",
    "            return  # skip until we have enough experiences\n",
    "        \n",
    "        # Remove any gradient\n",
    "        self.dnnetwork.optimizer.zero_grad()  \n",
    "        # Select a subset from the buffer\n",
    "        batch = self.buffer.sample_batch(batch_size=self.batch_size) \n",
    "        # Calculate the loss\n",
    "        loss = self.calculate_loss(batch) \n",
    "        # Difference to get the gradients\n",
    "        loss.backward() \n",
    "        \n",
    "        # add gradient clipping (FALTAAA)\n",
    "        torch.nn.utils.clip_grad_norm_(self.dnnetwork.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Apply the gradients to the neural network\n",
    "        self.dnnetwork.optimizer.step() \n",
    "        # Save loss values\n",
    "        if self.dnnetwork.device == 'cuda':\n",
    "            self.update_loss.append(loss.detach().cpu().numpy())\n",
    "        else:\n",
    "            self.update_loss.append(loss.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a78476",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        self.buffer.append((np.array(state, dtype=np.float32), int(action), float(reward), bool(done), np.array(next_state, dtype=np.float32)))\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, dones, next_states = zip(*batch)\n",
    "        return (\n",
    "            np.array(states, dtype=np.float32),\n",
    "            np.array(actions, dtype=np.int64),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.bool_),\n",
    "            np.array(next_states, dtype=np.float32),\n",
    "        )\n",
    "\n",
    "    def burn_in_capacity(self):\n",
    "        return len(self.buffer) / self.capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89efd4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4 #From paper\n",
    "MEMORY_SIZE = 15000 #From paper\n",
    "MAX_EPISODES = 90 #From paper\n",
    "EPSILON = 0.7 #From paper\n",
    "EPSILON_DECAY = 1e-4 #From paper\n",
    "EPSILON_MIN = 1e-4 #From paper\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 128 #From paper\n",
    "BURN_IN = 1000\n",
    "DNN_UPD = 1\n",
    "DNN_SYNC = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b21fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/home/martina/codi2/4year/tfg/training_set_npy\"\n",
    "csv_path = \"/home/martina/codi2/4year/tfg/training_dataset_slices.csv\"\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Construct image and mask filenames\n",
    "df[\"image_path\"] = df.apply(\n",
    "    lambda row: os.path.join(base_dir, f\"{row['Patient']:03d}_{row['SliceIndex']}.npy\"), axis=1\n",
    ")\n",
    "df[\"mask_path\"] = df.apply(\n",
    "    lambda row: os.path.join(base_dir, f\"{row['Patient']:03d}_{row['SliceIndex']}_mask.npy\"), axis=1\n",
    ")\n",
    "\n",
    "# Sanity check (optional)\n",
    "train_pairs = [\n",
    "    (img, mask)\n",
    "    for img, mask in zip(df[\"image_path\"], df[\"mask_path\"])\n",
    "    if os.path.exists(img) and os.path.exists(mask)\n",
    "]\n",
    "\n",
    "print(f\"✅ Found {len(train_pairs)} training pairs out of {len(df)} listed in CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06deb4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DQN(Glioblastoma(*train_pairs[0], grid_size=4), learning_rate=LR, device='cpu')\n",
    "buffer = ReplayBuffer(capacity=MEMORY_SIZE)\n",
    "agent = DQNAgent(Glioblastoma(*train_pairs[0], grid_size=4), net, buffer,\n",
    "                 epsilon=EPSILON, eps_decay=EPSILON_DECAY, epsilon_min=EPSILON_MIN,\n",
    "                 batch_size=BATCH_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc753ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "wandb.Settings(quiet=True)\n",
    "wandb.init(project=\"TFG_Glioblastoma\", config={\n",
    "    \"lr\": LR,\n",
    "    \"MEMORY_SIZE\": MEMORY_SIZE,\n",
    "    \"MAX_EPISODES\": MAX_EPISODES,\n",
    "    \"EPSILON\": EPSILON,\n",
    "    \"EPSILON_DECAY\": EPSILON_DECAY,\n",
    "    \"EPSILON_MIN\": EPSILON_MIN,\n",
    "    \"GAMMA\": GAMMA,\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "    \"BURN_IN\": BURN_IN,\n",
    "    \"DNN_UPD\": DNN_UPD,\n",
    "    \"DNN_SYNC\": DNN_SYNC\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16003ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>> Training starts at \", datetime.datetime.now())\n",
    "agent.train(\n",
    "    train_pairs=train_pairs,\n",
    "    gamma=GAMMA,\n",
    "    max_episodes=MAX_EPISODES,\n",
    "    dnn_update_frequency=DNN_UPD,\n",
    "    dnn_sync_frequency=DNN_SYNC\n",
    ")\n",
    "print(\">>> Training completed at \", datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61533d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab631f23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
