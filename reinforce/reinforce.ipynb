{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c8c80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "from torch.distributions import Categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b15746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/martina/codi2/4year/tfg\")  # add parent folder of general.py\n",
    "\n",
    "from general import prepare, Glioblastoma, GlioblastomaPositionalEncoding, testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc4d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNPolicy(nn.Module):\n",
    "    def __init__(self, obs_shape, action_dim):\n",
    "        \"\"\"\n",
    "        obs_shape: (C, H, W)\n",
    "        action_dim: number of discrete actions\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        C, H, W = obs_shape\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(C, 16, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Dynamically compute the flatten size instead of hardcoding 64*5*5\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, C, H, W)\n",
    "            conv_out = self.conv(dummy)\n",
    "            flat_dim = conv_out.view(1, -1).size(1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(flat_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, C, H, W)\n",
    "        returns: probs: (B, action_dim)\n",
    "        \"\"\"\n",
    "        x = self.conv(x)\n",
    "        x = x.flatten(1)\n",
    "        logits = self.fc(x)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        return probs\n",
    "\n",
    "    def act(self, state, device):\n",
    "        \"\"\"\n",
    "        state: np.array\n",
    "          - (H, W)          for grayscale\n",
    "          - (C, H, W)       for multi-channel (positional encoding)\n",
    "        returns:\n",
    "          action (int), log_prob (scalar tensor on `device`)\n",
    "        \"\"\"\n",
    "        state_t = torch.as_tensor(state, dtype=torch.float32, device=device)\n",
    "\n",
    "        if state_t.ndim == 2:        # (H, W) -> (1, 1, H, W)\n",
    "            state_t = state_t.unsqueeze(0).unsqueeze(0)\n",
    "        elif state_t.ndim == 3:      # (C, H, W) -> (1, C, H, W)\n",
    "            state_t = state_t.unsqueeze(0)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected state ndim={state_t.ndim}, shape={state_t.shape}\")\n",
    "\n",
    "        probs = self.forward(state_t)           # (1, action_dim)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()                  # (1,)\n",
    "        log_prob = dist.log_prob(action)        # (1,)\n",
    "\n",
    "        return action.item(), log_prob.squeeze(0)  # scalar tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e9a7664",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEAgent:\n",
    "    def __init__(self, env_class, train_pairs, env_config,\n",
    "                 gamma=0.99, lr=1e-4, save_path=\"reinforce_policy.pt\"):\n",
    "\n",
    "        self.env_class = env_class\n",
    "        self.train_pairs = train_pairs\n",
    "        self.env_config = env_config\n",
    "        self.gamma = gamma\n",
    "        self.save_path = save_path\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # --- Infer observation shape & action_dim from a sample env ---\n",
    "        sample_img, sample_mask = train_pairs[0]\n",
    "        sample_env = env_class(sample_img, sample_mask, **env_config)\n",
    "        obs, _ = sample_env.reset()\n",
    "\n",
    "        if obs.ndim == 2:\n",
    "            C, H, W = 1, obs.shape[0], obs.shape[1]\n",
    "        elif obs.ndim == 3:\n",
    "            C, H, W = obs.shape\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected obs ndim={obs.ndim}, shape={obs.shape}\")\n",
    "\n",
    "        obs_shape = (C, H, W)\n",
    "        self.action_dim = env_config[\"action_space\"].n\n",
    "\n",
    "        # --- Policy network ---\n",
    "        self.policy = CNNPolicy(obs_shape, self.action_dim).to(self.device)\n",
    "        self.model = self.policy  # for compatibility with your testing() function\n",
    "        self.optim = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "\n",
    "        self.best_reward = -1e9\n",
    "\n",
    "    def make_env(self, img_path, mask_path):\n",
    "        return self.env_class(img_path, mask_path, **self.env_config)\n",
    "\n",
    "    def run_episode(self, img_path, mask_path):\n",
    "        \"\"\"\n",
    "        Runs one full episode on a single image/mask pair.\n",
    "        Returns list of log_probs and list of rewards.\n",
    "        \"\"\"\n",
    "        env = self.make_env(img_path, mask_path)\n",
    "\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action, log_prob = self.policy.act(state, self.device)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            state = next_state\n",
    "            done = terminated or truncated\n",
    "\n",
    "        return log_probs, rewards\n",
    "\n",
    "    def compute_returns(self, rewards):\n",
    "        \"\"\"\n",
    "        Compute discounted returns G_t for a single episode (no normalization here).\n",
    "        \"\"\"\n",
    "        G = 0.0\n",
    "        returns = []\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        return returns  # plain Python list of floats\n",
    "\n",
    "    def update_policy_batch(self, log_probs, returns):\n",
    "        \"\"\"\n",
    "        log_probs: list of scalar tensors (already on device)\n",
    "        returns:   1D tensor on device (same length as log_probs)\n",
    "        \"\"\"\n",
    "        log_probs_t = torch.stack(log_probs)               # (N,)\n",
    "        loss = -(log_probs_t * returns).sum()\n",
    "\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self, epochs=200):\n",
    "        \"\"\"\n",
    "        One epoch = run 1 episode per train_pair, then do a single batch update\n",
    "        using all (log_prob, G) pairs, with returns normalized across the epoch.\n",
    "        \"\"\"\n",
    "        for e in range(1, epochs + 1):\n",
    "            all_log_probs = []\n",
    "            all_returns = []\n",
    "            episode_returns = []  # sum of rewards per episode\n",
    "\n",
    "            # ---- Collect trajectories on ALL train_pairs ----\n",
    "            for i, (img_path, mask_path) in enumerate(self.train_pairs):\n",
    "                log_probs, rewards = self.run_episode(img_path, mask_path)\n",
    "\n",
    "                Gs = self.compute_returns(rewards)   # list of floats\n",
    "                all_log_probs.extend(log_probs)\n",
    "                all_returns.extend(Gs)\n",
    "\n",
    "                ep_ret = sum(rewards)\n",
    "                episode_returns.append(ep_ret)\n",
    "\n",
    "                if i % 10 == 0:\n",
    "                    print(f\"[Epoch {e} | Episode {i}] \"\n",
    "                          f\"Return={ep_ret:.2f} (len={len(rewards)})\")\n",
    "\n",
    "            # ---- Normalize returns across the WHOLE epoch ----\n",
    "            all_returns_t = torch.tensor(all_returns, dtype=torch.float32, device=self.device)\n",
    "            all_returns_t = (all_returns_t - all_returns_t.mean()) / (all_returns_t.std() + 1e-8)\n",
    "\n",
    "            # ---- Single policy update for the epoch ----\n",
    "            loss = self.update_policy_batch(all_log_probs, all_returns_t)\n",
    "            avg_ep_return = float(np.mean(episode_returns))\n",
    "\n",
    "            # ---- Save best model ----\n",
    "            if avg_ep_return > self.best_reward:\n",
    "                self.best_reward = avg_ep_return\n",
    "                torch.save(self.policy.state_dict(), self.save_path)\n",
    "                print(f\"  -> New best model saved ({self.save_path}), \"\n",
    "                      f\"Avg return={avg_ep_return:.2f}\")\n",
    "\n",
    "            if e % 5 == 0 or e == 1:\n",
    "                print(f\"[Epoch {e}] Avg Return per Episode={avg_ep_return:.2f} | Loss={loss:.4f}\")\n",
    "\n",
    "        torch.save(self.policy.state_dict(), \"reinforce_final.pt\")\n",
    "        print(\"Training finished. Final model saved to reinforce_final.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "839a510c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found 30 pairs out of 30 listed in CSV.\n"
     ]
    }
   ],
   "source": [
    "CURRENT_CONFIG = {\n",
    "    'grid_size': 6,\n",
    "    'rewards': [10.0, -2.0, 2.5, -0.1], # [staying on tumor, staying off tumor, moving into tumor, movement cost] #[3.0, -1.0, -0.2],\n",
    "    'action_space': spaces.Discrete(5), \n",
    "    'max_steps': 0\n",
    "    # 'stop': False\n",
    "}\n",
    "\n",
    "train_pairs = prepare()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ce38556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Episode 0] Return=-7.10 (len=10)\n",
      "[Epoch 1 | Episode 10] Return=-1.99 (len=2)\n",
      "[Epoch 1 | Episode 20] Return=-8.00 (len=25)\n",
      "  -> New best model saved (reinforce_best.pt), Avg return=-2.38\n",
      "[Epoch 1] Avg Return per Episode=-2.38 | Loss=0.3314\n",
      "[Epoch 2 | Episode 0] Return=-2.46 (len=5)\n",
      "[Epoch 2 | Episode 10] Return=-2.28 (len=2)\n",
      "[Epoch 2 | Episode 20] Return=-2.00 (len=1)\n",
      "  -> New best model saved (reinforce_best.pt), Avg return=-1.61\n",
      "[Epoch 3 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 3 | Episode 10] Return=-2.19 (len=2)\n",
      "[Epoch 3 | Episode 20] Return=-6.24 (len=13)\n",
      "  -> New best model saved (reinforce_best.pt), Avg return=-1.54\n",
      "[Epoch 4 | Episode 0] Return=-2.96 (len=7)\n",
      "[Epoch 4 | Episode 10] Return=-2.01 (len=2)\n",
      "[Epoch 4 | Episode 20] Return=-1.95 (len=2)\n",
      "  -> New best model saved (reinforce_best.pt), Avg return=-0.85\n",
      "[Epoch 5 | Episode 0] Return=10.00 (len=1)\n",
      "[Epoch 5 | Episode 10] Return=-2.21 (len=2)\n",
      "[Epoch 5 | Episode 20] Return=-1.96 (len=2)\n",
      "  -> New best model saved (reinforce_best.pt), Avg return=-0.72\n",
      "[Epoch 5] Avg Return per Episode=-0.72 | Loss=-0.3243\n",
      "[Epoch 6 | Episode 0] Return=-1.94 (len=2)\n",
      "[Epoch 6 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 6 | Episode 20] Return=-1.92 (len=3)\n",
      "[Epoch 7 | Episode 0] Return=-5.22 (len=9)\n",
      "[Epoch 7 | Episode 10] Return=-1.58 (len=3)\n",
      "[Epoch 7 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 8 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 8 | Episode 10] Return=-2.31 (len=3)\n",
      "[Epoch 8 | Episode 20] Return=-3.12 (len=7)\n",
      "[Epoch 9 | Episode 0] Return=-1.30 (len=2)\n",
      "[Epoch 9 | Episode 10] Return=-2.09 (len=3)\n",
      "[Epoch 9 | Episode 20] Return=-1.92 (len=3)\n",
      "[Epoch 10 | Episode 0] Return=-2.22 (len=3)\n",
      "[Epoch 10 | Episode 10] Return=-2.27 (len=2)\n",
      "[Epoch 10 | Episode 20] Return=-2.53 (len=3)\n",
      "[Epoch 10] Avg Return per Episode=-2.53 | Loss=0.1100\n",
      "[Epoch 11 | Episode 0] Return=-1.98 (len=2)\n",
      "[Epoch 11 | Episode 10] Return=-3.23 (len=4)\n",
      "[Epoch 11 | Episode 20] Return=-3.05 (len=11)\n",
      "[Epoch 12 | Episode 0] Return=-1.94 (len=2)\n",
      "[Epoch 12 | Episode 10] Return=9.02 (len=3)\n",
      "[Epoch 12 | Episode 20] Return=-3.50 (len=5)\n",
      "[Epoch 13 | Episode 0] Return=-2.23 (len=2)\n",
      "[Epoch 13 | Episode 10] Return=-2.23 (len=3)\n",
      "[Epoch 13 | Episode 20] Return=-2.25 (len=2)\n",
      "  -> New best model saved (reinforce_best.pt), Avg return=-0.60\n",
      "[Epoch 14 | Episode 0] Return=-1.97 (len=2)\n",
      "[Epoch 14 | Episode 10] Return=-2.58 (len=8)\n",
      "[Epoch 14 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 15 | Episode 0] Return=-2.00 (len=2)\n",
      "[Epoch 15 | Episode 10] Return=-3.31 (len=4)\n",
      "[Epoch 15 | Episode 20] Return=-1.30 (len=2)\n",
      "[Epoch 15] Avg Return per Episode=-1.47 | Loss=-0.8151\n",
      "[Epoch 16 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 16 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 16 | Episode 20] Return=-0.73 (len=4)\n",
      "[Epoch 17 | Episode 0] Return=-3.40 (len=6)\n",
      "[Epoch 17 | Episode 10] Return=-2.28 (len=2)\n",
      "[Epoch 17 | Episode 20] Return=0.65 (len=6)\n",
      "[Epoch 18 | Episode 0] Return=-3.30 (len=5)\n",
      "[Epoch 18 | Episode 10] Return=-3.31 (len=7)\n",
      "[Epoch 18 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 19 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 19 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 19 | Episode 20] Return=-4.22 (len=5)\n",
      "[Epoch 20 | Episode 0] Return=-1.98 (len=2)\n",
      "[Epoch 20 | Episode 10] Return=-6.80 (len=14)\n",
      "[Epoch 20 | Episode 20] Return=12.80 (len=7)\n",
      "[Epoch 20] Avg Return per Episode=-1.56 | Loss=-2.5812\n",
      "[Epoch 21 | Episode 0] Return=-2.42 (len=5)\n",
      "[Epoch 21 | Episode 10] Return=-4.40 (len=12)\n",
      "[Epoch 21 | Episode 20] Return=-1.92 (len=3)\n",
      "[Epoch 22 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 22 | Episode 10] Return=-5.57 (len=7)\n",
      "[Epoch 22 | Episode 20] Return=8.06 (len=12)\n",
      "  -> New best model saved (reinforce_best.pt), Avg return=0.23\n",
      "[Epoch 23 | Episode 0] Return=-3.42 (len=5)\n",
      "[Epoch 23 | Episode 10] Return=-3.28 (len=3)\n",
      "[Epoch 23 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 24 | Episode 0] Return=-1.95 (len=2)\n",
      "[Epoch 24 | Episode 10] Return=-1.98 (len=6)\n",
      "[Epoch 24 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 25 | Episode 0] Return=-5.43 (len=10)\n",
      "[Epoch 25 | Episode 10] Return=-2.26 (len=2)\n",
      "[Epoch 25 | Episode 20] Return=-3.98 (len=8)\n",
      "[Epoch 25] Avg Return per Episode=-2.05 | Loss=-1.5424\n",
      "[Epoch 26 | Episode 0] Return=-1.25 (len=3)\n",
      "[Epoch 26 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 26 | Episode 20] Return=-2.80 (len=6)\n",
      "[Epoch 27 | Episode 0] Return=0.18 (len=13)\n",
      "[Epoch 27 | Episode 10] Return=-1.94 (len=3)\n",
      "[Epoch 27 | Episode 20] Return=0.55 (len=3)\n",
      "[Epoch 28 | Episode 0] Return=-2.57 (len=3)\n",
      "[Epoch 28 | Episode 10] Return=-4.27 (len=10)\n",
      "[Epoch 28 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 29 | Episode 0] Return=11.24 (len=4)\n",
      "[Epoch 29 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 29 | Episode 20] Return=-1.51 (len=4)\n",
      "[Epoch 30 | Episode 0] Return=-4.56 (len=11)\n",
      "[Epoch 30 | Episode 10] Return=-3.30 (len=4)\n",
      "[Epoch 30 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 30] Avg Return per Episode=-1.64 | Loss=-3.2385\n",
      "[Epoch 31 | Episode 0] Return=-0.95 (len=6)\n",
      "[Epoch 31 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 31 | Episode 20] Return=11.50 (len=3)\n",
      "[Epoch 32 | Episode 0] Return=-2.14 (len=3)\n",
      "[Epoch 32 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 32 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 33 | Episode 0] Return=-2.96 (len=8)\n",
      "[Epoch 33 | Episode 10] Return=-2.28 (len=2)\n",
      "[Epoch 33 | Episode 20] Return=-2.20 (len=4)\n",
      "[Epoch 34 | Episode 0] Return=-2.94 (len=3)\n",
      "[Epoch 34 | Episode 10] Return=-2.57 (len=5)\n",
      "[Epoch 34 | Episode 20] Return=10.50 (len=4)\n",
      "[Epoch 35 | Episode 0] Return=-3.24 (len=3)\n",
      "[Epoch 35 | Episode 10] Return=-2.48 (len=3)\n",
      "[Epoch 35 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 35] Avg Return per Episode=-2.20 | Loss=-2.3563\n",
      "[Epoch 36 | Episode 0] Return=-1.93 (len=2)\n",
      "[Epoch 36 | Episode 10] Return=-2.18 (len=2)\n",
      "[Epoch 36 | Episode 20] Return=-2.49 (len=3)\n",
      "[Epoch 37 | Episode 0] Return=-1.30 (len=2)\n",
      "[Epoch 37 | Episode 10] Return=-2.20 (len=2)\n",
      "[Epoch 37 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 38 | Episode 0] Return=-3.12 (len=8)\n",
      "[Epoch 38 | Episode 10] Return=0.49 (len=13)\n",
      "[Epoch 38 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 39 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 39 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 39 | Episode 20] Return=11.26 (len=4)\n",
      "[Epoch 40 | Episode 0] Return=-2.21 (len=2)\n",
      "[Epoch 40 | Episode 10] Return=-2.28 (len=2)\n",
      "[Epoch 40 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 40] Avg Return per Episode=-1.46 | Loss=-1.9347\n",
      "[Epoch 41 | Episode 0] Return=-3.20 (len=4)\n",
      "[Epoch 41 | Episode 10] Return=-2.29 (len=3)\n",
      "[Epoch 41 | Episode 20] Return=-1.95 (len=2)\n",
      "[Epoch 42 | Episode 0] Return=-4.46 (len=9)\n",
      "[Epoch 42 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 42 | Episode 20] Return=-2.47 (len=3)\n",
      "[Epoch 43 | Episode 0] Return=-3.20 (len=3)\n",
      "[Epoch 43 | Episode 10] Return=-2.26 (len=2)\n",
      "[Epoch 43 | Episode 20] Return=10.00 (len=1)\n",
      "[Epoch 44 | Episode 0] Return=-4.57 (len=5)\n",
      "[Epoch 44 | Episode 10] Return=-2.25 (len=2)\n",
      "[Epoch 44 | Episode 20] Return=-2.40 (len=5)\n",
      "[Epoch 45 | Episode 0] Return=-1.30 (len=2)\n",
      "[Epoch 45 | Episode 10] Return=12.50 (len=2)\n",
      "[Epoch 45 | Episode 20] Return=-2.50 (len=3)\n",
      "[Epoch 45] Avg Return per Episode=-0.66 | Loss=-2.4393\n",
      "[Epoch 46 | Episode 0] Return=-1.30 (len=2)\n",
      "[Epoch 46 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 46 | Episode 20] Return=-3.74 (len=6)\n",
      "[Epoch 47 | Episode 0] Return=-1.30 (len=2)\n",
      "[Epoch 47 | Episode 10] Return=-2.31 (len=3)\n",
      "[Epoch 47 | Episode 20] Return=-2.25 (len=2)\n",
      "[Epoch 48 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 48 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 48 | Episode 20] Return=-2.24 (len=2)\n",
      "[Epoch 49 | Episode 0] Return=-3.98 (len=4)\n",
      "[Epoch 49 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 49 | Episode 20] Return=-2.30 (len=3)\n",
      "[Epoch 50 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 50 | Episode 10] Return=-1.30 (len=2)\n",
      "[Epoch 50 | Episode 20] Return=10.00 (len=1)\n",
      "[Epoch 50] Avg Return per Episode=-0.14 | Loss=-4.4877\n",
      "[Epoch 51 | Episode 0] Return=10.34 (len=8)\n",
      "[Epoch 51 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 51 | Episode 20] Return=-2.07 (len=7)\n",
      "[Epoch 52 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 52 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 52 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 53 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 53 | Episode 10] Return=-2.22 (len=4)\n",
      "[Epoch 53 | Episode 20] Return=-0.50 (len=3)\n",
      "[Epoch 54 | Episode 0] Return=-3.23 (len=10)\n",
      "[Epoch 54 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 54 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 55 | Episode 0] Return=-4.26 (len=5)\n",
      "[Epoch 55 | Episode 10] Return=-3.17 (len=5)\n",
      "[Epoch 55 | Episode 20] Return=10.00 (len=1)\n",
      "[Epoch 55] Avg Return per Episode=-0.99 | Loss=-4.9208\n",
      "[Epoch 56 | Episode 0] Return=10.00 (len=1)\n",
      "[Epoch 56 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 56 | Episode 20] Return=3.05 (len=4)\n",
      "[Epoch 57 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 57 | Episode 10] Return=-1.30 (len=2)\n",
      "[Epoch 57 | Episode 20] Return=10.18 (len=16)\n",
      "[Epoch 58 | Episode 0] Return=-2.30 (len=3)\n",
      "[Epoch 58 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 58 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 59 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 59 | Episode 10] Return=-2.28 (len=2)\n",
      "[Epoch 59 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 60 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 60 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 60 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 60] Avg Return per Episode=-1.94 | Loss=3.0651\n",
      "[Epoch 61 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 61 | Episode 10] Return=-2.42 (len=4)\n",
      "[Epoch 61 | Episode 20] Return=-2.26 (len=2)\n",
      "[Epoch 62 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 62 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 62 | Episode 20] Return=10.00 (len=1)\n",
      "[Epoch 63 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 63 | Episode 10] Return=-3.47 (len=4)\n",
      "[Epoch 63 | Episode 20] Return=-2.24 (len=2)\n",
      "[Epoch 64 | Episode 0] Return=-3.07 (len=7)\n",
      "[Epoch 64 | Episode 10] Return=-1.87 (len=3)\n",
      "[Epoch 64 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 65 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 65 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 65 | Episode 20] Return=-1.96 (len=2)\n",
      "[Epoch 65] Avg Return per Episode=-0.98 | Loss=17.0455\n",
      "[Epoch 66 | Episode 0] Return=-2.25 (len=3)\n",
      "[Epoch 66 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 66 | Episode 20] Return=-1.26 (len=4)\n",
      "[Epoch 67 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 67 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 67 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 68 | Episode 0] Return=-1.93 (len=2)\n",
      "[Epoch 68 | Episode 10] Return=-2.27 (len=2)\n",
      "[Epoch 68 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 69 | Episode 0] Return=-2.26 (len=2)\n",
      "[Epoch 69 | Episode 10] Return=-1.94 (len=3)\n",
      "[Epoch 69 | Episode 20] Return=10.00 (len=1)\n",
      "  -> New best model saved (reinforce_best.pt), Avg return=0.43\n",
      "[Epoch 70 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 70 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 70 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 70] Avg Return per Episode=-0.63 | Loss=5.3441\n",
      "[Epoch 71 | Episode 0] Return=-3.00 (len=7)\n",
      "[Epoch 71 | Episode 10] Return=-1.93 (len=2)\n",
      "[Epoch 71 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 72 | Episode 0] Return=-1.54 (len=3)\n",
      "[Epoch 72 | Episode 10] Return=10.00 (len=1)\n",
      "[Epoch 72 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 73 | Episode 0] Return=-2.00 (len=2)\n",
      "[Epoch 73 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 73 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 74 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 74 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 74 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 75 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 75 | Episode 10] Return=-2.15 (len=4)\n",
      "[Epoch 75 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 75] Avg Return per Episode=-0.03 | Loss=-2.2847\n",
      "[Epoch 76 | Episode 0] Return=-1.97 (len=3)\n",
      "[Epoch 76 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 76 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 77 | Episode 0] Return=-3.27 (len=4)\n",
      "[Epoch 77 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 77 | Episode 20] Return=12.55 (len=3)\n",
      "[Epoch 78 | Episode 0] Return=-2.26 (len=2)\n",
      "[Epoch 78 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 78 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 79 | Episode 0] Return=-1.93 (len=2)\n",
      "[Epoch 79 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 79 | Episode 20] Return=-2.48 (len=3)\n",
      "[Epoch 80 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 80 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 80 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 80] Avg Return per Episode=-1.22 | Loss=-5.3916\n",
      "[Epoch 81 | Episode 0] Return=-3.39 (len=4)\n",
      "[Epoch 81 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 81 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 82 | Episode 0] Return=-3.00 (len=3)\n",
      "[Epoch 82 | Episode 10] Return=-1.92 (len=2)\n",
      "[Epoch 82 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 83 | Episode 0] Return=10.00 (len=1)\n",
      "[Epoch 83 | Episode 10] Return=-1.94 (len=3)\n",
      "[Epoch 83 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 84 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 84 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 84 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 85 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 85 | Episode 10] Return=-1.93 (len=2)\n",
      "[Epoch 85 | Episode 20] Return=-2.00 (len=1)\n",
      "  -> New best model saved (reinforce_best.pt), Avg return=0.48\n",
      "[Epoch 85] Avg Return per Episode=0.48 | Loss=0.0696\n",
      "[Epoch 86 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 86 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 86 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 87 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 87 | Episode 10] Return=-2.27 (len=2)\n",
      "[Epoch 87 | Episode 20] Return=-1.95 (len=2)\n",
      "[Epoch 88 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 88 | Episode 10] Return=-1.98 (len=2)\n",
      "[Epoch 88 | Episode 20] Return=12.25 (len=4)\n",
      "[Epoch 89 | Episode 0] Return=-1.99 (len=2)\n",
      "[Epoch 89 | Episode 10] Return=-2.25 (len=2)\n",
      "[Epoch 89 | Episode 20] Return=10.00 (len=1)\n",
      "[Epoch 90 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 90 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 90 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 90] Avg Return per Episode=-1.21 | Loss=-1.7103\n",
      "[Epoch 91 | Episode 0] Return=-2.93 (len=3)\n",
      "[Epoch 91 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 91 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 92 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 92 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 92 | Episode 20] Return=10.00 (len=1)\n",
      "  -> New best model saved (reinforce_best.pt), Avg return=0.53\n",
      "[Epoch 93 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 93 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 93 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 94 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 94 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 94 | Episode 20] Return=-1.95 (len=2)\n",
      "[Epoch 95 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 95 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 95 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 95] Avg Return per Episode=-1.61 | Loss=-5.8215\n",
      "[Epoch 96 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 96 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 96 | Episode 20] Return=-3.23 (len=5)\n",
      "[Epoch 97 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 97 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 97 | Episode 20] Return=-1.96 (len=2)\n",
      "[Epoch 98 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 98 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 98 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 99 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 99 | Episode 10] Return=-1.93 (len=2)\n",
      "[Epoch 99 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 100 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 100 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 100 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 100] Avg Return per Episode=0.00 | Loss=-6.3325\n",
      "[Epoch 101 | Episode 0] Return=-1.88 (len=3)\n",
      "[Epoch 101 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 101 | Episode 20] Return=-2.30 (len=3)\n",
      "[Epoch 102 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 102 | Episode 10] Return=-3.28 (len=3)\n",
      "[Epoch 102 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 103 | Episode 0] Return=-1.88 (len=4)\n",
      "[Epoch 103 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 103 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 104 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 104 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 104 | Episode 20] Return=10.00 (len=1)\n",
      "[Epoch 105 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 105 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 105 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 105] Avg Return per Episode=-1.65 | Loss=-4.2448\n",
      "[Epoch 106 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 106 | Episode 10] Return=10.00 (len=1)\n",
      "[Epoch 106 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 107 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 107 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 107 | Episode 20] Return=12.50 (len=2)\n",
      "[Epoch 108 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 108 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 108 | Episode 20] Return=-2.26 (len=3)\n",
      "[Epoch 109 | Episode 0] Return=-1.98 (len=2)\n",
      "[Epoch 109 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 109 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 110 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 110 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 110 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 110] Avg Return per Episode=-1.24 | Loss=-4.5948\n",
      "[Epoch 111 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 111 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 111 | Episode 20] Return=10.00 (len=1)\n",
      "[Epoch 112 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 112 | Episode 10] Return=12.57 (len=3)\n",
      "[Epoch 112 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 113 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 113 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 113 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 114 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 114 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 114 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 115 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 115 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 115 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 115] Avg Return per Episode=-1.52 | Loss=7.3903\n",
      "[Epoch 116 | Episode 0] Return=-1.94 (len=2)\n",
      "[Epoch 116 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 116 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 117 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 117 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 117 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 118 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 118 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 118 | Episode 20] Return=10.00 (len=1)\n",
      "[Epoch 119 | Episode 0] Return=-2.26 (len=2)\n",
      "[Epoch 119 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 119 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 120 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 120 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 120 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 120] Avg Return per Episode=-0.73 | Loss=5.9251\n",
      "[Epoch 121 | Episode 0] Return=10.00 (len=1)\n",
      "[Epoch 121 | Episode 10] Return=-2.00 (len=2)\n",
      "[Epoch 121 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 122 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 122 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 122 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 123 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 123 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 123 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 124 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 124 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 124 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 125 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 125 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 125 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 125] Avg Return per Episode=-0.99 | Loss=8.5121\n",
      "[Epoch 126 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 126 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 126 | Episode 20] Return=0.55 (len=3)\n",
      "[Epoch 127 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 127 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 127 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 128 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 128 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 128 | Episode 20] Return=-1.83 (len=4)\n",
      "[Epoch 129 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 129 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 129 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 130 | Episode 0] Return=-2.01 (len=3)\n",
      "[Epoch 130 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 130 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 130] Avg Return per Episode=-1.65 | Loss=-7.6493\n",
      "[Epoch 131 | Episode 0] Return=-1.96 (len=2)\n",
      "[Epoch 131 | Episode 10] Return=-1.93 (len=3)\n",
      "[Epoch 131 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 132 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 132 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 132 | Episode 20] Return=-1.94 (len=2)\n",
      "[Epoch 133 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 133 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 133 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 134 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 134 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 134 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 135 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 135 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 135 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 135] Avg Return per Episode=-1.19 | Loss=-3.5081\n",
      "[Epoch 136 | Episode 0] Return=10.00 (len=1)\n",
      "[Epoch 136 | Episode 10] Return=-3.28 (len=3)\n",
      "[Epoch 136 | Episode 20] Return=10.00 (len=1)\n",
      "[Epoch 137 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 137 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 137 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 138 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 138 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 138 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 139 | Episode 0] Return=10.00 (len=1)\n",
      "[Epoch 139 | Episode 10] Return=-1.30 (len=2)\n",
      "[Epoch 139 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 140 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 140 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 140 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 140] Avg Return per Episode=-1.58 | Loss=-1.1111\n",
      "[Epoch 141 | Episode 0] Return=-1.98 (len=2)\n",
      "[Epoch 141 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 141 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 142 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 142 | Episode 10] Return=-2.22 (len=4)\n",
      "[Epoch 142 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 143 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 143 | Episode 10] Return=-1.22 (len=3)\n",
      "[Epoch 143 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 144 | Episode 0] Return=10.00 (len=1)\n",
      "[Epoch 144 | Episode 10] Return=-1.93 (len=3)\n",
      "[Epoch 144 | Episode 20] Return=-1.95 (len=2)\n",
      "[Epoch 145 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 145 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 145 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 145] Avg Return per Episode=-0.84 | Loss=-4.2214\n",
      "[Epoch 146 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 146 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 146 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 147 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 147 | Episode 10] Return=-2.01 (len=2)\n",
      "[Epoch 147 | Episode 20] Return=-2.00 (len=1)\n",
      "  -> New best model saved (reinforce_best.pt), Avg return=1.11\n",
      "[Epoch 148 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 148 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 148 | Episode 20] Return=10.00 (len=1)\n",
      "[Epoch 149 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 149 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 149 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 150 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 150 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 150 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 150] Avg Return per Episode=0.53 | Loss=-0.1499\n",
      "[Epoch 151 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 151 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 151 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 152 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 152 | Episode 10] Return=-1.97 (len=2)\n",
      "[Epoch 152 | Episode 20] Return=-1.30 (len=2)\n",
      "[Epoch 153 | Episode 0] Return=-1.87 (len=3)\n",
      "[Epoch 153 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 153 | Episode 20] Return=-1.94 (len=2)\n",
      "[Epoch 154 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 154 | Episode 10] Return=-2.27 (len=2)\n",
      "[Epoch 154 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 155 | Episode 0] Return=12.50 (len=2)\n",
      "[Epoch 155 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 155 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 155] Avg Return per Episode=-1.01 | Loss=13.6453\n",
      "[Epoch 156 | Episode 0] Return=-1.98 (len=2)\n",
      "[Epoch 156 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 156 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 157 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 157 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 157 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 158 | Episode 0] Return=-1.92 (len=2)\n",
      "[Epoch 158 | Episode 10] Return=-1.99 (len=2)\n",
      "[Epoch 158 | Episode 20] Return=-1.94 (len=2)\n",
      "[Epoch 159 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 159 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 159 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 160 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 160 | Episode 10] Return=0.56 (len=4)\n",
      "[Epoch 160 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 160] Avg Return per Episode=-0.12 | Loss=6.3327\n",
      "[Epoch 161 | Episode 0] Return=-1.98 (len=2)\n",
      "[Epoch 161 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 161 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 162 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 162 | Episode 10] Return=-1.99 (len=2)\n",
      "[Epoch 162 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 163 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 163 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 163 | Episode 20] Return=-1.30 (len=2)\n",
      "[Epoch 164 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 164 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 164 | Episode 20] Return=-3.92 (len=5)\n",
      "[Epoch 165 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 165 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 165 | Episode 20] Return=11.26 (len=4)\n",
      "[Epoch 165] Avg Return per Episode=0.29 | Loss=15.0323\n",
      "[Epoch 166 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 166 | Episode 10] Return=15.00 (len=3)\n",
      "[Epoch 166 | Episode 20] Return=-1.30 (len=2)\n",
      "[Epoch 167 | Episode 0] Return=-2.01 (len=3)\n",
      "[Epoch 167 | Episode 10] Return=-3.19 (len=5)\n",
      "[Epoch 167 | Episode 20] Return=-1.96 (len=2)\n",
      "[Epoch 168 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 168 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 168 | Episode 20] Return=10.00 (len=1)\n",
      "[Epoch 169 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 169 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 169 | Episode 20] Return=-1.94 (len=2)\n",
      "[Epoch 170 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 170 | Episode 10] Return=-1.30 (len=2)\n",
      "[Epoch 170 | Episode 20] Return=-2.25 (len=2)\n",
      "[Epoch 170] Avg Return per Episode=0.84 | Loss=4.2938\n",
      "[Epoch 171 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 171 | Episode 10] Return=-2.31 (len=6)\n",
      "[Epoch 171 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 172 | Episode 0] Return=-2.01 (len=3)\n",
      "[Epoch 172 | Episode 10] Return=-1.99 (len=2)\n",
      "[Epoch 172 | Episode 20] Return=-1.30 (len=2)\n",
      "[Epoch 173 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 173 | Episode 10] Return=-1.98 (len=2)\n",
      "[Epoch 173 | Episode 20] Return=-2.26 (len=2)\n",
      "[Epoch 174 | Episode 0] Return=-1.98 (len=2)\n",
      "[Epoch 174 | Episode 10] Return=-2.01 (len=2)\n",
      "[Epoch 174 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 175 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 175 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 175 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 175] Avg Return per Episode=-1.05 | Loss=-5.6014\n",
      "[Epoch 176 | Episode 0] Return=12.50 (len=2)\n",
      "[Epoch 176 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 176 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 177 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 177 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 177 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 178 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 178 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 178 | Episode 20] Return=10.00 (len=1)\n",
      "[Epoch 179 | Episode 0] Return=-2.01 (len=2)\n",
      "[Epoch 179 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 179 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 180 | Episode 0] Return=-1.97 (len=2)\n",
      "[Epoch 180 | Episode 10] Return=-2.23 (len=5)\n",
      "[Epoch 180 | Episode 20] Return=3.08 (len=5)\n",
      "[Epoch 180] Avg Return per Episode=-0.37 | Loss=-2.5107\n",
      "[Epoch 181 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 181 | Episode 10] Return=10.00 (len=1)\n",
      "[Epoch 181 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 182 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 182 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 182 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 183 | Episode 0] Return=-1.27 (len=3)\n",
      "[Epoch 183 | Episode 10] Return=-1.99 (len=2)\n",
      "[Epoch 183 | Episode 20] Return=-1.30 (len=2)\n",
      "[Epoch 184 | Episode 0] Return=10.00 (len=1)\n",
      "[Epoch 184 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 184 | Episode 20] Return=10.00 (len=1)\n",
      "  -> New best model saved (reinforce_best.pt), Avg return=1.29\n",
      "[Epoch 185 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 185 | Episode 10] Return=-2.30 (len=3)\n",
      "[Epoch 185 | Episode 20] Return=10.00 (len=1)\n",
      "[Epoch 185] Avg Return per Episode=-1.64 | Loss=-7.3669\n",
      "[Epoch 186 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 186 | Episode 10] Return=-2.46 (len=5)\n",
      "[Epoch 186 | Episode 20] Return=-2.22 (len=4)\n",
      "[Epoch 187 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 187 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 187 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 188 | Episode 0] Return=-1.30 (len=2)\n",
      "[Epoch 188 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 188 | Episode 20] Return=-1.99 (len=2)\n",
      "[Epoch 189 | Episode 0] Return=-1.96 (len=2)\n",
      "[Epoch 189 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 189 | Episode 20] Return=-1.98 (len=2)\n",
      "[Epoch 190 | Episode 0] Return=-1.97 (len=2)\n",
      "[Epoch 190 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 190 | Episode 20] Return=10.00 (len=1)\n",
      "[Epoch 190] Avg Return per Episode=-0.26 | Loss=-3.4351\n",
      "[Epoch 191 | Episode 0] Return=-1.93 (len=2)\n",
      "[Epoch 191 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 191 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 192 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 192 | Episode 10] Return=-2.21 (len=5)\n",
      "[Epoch 192 | Episode 20] Return=12.50 (len=2)\n",
      "[Epoch 193 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 193 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 193 | Episode 20] Return=-1.92 (len=3)\n",
      "[Epoch 194 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 194 | Episode 10] Return=-1.97 (len=3)\n",
      "[Epoch 194 | Episode 20] Return=12.54 (len=3)\n",
      "[Epoch 195 | Episode 0] Return=-1.92 (len=3)\n",
      "[Epoch 195 | Episode 10] Return=-1.99 (len=2)\n",
      "[Epoch 195 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 195] Avg Return per Episode=-1.48 | Loss=-1.9667\n",
      "[Epoch 196 | Episode 0] Return=-1.92 (len=5)\n",
      "[Epoch 196 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 196 | Episode 20] Return=-2.23 (len=3)\n",
      "[Epoch 197 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 197 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 197 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 198 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 198 | Episode 10] Return=-2.58 (len=4)\n",
      "[Epoch 198 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 199 | Episode 0] Return=-1.97 (len=2)\n",
      "[Epoch 199 | Episode 10] Return=10.00 (len=1)\n",
      "[Epoch 199 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 200 | Episode 0] Return=-2.00 (len=1)\n",
      "[Epoch 200 | Episode 10] Return=-2.00 (len=1)\n",
      "[Epoch 200 | Episode 20] Return=-2.00 (len=1)\n",
      "[Epoch 200] Avg Return per Episode=-1.22 | Loss=-6.1813\n",
      "Training finished. Final model saved to reinforce_final.pt\n"
     ]
    }
   ],
   "source": [
    "agent = REINFORCEAgent(\n",
    "    env_class=GlioblastomaPositionalEncoding,\n",
    "    train_pairs=train_pairs,\n",
    "    env_config=CURRENT_CONFIG,\n",
    "    gamma=0.99,\n",
    "    lr=1e-4,\n",
    "    save_path=\"reinforce_best.pt\"\n",
    ")\n",
    "\n",
    "agent.train(epochs=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db9fbe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
