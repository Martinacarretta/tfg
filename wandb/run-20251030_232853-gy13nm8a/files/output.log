>>> Training starts at  2025-10-30 23:29:01.592415
Filling replay buffer...
Agent moved into tumor at position: [2, 1]
Agent stayed at tumor position
Agent moved into tumor at position: [1, 2]
Agent moved into tumor at position: [1, 1]
Agent moved into tumor at position: [2, 1]
Training...
[Episode 0] Using image: 170_67.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.93359375 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.96875 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.34436882
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.95703125 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.95703125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [2, 2]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent stayed at tumor position
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.93359375 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -2.  -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.9464713
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-2.  -0.5  1.  -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.6383074
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.96875 1.0
sample rewards [-0.5  1.  -2.  -0.5 -2.  -2.  -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.91015625 1.0
sample rewards [ 1.  -0.5 -2.  -0.5  1.  -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.76291794
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.91015625 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -2.  -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.93359375 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.9453125 1.0
sample rewards [-2.  -2.  -2.  -0.5  1.  -0.5  1.   1.  -2.  -0.5]
state min/max example: 0.0 0.3839635
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -2.  -0.5 -2.  -2. ]
state min/max example: 0.0 0.7015915
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.93359375 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.9453125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.9453125 1.0
sample rewards [-0.5 -0.5 -2.  -2.   1.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.9346505
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 1 Mean Rewards -16.00 Epsilon 0.7 Loss 1.0830100774765015
[Episode 1] Using image: 235_114.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.91015625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.37914693
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.58557135
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-2.   1.  -0.5 -2.  -0.5 -2.  -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.91015625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5  1.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.9921875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [1, 2]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-2.  -2.   1.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.91015625 1.0
sample rewards [-2.   1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.6383074
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.93359375 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.93359375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5  1.  -0.5 -0.5]
state min/max example: 0.0 0.91729325
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-2.  -0.5 -2.   1.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.3839635
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.   1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.91015625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -2. ]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-2.  -0.5 -2.  -2.  -0.5 -0.5 -2.  -0.5 -2.   1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.9453125 1.0
sample rewards [ 1.  -2.  -0.5 -2.   1.  -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.85643566
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.96875 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.8250366
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 2 Mean Rewards -16.75 Epsilon 0.6922233333333333 Loss 0.26946189999580383
[Episode 2] Using image: 325_17.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.91015625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.4481306
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -1.0039062 1.0
sample rewards [-0.5 -2.  -2.  -2.  -2.  -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.60761344
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-2.  -2.  -2.  -2.  -0.5 -2.  -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [2, 2]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.96875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.7752562
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.91015625 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -2.  -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.93359375 1.0
sample rewards [-0.5 -2.   1.  -0.5 -0.5 -0.5  1.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-2.  -0.5 -0.5  1.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [ 1.  -2.  -0.5 -2.  -0.5 -2.   1.  -0.5 -0.5 -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5  1.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-2.  -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.93359375 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -2.  -0.5 -2.  -2.   1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.65413535
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.9453125 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -2.  -2.   1.  -2.  -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.95703125 1.0
sample rewards [-2.  -2.  -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -2.  -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.36636245
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 3 Mean Rewards -15.50 Epsilon 0.6844466666666666 Loss 0.23629498481750488
[Episode 3] Using image: 265_75.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.93359375 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.7752562
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.9453125 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -2.  -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.93359375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -2.   1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.9453125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -2.   1.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [ 1.  -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.76291794
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-2.  -0.5 -0.5  1.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.7752562
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.93359375 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -2.  -2.  -2.  -2.  -2. ]
state min/max example: 0.0 0.7752562
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -2.  -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.91015625 1.0
sample rewards [-2.  -0.5  1.  -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.95703125 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.70212764
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 4 Mean Rewards -16.75 Epsilon 0.67667 Loss 0.2235112190246582
[Episode 4] Using image: 162_106.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.9453125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -1.0039062 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -2.  -2.  -0.5  1. ]
state min/max example: 0.0 0.6383074
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -2.  -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.63754886
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.96875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -2.  -0.5 -2. ]
state min/max example: 0.0 0.8389458
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.9464713
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.93359375 1.0
sample rewards [-2.  -2.  -0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7752562
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.9921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.9206081
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -1.0039062 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.34436882
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.9453125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.85643566
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.91015625 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7128378
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.91729325
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-2.  -2.  -2.  -2.  -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -2.  -2.   1.  -0.5 -0.5]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.65413535
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 5 Mean Rewards -16.30 Epsilon 0.6688933333333333 Loss 0.25813615322113037
[Episode 5] Using image: 362_68.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.58557135
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.95703125 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -2.  -0.5 -2.  -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.95703125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5  1.  -2. ]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -1.0273438 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.68032295
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -0.5 -2.  -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.9453125 1.0
sample rewards [-2.  -2.  -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.9453125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.6112731
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5  1.   1.  -0.5]
state min/max example: 0.0 0.36636245
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -2.  -0.5 -2.  -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.91015625 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -2.   1.  -2.  -2.  -0.5 -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.96875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 6 Mean Rewards -17.50 Epsilon 0.6611166666666667 Loss 0.29856157302856445
[Episode 6] Using image: 349_99.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [ 1.  -0.5 -2.  -0.5 -0.5  1.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [1, 1]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-2.  -2.  -2.  -2.  -0.5 -0.5 -0.5  1.  -0.5 -0.5]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent stayed at tumor position
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -0.5  1.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.37914693
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -2.  -2.   1.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-2.  -0.5  1.  -0.5 -0.5 -0.5 -2.  -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -2.  -0.5  1.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.9673684
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.33082706
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.9453125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5  1.  -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.42857143
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.96875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.96875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7752562
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.   1. ]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -2.  -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 7 Mean Rewards -16.64 Epsilon 0.65334 Loss 0.37188994884490967
[Episode 7] Using image: 362_68.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.9206081
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -0.5  1.  -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-2.  -2.  -0.5 -2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.40547588
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -2.  -0.5 -2.  -2.  -2. ]
state min/max example: 0.0 0.6890185
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-2.  -2.  -2.  -0.5 -0.5 -2.  -0.5 -2.  -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.   1.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.93359375 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.4429708
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [ 1.  -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -2.  -0.5 -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 8 Mean Rewards -16.00 Epsilon 0.6455633333333334 Loss 0.38460081815719604
[Episode 8] Using image: 249_54.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-0.5 -0.5 -2.   1.  -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.3839635
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.95703125 1.0
sample rewards [-0.5 -0.5 -2.   1.  -2.  -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5  1.  -0.5]
state min/max example: 0.0 0.33082706
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.95703125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5  1.  -2. ]
state min/max example: 0.0 0.6383074
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.95703125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [ 1.  -0.5 -2.   1.  -0.5 -2.  -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.83106434
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.7752562
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-2.   1.  -2.  -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [ 1.  -2.  -0.5 -2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.9346505
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.8250366
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 9 Mean Rewards -15.67 Epsilon 0.6377866666666667 Loss 0.4785991311073303
[Episode 9] Using image: 325_17.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5  1.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.91015625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.91015625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5  1.  -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.40547588
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.96875 1.0
sample rewards [ 1.  -2.  -2.   1.  -2.  -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.83106434
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.   1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.   1.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7015915
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.   1.  -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5  1.  -2.  -2.  -0.5  1.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.36248785
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.95703125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 10 Mean Rewards -16.15 Epsilon 0.6300100000000001 Loss 0.4672172963619232
[Episode 10] Using image: 249_54.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.45189506
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.91015625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.8586626
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -2. ]
state min/max example: 0.0 0.26487625
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.93359375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -2.   1.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [ 1.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.95703125 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -2.  -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -0.5  1.  -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.9346505
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5  1.  -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.96875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -2.  -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.91015625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.3839635
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 11 Mean Rewards -16.95 Epsilon 0.6222333333333334 Loss 0.5011076927185059
[Episode 11] Using image: 020_26.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.83106434
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.91015625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.   1.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.85643566
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.91015625 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -2.  -2.  -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.96875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.9921875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.26487625
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.9673684
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -2.  -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.6383074
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -2.  -2.  -0.5]
state min/max example: 0.0 0.44360903
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.98046875 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.93359375 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.9453125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-2.  -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.93359375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.   1.  -0.5 -2.  -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -2.  -2.  -2.  -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.65413535
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5  1.  -0.5  1.  -2.  -0.5 -0.5 -2.  -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 12 Mean Rewards -16.88 Epsilon 0.6144566666666668 Loss 0.5689284801483154
[Episode 12] Using image: 037_83.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.33082706
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.91015625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.36073825
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5  1.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.5486577
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -2.   1.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.95703125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5  1.  -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.4481306
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.95703125 1.0
sample rewards [-2.  -0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.93359375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.95703125 -0.5
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.83106434
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [ 1.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.7128713
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [ 1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.83106434
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5  1.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7752562
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 13 Mean Rewards -16.69 Epsilon 0.6066800000000001 Loss 0.6688619256019592
[Episode 13] Using image: 325_17.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.9453125 -0.5
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.93877554
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.5847315
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [2, 2]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -0.5 -2.  -0.5 -2.  -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -2.  -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5  1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.95703125 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5  1.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5  1.  -0.5 -0.5]
state min/max example: 0.0 0.36472148
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.36073825
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -2.   1.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.37914693
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 14 Mean Rewards -16.64 Epsilon 0.5989033333333335 Loss 0.6204532384872437
[Episode 14] Using image: 325_17.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -0.5 -0.5  1.  -2.  -2.  -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -0.5  1.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -2.  -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.5486577
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.91015625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.63754886
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.6112731
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.91015625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -2.  -0.5 -2.  -2. ]
state min/max example: 0.0 0.20278184
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.7128713
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.5847315
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -0.5  1.  -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.96875 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.98046875 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 15 Mean Rewards -16.50 Epsilon 0.5911266666666668 Loss 0.7441339492797852
[Episode 15] Using image: 089_46.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -0.5 -2.  -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.   1. ]
state min/max example: 0.0 0.8389458
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.7035957
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [1, 2]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [2, 2]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [ 1.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.85643566
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7128378
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-2.  -0.5 -2.  -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 -0.5
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.95821184
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -2.  -2.  -0.5 -2.  -2. ]
state min/max example: 0.0 0.60761344
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5  1.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -0.5 -0.5  1.  -2.  -2.  -2.  -2.  -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 16 Mean Rewards -16.09 Epsilon 0.5833500000000001 Loss 0.8252853155136108
[Episode 16] Using image: 249_54.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -2.  -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [1, 2]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent stayed at tumor position
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-2.  -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.60761344
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [2, 2]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-2.  -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.8250366
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-2.  -0.5 -0.5  1.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.85643566
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.9453125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.9453125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-2.  -2.  -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.9453125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.83106434
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 17 Mean Rewards -15.74 Epsilon 0.5755733333333335 Loss 0.9297137260437012
[Episode 17] Using image: 027_61.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.6112731
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [1, 2]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -2.  -0.5 -2.  -0.5 -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [2, 2]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-2.  -2.  -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-2.  -2.  -0.5 -2.  -2.  -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-2.  -0.5 -2.   1.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [ 1.  -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.7055369
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.34436882
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7752562
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -2.  -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.8429054
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -0.5 -2.  -2.  -2.  -0.5]
state min/max example: 0.0 0.7035957
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 18 Mean Rewards -15.58 Epsilon 0.5677966666666668 Loss 0.9178361892700195
[Episode 18] Using image: 349_99.npy
Agent moved into tumor at position: [0, 1]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-2.  -0.5  1.  -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.83106434
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.   1.  -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -2.  -2.  -2.  -0.5 -2.  -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.3839635
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7752562
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-2.  -0.5 -0.5  1.  -2.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-2.  -0.5 -2.  -2.  -2.  -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.91015625 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -0.5 -2.  -2.  -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.93359375 1.0
sample rewards [-2.  -2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.5729866
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 19 Mean Rewards -15.53 Epsilon 0.5600200000000002 Loss 0.9512112736701965
[Episode 19] Using image: 187_122.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.93359375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -2.  -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.3839635
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5  1.  -2.  -0.5 -2.  -0.5 -2.  -0.5 -0.5  1. ]
state min/max example: 0.0 0.58557135
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7752562
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.34436882
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-2.   1.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 20 Mean Rewards -15.70 Epsilon 0.5522433333333335 Loss 1.100607991218567
[Episode 20] Using image: 325_17.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.   1.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.91015625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [2, 1]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5  1.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.   1.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.6112731
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -2.  -2.   1.  -2.  -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5  1.   1.  -0.5 -2.  -2.  -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.70212764
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 21 Mean Rewards -15.43 Epsilon 0.5444666666666669 Loss 1.2514926195144653
[Episode 21] Using image: 115_123.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.83106434
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.36248785
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.49473685
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -2.  -2.  -0.5  1.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [ 1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -2. ]
state min/max example: 0.0 0.7752562
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -2.  -2.  -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5  1.  -2.  -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-2.   1.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 22 Mean Rewards -15.39 Epsilon 0.5366900000000002 Loss 1.1851872205734253
[Episode 22] Using image: 153_61.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -2.  -2.  -0.5]
state min/max example: 0.0 0.5612416
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -2.  -2.  -0.5  1.  -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.8586626
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5  1.  -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.36636245
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -2.  -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.5486577
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -2.   1.   1.  -0.5 -0.5]
state min/max example: 0.0 0.29383886
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.96875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-2.  -2.  -2.  -0.5 -0.5 -2.  -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -0.5 -2.  -0.5 -2.  -2. ]
state min/max example: 0.0 0.7035957
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 23 Mean Rewards -15.41 Epsilon 0.5289133333333336 Loss 1.196286916732788
[Episode 23] Using image: 364_25.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-2.  -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [1, 1]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -2.  -2.   1.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.95703125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5  1.  -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -2.  -2.   1.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [ 1.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.83106434
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.7128378
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.7015915
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -2.  -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -2.  -0.5  1.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.87218046
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-2.  -2.  -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [ 1.   1.  -0.5 -0.5 -2.  -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 24 Mean Rewards -15.31 Epsilon 0.5211366666666669 Loss 1.3475853204727173
[Episode 24] Using image: 325_17.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.91015625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -2.   1.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.37914693
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.89694655
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.45234334
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.83106434
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.9453125 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5  1.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5  1.  -2.  -0.5 -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 25 Mean Rewards -15.40 Epsilon 0.5133600000000003 Loss 1.2203172445297241
[Episode 25] Using image: 235_114.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.68032295
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [1, 1]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [1, 2]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.36248785
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent stayed at tumor position
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.   1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-2.  -0.5 -2.  -0.5  1.  -0.5  1.  -2.  -2.  -0.5]
state min/max example: 0.0 0.36073825
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.55263156
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5 -2.  -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-2.  -0.5 -2.   1.  -0.5 -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -2.  -0.5 -2.   1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 -0.5
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -2.  -2.  -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.83106434
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -2.  -2.  -2. ]
state min/max example: 0.0 0.31241032
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -2.  -2.   1.  -2.  -2.  -0.5 -0.5  1. ]
state min/max example: 0.0 0.36073825
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 26 Mean Rewards -15.25 Epsilon 0.5055833333333336 Loss 1.4907540082931519
[Episode 26] Using image: 037_83.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [1, 1]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.9346505
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.31241032
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -0.5 -0.5  1.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.91015625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.   1.  -0.5 -2. ]
state min/max example: 0.0 0.7752562
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [ 1.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -2. ]
state min/max example: 0.0 0.9346505
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.91015625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.83106434
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 27 Mean Rewards -15.28 Epsilon 0.49780666666666695 Loss 1.7294893264770508
[Episode 27] Using image: 187_122.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.91015625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.45234334
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -2.  -2.  -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.98046875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.45692477
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5  1.  -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5  1.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5  1.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.4481306
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 28 Mean Rewards -15.36 Epsilon 0.4900300000000003 Loss 1.7856003046035767
[Episode 28] Using image: 115_123.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [1, 1]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.91015625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -2.  -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.36636245
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -0.5 -2.  -2.  -0.5  1. ]
state min/max example: 0.0 0.9857627
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5  1.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.34436882
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.45692477
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5  1.  -2.  -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -2.  -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.60761344
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 29 Mean Rewards -15.33 Epsilon 0.48225333333333364 Loss 1.7211863994598389
[Episode 29] Using image: 288_45.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5  1.  -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -2.   1.  -0.5]
state min/max example: 0.0 0.63754886
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [1, 1]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -0.5 -2.  -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [2, 1]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.   1.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.45692477
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.9455645
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -2.   1.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.34436882
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5  1.  -2.  -2.  -2.  -2.  -0.5]
state min/max example: 0.0 0.36636245
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [ 1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7055369
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5  1.  -0.5 -0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-2.  -2.  -0.5 -2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 30 Mean Rewards -15.35 Epsilon 0.474476666666667 Loss 2.007629156112671
[Episode 30] Using image: 183_66.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.7015915
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -2.   1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [ 1.  -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7203451
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.45692477
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.85737437
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.45692477
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-2.  -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 31 Mean Rewards -15.32 Epsilon 0.46670000000000034 Loss 1.8148841857910156
[Episode 31] Using image: 091_92.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5  1.  -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5  1.  -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.36472148
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5  1.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.45234334
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -2.   1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-2.  -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 32 Mean Rewards -15.30 Epsilon 0.4589233333333337 Loss 1.5863451957702637
[Episode 32] Using image: 325_17.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-2.  -0.5  1.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.36472148
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.83106434
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.   1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5  1.  -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.93359375 1.0
sample rewards [-2.  -2.  -2.  -0.5 -0.5 -0.5 -0.5 -2.  -2.  -2. ]
state min/max example: 0.0 0.8250366
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5  1.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7752562
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [ 1.   1.   1.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.76291794
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 -0.5
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.85643566
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 33 Mean Rewards -15.23 Epsilon 0.45114666666666703 Loss 1.9631996154785156
[Episode 33] Using image: 091_92.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [ 1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.83106434
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [ 1.  -2.  -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.9346505
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -2.  -2.  -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -2.  -0.5  1.  -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.87218046
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -2.   1. ]
state min/max example: 0.0 0.85643566
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.   1.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 34 Mean Rewards -15.16 Epsilon 0.4433700000000004 Loss 1.9307832717895508
[Episode 34] Using image: 187_122.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [ 1.  -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.83106434
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.36472148
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [ 1.  -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.8658044
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.60305345
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5  1.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.95703125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -2.   1.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.   1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.7752562
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 35 Mean Rewards -15.19 Epsilon 0.4355933333333337 Loss 2.1312341690063477
[Episode 35] Using image: 235_114.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [1, 1]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [1, 2]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.26487625
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-2.  -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -0.5 -0.5  1.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5  1.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [ 1.  -2.  -0.5 -0.5  1.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.62957543
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-2.  -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.   1.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-2.  -2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5  1.  -0.5]
state min/max example: 0.0 0.7035957
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 36 Mean Rewards -14.96 Epsilon 0.42781666666666707 Loss 1.8224226236343384
[Episode 36] Using image: 187_122.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.8250366
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-2.  -2.  -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -2.   1.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [ 1.  -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.89647734
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5  1.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.91729325
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -2.  -0.5  1.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 37 Mean Rewards -15.03 Epsilon 0.4200400000000004 Loss 2.3909242153167725
[Episode 37] Using image: 225_94.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.7422286
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5  1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.36636245
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.34436882
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5  1.   1.   1.  -0.5 -0.5 -2.  -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [ 1.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.85643566
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [ 1.  -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5  1.  -0.5 -2.  -2.  -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.8250366
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 38 Mean Rewards -15.05 Epsilon 0.41226333333333376 Loss 2.3064942359924316
[Episode 38] Using image: 027_61.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -2.  -2.   1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [1, 2]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-2.  -0.5 -2.   1.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.65413535
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.60761344
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [ 1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7649919
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7015915
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -2.  -2.  -0.5]
state min/max example: 0.0 0.8250366
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.5486577
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 39 Mean Rewards -14.92 Epsilon 0.4044866666666671 Loss 1.9420194625854492
[Episode 39] Using image: 183_66.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -2.  -0.5  1.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -2.  -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [2, 1]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.3968421
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.31241032
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7196871
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.   1.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7128378
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.45234334
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 40 Mean Rewards -14.84 Epsilon 0.39671000000000045 Loss 1.9590644836425781
[Episode 40] Using image: 224_71.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [1, 2]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5  1.  -2.  -2.  -0.5 -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -2.  -0.5 -2. ]
state min/max example: 0.0 0.7580645
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-2.  -2.  -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -2.   1.  -2.  -0.5 -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -2.  -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5  1.  -0.5  1.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.45189506
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.36472148
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 41 Mean Rewards -14.90 Epsilon 0.3889333333333338 Loss 2.1562752723693848
[Episode 41] Using image: 224_71.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -2.  -2.  -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5  1.  -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.45692477
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -2.  -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [ 1.  -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7128713
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-2.  -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5  1.  -0.5 -0.5  1.  -0.5 -2.  -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5  1.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 42 Mean Rewards -14.86 Epsilon 0.38115666666666714 Loss 1.9586397409439087
[Episode 42] Using image: 364_25.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.42222223
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.95703125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [1, 1]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.   1.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.   1.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -2.   1.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7157258
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8984375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.3839635
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.91015625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7752562
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -2.  -0.5  1.  -0.5 -2. ]
state min/max example: 0.0 0.37914693
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.7752562
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.36636245
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 43 Mean Rewards -14.85 Epsilon 0.3733800000000005 Loss 2.146879196166992
[Episode 43] Using image: 249_54.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [1, 2]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-2.  -0.5 -0.5  1.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -0.5 -2.  -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-2.  -2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.9455645
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5  1.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.36073825
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5  1.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-2.  -0.5 -2.  -2.  -0.5 -2.  -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5  1.   1.  -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.46286473
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.5486577
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 44 Mean Rewards -14.74 Epsilon 0.36560333333333384 Loss 2.579972743988037
[Episode 44] Using image: 224_71.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [1, 2]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5  1.  -0.5 -2.  -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.3968421
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [ 1.  -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.39640132
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.31241032
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-2.  -2.  -0.5 -2.  -2.  -0.5 -2.   1.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.5486577
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.46222222
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -2. ]
state min/max example: 0.0 0.9673684
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.921875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.70291775
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 45 Mean Rewards -14.63 Epsilon 0.3578266666666672 Loss 2.2511932849884033
[Episode 45] Using image: 183_66.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -2.  -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.5981308
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -2.  -0.5 -2.  -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.89894736
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.5911111
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.3875724
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -2.  -0.5  1.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.5911111
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [ 1.  -0.5 -2.  -0.5 -2.  -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.6533333
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.   1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.31241032
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.9464713
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 46 Mean Rewards -14.60 Epsilon 0.3500500000000005 Loss 2.3795201778411865
[Episode 46] Using image: 298_78.npy
Agent moved into tumor at position: [0, 1]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -2.  -2.  -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [1, 1]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -2.   1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.3839635
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.6383074
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.34436882
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-2.  -0.5 -0.5 -2.   1.  -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7035957
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.9464713
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7752562
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -2.  -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-2.  -0.5 -2.  -2.   1.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [ 1.  -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 47 Mean Rewards -14.50 Epsilon 0.3422733333333339 Loss 2.0557219982147217
[Episode 47] Using image: 183_66.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-2.   1.  -0.5 -0.5  1.  -2.  -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.36472148
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.  -2.  -2.  -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.6979866
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5  1.  -0.5 -0.5 -2.   1.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -2.  -0.5 -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.9673684
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.71888727
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-2.   1.  -0.5 -2.  -0.5 -2.  -0.5 -2.  -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -2.  -0.5 -2.   1.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.45692477
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 48 Mean Rewards -14.47 Epsilon 0.3344966666666672 Loss 2.0544323921203613
[Episode 48] Using image: 162_106.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.8308581
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5  1.  -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.5911111
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-2.  -0.5  1.  -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5  1.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.46222222
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.5549077
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.8308581
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.8586626
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [ 1.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5  1.  -0.5 -0.5]
state min/max example: 0.0 0.62957543
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.34436882
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 49 Mean Rewards -14.44 Epsilon 0.32672000000000057 Loss 1.5760844945907593
[Episode 49] Using image: 234_39.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-2.  -0.5 -0.5  1.  -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5  1.  -2.  -0.5 -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.60761344
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.36472148
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [ 1.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.9673684
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [ 1.  -0.5  1.  -0.5  1.  -2.  -2.   1.  -0.5 -0.5]
state min/max example: 0.0 0.83106434
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.31241032
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-2.  -2.  -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.9593496
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-2.  -2.  -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -2.   1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.31241032
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-2.  -0.5  1.  -0.5  1.  -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5  1.  -0.5 -2.  -0.5 -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 50 Mean Rewards -14.47 Epsilon 0.3189433333333339 Loss 2.0317044258117676
[Episode 50] Using image: 146_45.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.   1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.8308581
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.20278184
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5  1.  -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.40547588
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -2.  -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-2.  -0.5 -0.5 -2.   1.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.36472148
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-2.  -0.5 -0.5  1.  -0.5  1.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.8586626
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 51 Mean Rewards -14.41 Epsilon 0.31116666666666726 Loss 2.013632297515869
[Episode 51] Using image: 234_39.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5  1.  -2.  -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.7422286
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [1, 2]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7965616
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.60761344
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -0.5 -0.5 -2.  -2.  -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.5486577
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -2.  -2.  -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 52 Mean Rewards -14.33 Epsilon 0.3033900000000006 Loss 1.945370078086853
[Episode 52] Using image: 187_122.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -2.  -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.   1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.49333334
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5  1.  -0.5 -0.5  1.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.4429708
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5  1. ]
state min/max example: 0.0 0.7470011
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5  1.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 53 Mean Rewards -14.33 Epsilon 0.29561333333333395 Loss 2.119250774383545
[Episode 53] Using image: 003_69.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.36472148
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5  1.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-2.  -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [2, 2]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -2.   1.  -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.91729325
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -2.   1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.   1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5  1.  -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.31241032
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 -0.5
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.36472148
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 54 Mean Rewards -14.33 Epsilon 0.2878366666666673 Loss 2.3776135444641113
[Episode 54] Using image: 234_39.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5  1.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [1, 2]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5  1.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent stayed at tumor position
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.36472148
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -2.  -2.  -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.7055369
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 -0.5
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.85737437
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 55 Mean Rewards -14.28 Epsilon 0.28006000000000064 Loss 2.4890570640563965
[Episode 55] Using image: 152_100.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.   1.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-2.  -2.  -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.58305085
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.   1.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -0.5  1.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-2.   1.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.65413535
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.71888727
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.  -2.  -2.  -2.  -0.5 -2. ]
state min/max example: 0.0 0.70291775
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -2.  -2.  -0.5]
state min/max example: 0.0 0.7015915
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.60761344
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5  1. ]
state min/max example: 0.0 0.9949622
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 56 Mean Rewards -14.26 Epsilon 0.272283333333334 Loss 2.275596857070923
[Episode 56] Using image: 115_123.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.45234334
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.36073825
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5  1.  -2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5  1.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5  1.  -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.42222223
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [ 1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.6533333
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 57 Mean Rewards -14.21 Epsilon 0.26450666666666733 Loss 2.335197687149048
[Episode 57] Using image: 152_100.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.36636245
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [1, 1]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.  -2.  -0.5 -2.  -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [1, 2]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-2.  -0.5 -2.  -0.5  1.  -2.  -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.5911111
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.5729866
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [ 1.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.76291794
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.8741221
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [ 1.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-2.  -2.   1.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.98762375
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5  1.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 58 Mean Rewards -14.16 Epsilon 0.2567300000000007 Loss 2.2830121517181396
[Episode 58] Using image: 225_94.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -2.  -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.60546875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5  1.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [3, 1]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.5486577
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.6383074
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5  1.  -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.93877554
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5  1. ]
state min/max example: 0.0 0.30543634
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-2.  -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.6383074
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5  1.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.4481306
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.5549077
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 59 Mean Rewards -14.07 Epsilon 0.24895333333333403 Loss 2.2239413261413574
[Episode 59] Using image: 224_71.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5  1.  -2.  -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [ 1.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.875 1.0
sample rewards [-2.  -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5  1.  -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -2.  -0.5  1.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.36444443
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.26487625
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7884131
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.45692477
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.8308581
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7752562
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.34436882
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 60 Mean Rewards -14.00 Epsilon 0.24117666666666737 Loss 2.2149417400360107
[Episode 60] Using image: 234_39.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.5845272
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.3839635
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.86328125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5  1.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -2.   1.  -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 61 Mean Rewards -13.96 Epsilon 0.23340000000000072 Loss 2.4092917442321777
[Episode 61] Using image: 249_54.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.29383886
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.8586626
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.36472148
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.69340974
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.42222223
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 62 Mean Rewards -13.92 Epsilon 0.22562333333333406 Loss 2.570138692855835
[Episode 62] Using image: 146_45.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.60546875 1.0
sample rewards [-0.5  1.  -0.5  1.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5  1.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.8658044
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -2.   1.  -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.29383886
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.42222223
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.4429708
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.   1.  -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.39198855
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.   1.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 63 Mean Rewards -13.93 Epsilon 0.2178466666666674 Loss 2.3499832153320312
[Episode 63] Using image: 089_46.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [ 1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.7649919
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [ 1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.7128713
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [1, 2]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [2, 2]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.5486577
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.36073825
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-2.  -0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.45692477
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-2.  -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.36472148
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.59375 1.0
sample rewards [-0.5 -0.5  1.  -2.  -2.  -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 64 Mean Rewards -13.84 Epsilon 0.21007000000000076 Loss 2.0446078777313232
[Episode 64] Using image: 115_123.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-2.  -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.59375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.5703125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.39198855
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.63754886
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.45692477
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 65 Mean Rewards -13.78 Epsilon 0.2022933333333341 Loss 2.3323235511779785
[Episode 65] Using image: 364_25.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.8658044
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5  1.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.3968421
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [ 1.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7055369
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 -0.5
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -2.  -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.40547588
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5  1.  -2.  -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -2.  -0.5]
state min/max example: 0.0 0.9455645
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5  1.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5  1.  -0.5 -0.5  1.  -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -0.5 -2.  -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 66 Mean Rewards -13.73 Epsilon 0.19451666666666745 Loss 1.9197323322296143
[Episode 66] Using image: 349_99.npy
Agent moved into tumor at position: [0, 1]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.36444443
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -0.5 -2.  -0.5  1.   1. ]
state min/max example: 0.0 0.42222223
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7752562
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.5324468
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5  1.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.36073825
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 67 Mean Rewards -13.67 Epsilon 0.1867400000000008 Loss 1.7187954187393188
[Episode 67] Using image: 115_123.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5  1. ]
state min/max example: 0.0 0.5911111
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5  1.  -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.42222223
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -2.  -2.  -0.5 -2. ]
state min/max example: 0.0 0.3839635
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.   1.  -2.  -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -2.  -0.5  1.  -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.5486577
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 68 Mean Rewards -13.62 Epsilon 0.17896333333333414 Loss 1.9277347326278687
[Episode 68] Using image: 349_99.npy
Agent moved into tumor at position: [0, 1]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.   1.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-2.  -0.5 -0.5 -2.   1.  -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.   1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -2.  -0.5  1.  -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -2.  -2.   1.   1.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.45272207
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.7422286
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 -0.5
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -2.  -2.  -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5  1.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 69 Mean Rewards -13.54 Epsilon 0.1711866666666675 Loss 1.8001559972763062
[Episode 69] Using image: 003_69.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.60546875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.5981308
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5  1.  -0.5 -2.  -0.5 -0.5 -2.   1.   1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 70 Mean Rewards -13.49 Epsilon 0.16341000000000083 Loss 2.192012310028076
[Episode 70] Using image: 355_93.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5  1.  -2.  -2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7752562
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.88671875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -0.5 -2.  -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.5729866
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5  1.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.828125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 71 Mean Rewards -13.46 Epsilon 0.15563333333333418 Loss 2.27565336227417
[Episode 71] Using image: 001_49.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [1, 1]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.60546875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.60305345
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.   1.  -2. ]
state min/max example: 0.0 0.34436882
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -2.  -0.5]
state min/max example: 0.0 0.91729325
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.69893897
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 72 Mean Rewards -13.44 Epsilon 0.14785666666666752 Loss 2.143965244293213
[Episode 72] Using image: 235_114.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.9593496
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [ 1.  -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.85643566
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.55263156
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.7128378
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5  1.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5  1.  -0.5  1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.45692477
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.83984375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -2.   1.   1.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.3875724
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 73 Mean Rewards -13.41 Epsilon 0.14008000000000087 Loss 1.9744539260864258
[Episode 73] Using image: 037_83.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8515625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.7580645
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [1, 1]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.4481306
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -2.  -0.5 -2. ]
state min/max example: 0.0 0.7531486
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.3968421
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 74 Mean Rewards -13.34 Epsilon 0.13230333333333422 Loss 2.322307586669922
[Episode 74] Using image: 298_78.npy
Agent moved into tumor at position: [0, 1]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.36073825
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5  1.  -0.5 -0.5]
state min/max example: 0.0 0.2737921
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5  1.  -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.55033004
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.36073825
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.59375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.36472148
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -2.   1.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 75 Mean Rewards -13.30 Epsilon 0.12452666666666755 Loss 2.0378518104553223
[Episode 75] Using image: 020_26.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7752562
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.45234334
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5  1.  -2.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.58203125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.3875724
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.8308581
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5  1.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.7423133
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.7015915
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 -0.5
sample rewards [-2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.6112731
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.42222223
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-2.  -0.5  1.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 76 Mean Rewards -13.28 Epsilon 0.11675000000000088 Loss 2.140946388244629
[Episode 76] Using image: 025_109.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Agent moved into tumor at position: [2, 2]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.59375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.7196871
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [ 1.  -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.83106434
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.36073825
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.65413535
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.60546875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.546875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.70212764
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 77 Mean Rewards -13.21 Epsilon 0.10897333333333421 Loss 2.162647247314453
[Episode 77] Using image: 288_45.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.36472148
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.9857627
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.36472148
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-2.  -0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.7752562
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 -0.5
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.36472148
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.42222223
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.   1.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -2.  -2.  -2.   1.  -0.5]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 78 Mean Rewards -13.21 Epsilon 0.10119666666666755 Loss 1.8713483810424805
[Episode 78] Using image: 003_69.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.59375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.34436882
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.34436882
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 -0.5
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.55033004
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.5234375 1.0
sample rewards [-0.5  1.  -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -2.   1.  -0.5 -0.5 -0.5 -2.   1.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5  1.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.36073825
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 79 Mean Rewards -13.17 Epsilon 0.09342000000000088 Loss 1.9832956790924072
[Episode 79] Using image: 170_67.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.36472148
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.45272207
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.59375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -0.5 -0.5  1.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5  1.  -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.5911111
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 -0.5
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.55859375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 80 Mean Rewards -13.13 Epsilon 0.08564333333333421 Loss 2.527278184890747
[Episode 80] Using image: 153_61.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.4481306
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -2.  -2.  -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.8741221
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5  1.  -2.  -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.8741221
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-2.  -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.12670565
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.59375 1.0
sample rewards [-0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.34436882
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.77232796
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.59375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.3839635
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.5911111
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.   1.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.9346505
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 81 Mean Rewards -13.09 Epsilon 0.07786666666666754 Loss 2.047119379043579
[Episode 81] Using image: 152_100.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [ 1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.   1.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.5234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5  1.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.81640625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.60546875 1.0
sample rewards [-2.   1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 0.5100671
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 82 Mean Rewards -13.05 Epsilon 0.07009000000000087 Loss 1.9056203365325928
[Episode 82] Using image: 234_39.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.5845272
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5  1.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.42222223
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.60546875 1.0
sample rewards [-2.  -0.5 -0.5  1.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.36073825
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.5845272
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 83 Mean Rewards -13.02 Epsilon 0.062313333333334206 Loss 1.9924579858779907
[Episode 83] Using image: 027_61.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.58203125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 -0.5
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.5703125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [ 1.  -2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.85643566
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.36636245
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 -0.5
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -2. ]
state min/max example: 0.0 0.36073825
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.36472148
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.9673684
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.5703125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.36073825
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 -0.5
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.9949622
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 84 Mean Rewards -12.98 Epsilon 0.05453666666666754 Loss 1.6715129613876343
[Episode 84] Using image: 162_106.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.5703125 1.0
sample rewards [-0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.55859375 1.0
sample rewards [-0.5 -2.  -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.8024316
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.58203125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.60546875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [ 1.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.39640132
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-2.  -2.   1.  -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.59375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.   1.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 85 Mean Rewards -12.96 Epsilon 0.04676000000000087 Loss 2.3712329864501953
[Episode 85] Using image: 047_87.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.68375134
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.7128378
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.74051154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.26171875
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.60546875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.60546875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 -0.5
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.60546875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.36073825
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -0.5  1.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5  1.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7015915
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 -0.5
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.40547588
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -0.5 -2.   1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 86 Mean Rewards -12.93 Epsilon 0.0389833333333342 Loss 2.0959770679473877
[Episode 86] Using image: 187_122.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.8308581
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.12670565
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.58203125 1.0
sample rewards [-0.5 -0.5 -0.5  1.  -0.5 -2.   1.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5  1.  -0.5 -2.  -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.59375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-2.  -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.3875724
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 -0.5
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.42222223
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.546875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.34436882
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.   1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5  1.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 87 Mean Rewards -12.90 Epsilon 0.031206666666667535 Loss 2.2212090492248535
[Episode 87] Using image: 047_87.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 -0.5
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.6112731
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.59375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.36472148
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.82370824
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.77232796
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.59375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.40356565
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 88 Mean Rewards -12.88 Epsilon 0.023430000000000867 Loss 2.1875460147857666
[Episode 88] Using image: 225_94.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.12670565
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.36073825
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.5234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.58203125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.   1.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-2.  -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.45692477
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5  1.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.58203125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.60546875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5  1.  -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 89 Mean Rewards -12.85 Epsilon 0.0156533333333342 Loss 1.9853832721710205
[Episode 89] Using image: 162_106.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.59375 1.0
sample rewards [-0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.60546875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.31241032
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.36472148
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.29383886
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.5981308
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.59375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.5486577
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 90 Mean Rewards -12.82 Epsilon 0.007876666666667533 Loss 2.177611827850342
[Episode 90] Using image: 325_17.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.5100671
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.3839635
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.60546875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.59375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.95821184
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.59375 1.0
sample rewards [-2.   1.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.5911111
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5  1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 -0.5
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.5911111
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 91 Mean Rewards -12.79 Epsilon 0.00010000000000086676 Loss 2.4829585552215576
[Episode 91] Using image: 089_46.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.55859375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.7752562
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 -0.5
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.5703125 1.0
sample rewards [-2.  -0.5  1.  -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.60305345
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.26171875
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.5612416
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.58203125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.3077153
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 92 Mean Rewards -12.76 Epsilon 0.0001 Loss 2.5003585815429688
[Episode 92] Using image: 225_94.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.83106434
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7128713
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.5703125 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5  1.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.   1.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.74609375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.546875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.55033004
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.60546875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [ 1.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.8658044
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -2.   1.  -0.5 -0.5 -0.5 -2.  -2.  -0.5 -2. ]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.26171875
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.91729325
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.53515625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.20278184
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 93 Mean Rewards -12.73 Epsilon 0.0001 Loss 2.289661169052124
[Episode 93] Using image: 037_83.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.58203125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.29383886
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.78125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5  1.  -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.8741221
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.60546875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.3839635
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.55859375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.55859375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.39198855
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.59375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.46286473
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 -0.5
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 94 Mean Rewards -12.70 Epsilon 0.0001 Loss 2.343247652053833
[Episode 94] Using image: 020_26.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -2.  -2. ]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.59375 1.0
sample rewards [-0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.20691489
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.60546875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.790932
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.60546875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5  1.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.5703125 1.0
sample rewards [-2.  -0.5 -0.5  1.  -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.5911111
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.8308581
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [ 1.  -2.  -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.7055369
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.546875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7752562
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.8046875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.36472148
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 95 Mean Rewards -12.67 Epsilon 0.0001 Loss 2.146782398223877
[Episode 95] Using image: 349_99.npy
Agent moved into tumor at position: [0, 1]
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-2.   1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1. ]
state min/max example: 0.0 0.36472148
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.72265625 -0.5
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.59375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.5911111
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.59375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.70291775
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.60546875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7423133
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.79296875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.60546875 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 96 Mean Rewards -12.62 Epsilon 0.0001 Loss 2.284515857696533
[Episode 96] Using image: 037_83.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.59375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.36472148
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.60546875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.60546875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -2.   1.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.58203125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.70212764
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.59375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 -0.5
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.85643566
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [ 1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7201592
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.59375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.59375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 97 Mean Rewards -12.60 Epsilon 0.0001 Loss 2.3210408687591553
[Episode 97] Using image: 001_49.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.60546875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.40547588
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.60546875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.3846154
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -2.  -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5]
state min/max example: 0.0 0.44360903
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 -0.5
sample rewards [-0.5 -0.5 -2.  -2.  -2.  -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.8250366
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.60546875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.5703125 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.7109375 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.60761344
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 98 Mean Rewards -12.57 Epsilon 0.0001 Loss 2.740302324295044
[Episode 98] Using image: 162_106.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 1.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-2.  -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.734375 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.5703125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.76953125 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -2.   1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.5703125 1.0
sample rewards [-0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-2.  -0.5 -0.5 -2.  -2.  -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.40547588
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.55859375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -2.   1.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.67578125 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 99 Mean Rewards -12.55 Epsilon 0.0001 Loss 3.2618606090545654
[Episode 99] Using image: 089_46.npy
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.69921875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.55859375 1.0
sample rewards [-2.   1.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7752562
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-2.  -0.5 -0.5 -0.5  1.  -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.58203125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.7422286
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2. ]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6640625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.7470011
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-2.  -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.55263156
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.62890625 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.59375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.65234375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.6171875 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.58203125 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.640625 1.0
sample rewards [-0.5 -0.5 -0.5 -2.  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
BATCH shapes (128, 60, 60) (128, 60, 60) (128,) (128,) (128,)
rewards stats: min,mean,max -2.0 -0.59375 1.0
sample rewards [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -2.  -0.5 -0.5]
state min/max example: 0.0 0.0
qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])
Episode 100 Mean Rewards -12.52 Epsilon 0.0001 Loss 2.6626691818237305

Episode limit reached.
>>> Training completed at  2025-10-30 23:36:49.396430
Q-values shape: torch.Size([1, 3])
(60, 60)
3
[0 1 2]
Starting at 0.7, decaying 0.007776666666666666, will reach 0.0001 after 100 episodes
 Found 30 training pairs out of 30 listed in CSV.
100%|| 30/30 [00:00<00:00, 136.25it/s]
Mask unique values (counts): Counter({0.0: 30, 2.0: 29, 4.0: 27, 1.0: 26})
Percentiles of %patches/ image that are 'inside': [0.0625 0.0625 0.0625 0.125  0.125  0.1875 0.25   0.25   0.25  ]
Mean fraction of patches per image that are inside:  0.13541666666666666
Warning: extracted patch has max value 0 at position: [0, 0]
[34m[1mwandb[0m: [33mWARNING[0m Calling wandb.login() after wandb.init() has no effect.
