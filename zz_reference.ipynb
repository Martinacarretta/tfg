{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ad2861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "import wandb\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "255fc97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tumor_ratio(mask_path):\n",
    "    mask = np.load(mask_path)\n",
    "    return np.mean(np.isin(mask, [1,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feae392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glioblastoma(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, image_path, mask_path, grid_size=4, render_mode=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load and normalize image exactly as described\n",
    "        self.image = np.load(image_path).astype(np.float32)\n",
    "        self.mask = np.load(mask_path).astype(np.uint8)\n",
    "        \n",
    "        # Normalize to [0,1] as in paper\n",
    "        img_min, img_max = self.image.min(), self.image.max()\n",
    "        if img_max > 1.0:\n",
    "            self.image = (self.image - img_min) / (img_max - img_min + 1e-8)\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.block_size = self.image.shape[0] // grid_size  # 240/4 = 60\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Exact action space from paper: 0=stay, 1=down, 2=right\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        # Observation: single 60x60 patch\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1,\n",
    "            shape=(self.block_size, self.block_size),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Always start at top-left as in paper Fig 1a\n",
    "        self.agent_pos = [0, 0]\n",
    "        self.current_step = 0\n",
    "        self.max_steps = 20  # Exactly as in paper\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Reset to top-left corner exactly as in paper\n",
    "        self.agent_pos = [0, 0]\n",
    "        self.current_step = 0\n",
    "        obs = self._get_obs()\n",
    "        info = {}\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        prev_pos = self.agent_pos.copy()\n",
    "        \n",
    "        # Exact movement logic from paper - only down and right\n",
    "        if action == 1 and self.agent_pos[0] < self.grid_size - 1:  # move down\n",
    "            self.agent_pos[0] += 1\n",
    "        elif action == 2 and self.agent_pos[1] < self.grid_size - 1:  # move right\n",
    "            self.agent_pos[1] += 1\n",
    "        # action == 0: stay still (no position change)\n",
    "\n",
    "        # Get reward using paper's exact scheme\n",
    "        reward = self._get_reward_paper_exact(action, prev_pos)\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        # Episode ends after exactly 20 steps as in paper\n",
    "        terminated = self.current_step >= self.max_steps\n",
    "        truncated = False\n",
    "        info = {}\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        r0 = self.agent_pos[0] * self.block_size\n",
    "        c0 = self.agent_pos[1] * self.block_size\n",
    "        patch = self.image[r0:r0+self.block_size, c0:c0+self.block_size].astype(np.float32)\n",
    "        return patch\n",
    "\n",
    "    def _get_reward_paper_exact(self, action, prev_pos):\n",
    "        \"\"\"EXACT reward scheme from paper Figure 1b-d\"\"\"\n",
    "        current_r0 = self.agent_pos[0] * self.block_size\n",
    "        current_c0 = self.agent_pos[1] * self.block_size\n",
    "        current_patch_mask = self.mask[current_r0:current_r0+self.block_size, \n",
    "                                     current_c0:current_c0+self.block_size]\n",
    "        \n",
    "        # Check if current position overlaps tumor (any tumor voxel)\n",
    "        current_on_tumor = np.any(np.isin(current_patch_mask, [1, 4]))\n",
    "        \n",
    "        # Check if previous position overlapped tumor\n",
    "        prev_r0 = prev_pos[0] * self.block_size\n",
    "        prev_c0 = prev_pos[1] * self.block_size\n",
    "        prev_patch_mask = self.mask[prev_r0:prev_r0+self.block_size,\n",
    "                                  prev_c0:prev_c0+self.block_size]\n",
    "        prev_on_tumor = np.any(np.isin(prev_patch_mask, [1, 4]))\n",
    "        \n",
    "        # Paper's exact reward logic from Figure 1:\n",
    "        if current_on_tumor and action == 0:  # On tumor and staying still\n",
    "            return +1.0\n",
    "        elif not current_on_tumor and action == 0:  # Off tumor and staying still  \n",
    "            return -2.0\n",
    "        elif current_on_tumor and action != 0:  # Moved to tumor\n",
    "            return +1.0\n",
    "        else:  # Moved but no tumor (action != 0 and not on tumor)\n",
    "            return -0.5\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode != \"human\":\n",
    "            return\n",
    "\n",
    "        vis_img = np.stack([self.image] * 3, axis=-1).astype(np.float32)\n",
    "        tumor_overlay = np.zeros_like(vis_img)\n",
    "        tumor_overlay[..., 0] = (self.mask > 0).astype(float)\n",
    "        \n",
    "        alpha = 0.4\n",
    "        vis_img = (1 - alpha) * vis_img + alpha * tumor_overlay\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(3, 3))\n",
    "        ax.imshow(vis_img, cmap='gray', origin='upper')\n",
    "\n",
    "        # Draw grid\n",
    "        for i in range(1, self.grid_size):\n",
    "            ax.axhline(i * self.block_size, color='white', lw=1, alpha=0.5)\n",
    "            ax.axvline(i * self.block_size, color='white', lw=1, alpha=0.5)\n",
    "\n",
    "        # Draw agent position\n",
    "        r0 = self.agent_pos[0] * self.block_size\n",
    "        c0 = self.agent_pos[1] * self.block_size\n",
    "        rect = patches.Rectangle(\n",
    "            (c0, r0),\n",
    "            self.block_size,\n",
    "            self.block_size,\n",
    "            linewidth=2,\n",
    "            edgecolor='yellow',\n",
    "            facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        ax.set_title(f\"Agent at {self.agent_pos} | Step {self.current_step}\")\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ad84f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "    \"\"\"EXACT architecture from paper - 4 conv layers + 3 FC layers\"\"\"\n",
    "    \n",
    "    def __init__(self, env, learning_rate=1e-4, device='cpu'):\n",
    "        super(DQN, self).__init__()\n",
    "        self.device = device\n",
    "        self.n_outputs = env.action_space.n\n",
    "        self.actions = np.arange(env.action_space.n)\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Exact CNN architecture from paper: 4 conv layers with 32 channels each\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),  # 60x60 -> 30x30\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),  # 30x30 -> 15x15\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),  # 15x15 -> 8x8\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),  # 8x8 -> 4x4\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        \n",
    "        # Calculate flattened size\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, 60, 60)\n",
    "            n_flatten = self.features(dummy_input).view(1, -1).size(1)\n",
    "            \n",
    "        # Exact FC architecture from paper: 512 -> 256 -> 128 -> 3\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_flatten, 512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, self.n_outputs)  # 3 actions\n",
    "        )\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        if self.device == 'cuda':\n",
    "            self.cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        features_flat = features.view(x.size(0), -1)\n",
    "        q_values = self.fc(features_flat)\n",
    "        return q_values\n",
    "    \n",
    "    def get_action(self, state, epsilon=0.05):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.choice(self.actions)\n",
    "        else:\n",
    "            qvals = self.get_qvals(state)\n",
    "            return torch.argmax(qvals, dim=-1).item()\n",
    "    \n",
    "    def get_qvals(self, state):\n",
    "        if isinstance(state, np.ndarray):\n",
    "            if state.ndim == 2:\n",
    "                state = np.expand_dims(np.expand_dims(state, 0), 0)\n",
    "            elif state.ndim == 3:\n",
    "                if state.shape[0] != 1:\n",
    "                    state = np.expand_dims(state, 1)\n",
    "                    \n",
    "        state_t = torch.FloatTensor(state).to(self.device)\n",
    "        qvals = self.forward(state_t)\n",
    "        return qvals\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        self.buffer.append((state.copy(), action, reward, done, next_state.copy()))\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, dones, next_states = zip(*batch)\n",
    "        return (\n",
    "            np.array(states, dtype=np.float32),\n",
    "            np.array(actions, dtype=np.int64),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.bool_),\n",
    "            np.array(next_states, dtype=np.float32),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebd00fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"EXACT training procedure from paper with SINGLE replay buffer\"\"\"\n",
    "    \n",
    "    def __init__(self, env, dnnetwork, train_pairs, epsilon=0.7, eps_decay=1e-4, \n",
    "                 epsilon_min=1e-4, batch_size=128, gamma=0.99):\n",
    "        self.env = env\n",
    "        self.dnnetwork = dnnetwork\n",
    "        self.target_network = deepcopy(dnnetwork)\n",
    "        self.target_network.optimizer = None\n",
    "        \n",
    "        # EXACT PAPER: Single replay buffer with 15,000 capacity\n",
    "        self.replay_buffer = ReplayBuffer(capacity=15000)\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.eps_decay = eps_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.train_pairs = train_pairs\n",
    "        \n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        self.update_loss = []\n",
    "        self.training_rewards = []\n",
    "        self.mean_training_rewards = []\n",
    "        self.total_reward = 0\n",
    "        self.step_count = 0\n",
    "        self.state0 = None\n",
    "\n",
    "    def take_step(self, eps, mode='train'):\n",
    "        if mode == 'explore':\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            action = self.dnnetwork.get_action(self.state0, eps)\n",
    "            self.step_count += 1\n",
    "            \n",
    "        new_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "        done = terminated or truncated\n",
    "        self.total_reward += reward\n",
    "        \n",
    "        # Store in SINGLE shared replay buffer (as in paper)\n",
    "        self.replay_buffer.append(self.state0, action, reward, done, new_state)\n",
    "        self.state0 = new_state.copy()\n",
    "        \n",
    "        if done:\n",
    "            self.state0 = self.env.reset()[0]\n",
    "        return done\n",
    "\n",
    "    def train(self, gamma=0.99, max_episodes=90, \n",
    "              dnn_update_frequency=4, dnn_sync_frequency=2000):\n",
    "        \"\"\"EXACT training procedure from paper - 90 episodes, 30 images\"\"\"\n",
    "        \n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Fill SINGLE replay buffer with random experiences from all images\n",
    "        print(\"Filling replay buffer...\")\n",
    "        for img_path, mask_path in self.train_pairs:\n",
    "            self.env = Glioblastoma(img_path, mask_path, grid_size=4)\n",
    "            self.state0, _ = self.env.reset()\n",
    "\n",
    "            # Add experiences from this image to the shared buffer\n",
    "            for _ in range(500):  # Distribute experiences across all images\n",
    "                self.take_step(self.epsilon, mode='explore')\n",
    "\n",
    "        # Training exactly as described\n",
    "        episode = 0\n",
    "        training = True\n",
    "        \n",
    "        print(\"Training for 90 episodes as in paper...\")\n",
    "        pbar = tqdm(total=max_episodes, desc=\"Training\")\n",
    "        \n",
    "        while training and episode < max_episodes:\n",
    "            # Sample random image from training set as described\n",
    "            img_path, mask_path = random.choice(self.train_pairs)\n",
    "            self.env = Glioblastoma(img_path, mask_path, grid_size=4)\n",
    "            self.state0, _ = self.env.reset()\n",
    "            self.total_reward = 0\n",
    "\n",
    "            # Run episode for exactly 20 steps\n",
    "            for step in range(self.env.max_steps):\n",
    "                done = self.take_step(self.epsilon, mode='train')\n",
    "\n",
    "                # Update network every 4 steps as typical in DQN\n",
    "                if self.step_count % dnn_update_frequency == 0 and len(self.replay_buffer.buffer) >= self.batch_size:\n",
    "                    self.update()\n",
    "\n",
    "                # Sync target network less frequently\n",
    "                if self.step_count % dnn_sync_frequency == 0:\n",
    "                    self.target_network.load_state_dict(self.dnnetwork.state_dict())\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # Episode complete\n",
    "            episode += 1\n",
    "            pbar.update(1)\n",
    "            \n",
    "            # Store metrics\n",
    "            self.training_rewards.append(self.total_reward)\n",
    "            \n",
    "            # Paper's exact epsilon decay: linear from 0.7 to 1e-4 over 90 episodes\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon - self.eps_decay)\n",
    "            #self.epsilon = max(self.epsilon_min, self.epsilon - self.eps_decay * self.env.max_steps)\n",
    "\n",
    "            \n",
    "            # Log progress\n",
    "            if episode % 10 == 0:\n",
    "                mean_reward = np.mean(self.training_rewards[-10:])\n",
    "                current_loss = np.mean(self.update_loss) if self.update_loss else 0\n",
    "                print(f\"Episode {episode:3d} | Mean Reward: {mean_reward:7.2f} | Epsilon: {self.epsilon:.4f} | Loss: {current_loss:.4f}\")\n",
    "                \n",
    "                wandb.log({\n",
    "                    'episode': episode,\n",
    "                    'mean_reward': mean_reward,\n",
    "                    'epsilon': self.epsilon,\n",
    "                    'loss': current_loss\n",
    "                })\n",
    "            \n",
    "            self.update_loss = []  # Reset loss tracking\n",
    "\n",
    "        pbar.close()\n",
    "        print(\"Training completed\")\n",
    "\n",
    "    def calculate_loss(self, batch):\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "        \n",
    "        # Add channel dimension for CNN\n",
    "        states = torch.FloatTensor(states).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(next_states).unsqueeze(1)\n",
    "        \n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1)\n",
    "        dones = torch.BoolTensor(dones)\n",
    "        \n",
    "        # Current Q values\n",
    "        current_q = self.dnnetwork.get_qvals(states).gather(1, actions).squeeze()\n",
    "        \n",
    "        # Target Q values\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_network.get_qvals(next_states).max(1)[0]\n",
    "            next_q[dones] = 0.0\n",
    "            target_q = rewards + self.gamma * next_q\n",
    "        \n",
    "        # MSE loss\n",
    "        loss = nn.MSELoss()(current_q, target_q)\n",
    "        return loss\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.replay_buffer.buffer) < self.batch_size:\n",
    "            return\n",
    "            \n",
    "        batch = self.replay_buffer.sample_batch(self.batch_size)\n",
    "        loss = self.calculate_loss(batch)\n",
    "        \n",
    "        self.dnnetwork.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.dnnetwork.parameters(), 1.0)\n",
    "        self.dnnetwork.optimizer.step()\n",
    "        \n",
    "        self.update_loss.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c19eeb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXACT HYPERPARAMETERS FROM PAPER\n",
    "LR = 1e-4\n",
    "MEMORY_SIZE = 15000\n",
    "MAX_EPISODES = 90\n",
    "EPSILON_START = 0.7\n",
    "EPSILON_DECAY = 1e-4  # Linear decay per episode\n",
    "EPSILON_MIN = 1e-4\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdbe5b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30 training pairs\n"
     ]
    }
   ],
   "source": [
    "# Load your data\n",
    "base_dir = \"/home/martina/codi2/4year/tfg/training_set_npy\"\n",
    "csv_path = \"/home/martina/codi2/4year/tfg/training_dataset_slices.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "df[\"image_path\"] = df.apply(\n",
    "    lambda row: os.path.join(base_dir, f\"{row['Patient']:03d}_{row['SliceIndex']}.npy\"), axis=1\n",
    ")\n",
    "df[\"mask_path\"] = df.apply(\n",
    "    lambda row: os.path.join(base_dir, f\"{row['Patient']:03d}_{row['SliceIndex']}_mask.npy\"), axis=1\n",
    ")\n",
    "\n",
    "train_pairs = [\n",
    "    (img, mask) for img, mask in zip(df[\"image_path\"], df[\"mask_path\"])\n",
    "    if os.path.exists(img) and os.path.exists(mask) # and tumor_ratio(mask) >= 0.01     # keep slices with ≥1% tumor\n",
    "]\n",
    "\n",
    "\n",
    "print(f\"Found {len(train_pairs)} training pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93b060a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmartinacarrettab\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/martina/codi2/4year/tfg/wandb/run-20251103_161347-pulhjmdx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/martinacarrettab/TFG_ExactPaperReplication/runs/pulhjmdx' target=\"_blank\">efficient-universe-3</a></strong> to <a href='https://wandb.ai/martinacarrettab/TFG_ExactPaperReplication' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/martinacarrettab/TFG_ExactPaperReplication' target=\"_blank\">https://wandb.ai/martinacarrettab/TFG_ExactPaperReplication</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/martinacarrettab/TFG_ExactPaperReplication/runs/pulhjmdx' target=\"_blank\">https://wandb.ai/martinacarrettab/TFG_ExactPaperReplication/runs/pulhjmdx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/martinacarrettab/TFG_ExactPaperReplication/runs/pulhjmdx?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f4114656500>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize with exact paper specifications\n",
    "env = Glioblastoma(*train_pairs[0], grid_size=4)\n",
    "net = DQN(env, learning_rate=LR, device='cpu')\n",
    "agent = DQNAgent(env, net, train_pairs,\n",
    "                epsilon=EPSILON_START, \n",
    "                eps_decay=EPSILON_DECAY,\n",
    "                epsilon_min=EPSILON_MIN,\n",
    "                batch_size=BATCH_SIZE, \n",
    "                gamma=GAMMA)\n",
    "\n",
    "# Train\n",
    "wandb.init(project=\"TFG_ExactPaperReplication\", config={\n",
    "    \"lr\": LR, \"episodes\": MAX_EPISODES, \"epsilon_start\": EPSILON_START,\n",
    "    \"epsilon_decay\": EPSILON_DECAY, \"epsilon_min\": EPSILON_MIN,\n",
    "    \"batch_size\": BATCH_SIZE, \"gamma\": GAMMA\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3370619d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling replay buffer...\n",
      "Training for 90 episodes as in paper...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "000089cb7797445ab790a060a5d194ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  10 | Mean Reward:  -17.35 | Epsilon: 0.6990 | Loss: 0.2671\n",
      "Episode  20 | Mean Reward:  -13.90 | Epsilon: 0.6980 | Loss: 0.2139\n",
      "Episode  30 | Mean Reward:  -13.60 | Epsilon: 0.6970 | Loss: 0.2352\n",
      "Episode  40 | Mean Reward:  -13.60 | Epsilon: 0.6960 | Loss: 0.2369\n",
      "Episode  50 | Mean Reward:  -17.35 | Epsilon: 0.6950 | Loss: 0.1822\n",
      "Episode  60 | Mean Reward:  -16.15 | Epsilon: 0.6940 | Loss: 0.1987\n",
      "Episode  70 | Mean Reward:  -15.40 | Epsilon: 0.6930 | Loss: 0.2298\n",
      "Episode  80 | Mean Reward:  -15.40 | Epsilon: 0.6920 | Loss: 0.2048\n",
      "Episode  90 | Mean Reward:  -16.30 | Epsilon: 0.6910 | Loss: 0.2368\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "agent.train(max_episodes=MAX_EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e05cc6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>▁▂▃▄▅▅▆▇█</td></tr><tr><td>epsilon</td><td>█▇▆▅▅▄▃▂▁</td></tr><tr><td>loss</td><td>█▄▅▆▁▂▅▃▆</td></tr><tr><td>mean_reward</td><td>▁▇██▁▃▅▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>90</td></tr><tr><td>epsilon</td><td>0.691</td></tr><tr><td>loss</td><td>0.23681</td></tr><tr><td>mean_reward</td><td>-16.3</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">efficient-universe-3</strong> at: <a href='https://wandb.ai/martinacarrettab/TFG_ExactPaperReplication/runs/pulhjmdx' target=\"_blank\">https://wandb.ai/martinacarrettab/TFG_ExactPaperReplication/runs/pulhjmdx</a><br> View project at: <a href='https://wandb.ai/martinacarrettab/TFG_ExactPaperReplication' target=\"_blank\">https://wandb.ai/martinacarrettab/TFG_ExactPaperReplication</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251103_161347-pulhjmdx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a315cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, test_pairs, num_episodes=30):\n",
    "    \"\"\"EXACT evaluation from paper - 30 test images, 20 steps each\"\"\"\n",
    "    correct = 0\n",
    "    \n",
    "    for img_path, mask_path in test_pairs:\n",
    "        env = Glioblastoma(img_path, mask_path, grid_size=4)\n",
    "        state, _ = env.reset()\n",
    "        \n",
    "        found_tumor = False\n",
    "        # Run for exactly 20 steps as in paper\n",
    "        for step in range(env.max_steps):\n",
    "            action = agent.dnnetwork.get_action(state, epsilon=0.0)  # No exploration\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            # Check if agent is on tumor at any point\n",
    "            r0 = env.agent_pos[0] * env.block_size\n",
    "            c0 = env.agent_pos[1] * env.block_size\n",
    "            patch_mask = env.mask[r0:r0+env.block_size, c0:c0+env.block_size]\n",
    "            if np.any(np.isin(patch_mask, [1, 4])):\n",
    "                found_tumor = True\n",
    "                break\n",
    "                \n",
    "        # Check final position if tumor not found during steps\n",
    "        if not found_tumor:\n",
    "            r0 = env.agent_pos[0] * env.block_size\n",
    "            c0 = env.agent_pos[1] * env.block_size\n",
    "            patch_mask = env.mask[r0:r0+env.block_size, c0:c0+env.block_size]\n",
    "            if np.any(np.isin(patch_mask, [1, 4])):\n",
    "                found_tumor = True\n",
    "        \n",
    "        if found_tumor:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / len(test_pairs)\n",
    "    print(f\"Test Accuracy: {accuracy:.1%} ({correct}/{len(test_pairs)})\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e8c7c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30 training pairs\n",
      "Test Accuracy: 23.3% (7/30)\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"/home/martina/codi2/4year/tfg/training_set_npy\"\n",
    "csv_path = \"/home/martina/codi2/4year/tfg/training_dataset_slices.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "df[\"image_path\"] = df.apply(\n",
    "    lambda row: os.path.join(base_dir, f\"{row['Patient']:03d}_{row['SliceIndex']}.npy\"), axis=1\n",
    ")\n",
    "df[\"mask_path\"] = df.apply(\n",
    "    lambda row: os.path.join(base_dir, f\"{row['Patient']:03d}_{row['SliceIndex']}_mask.npy\"), axis=1\n",
    ")\n",
    "\n",
    "train_pairs = [\n",
    "    (img, mask) for img, mask in zip(df[\"image_path\"], df[\"mask_path\"])\n",
    "    if os.path.exists(img) and os.path.exists(mask)\n",
    "]\n",
    "\n",
    "print(f\"Found {len(train_pairs)} training pairs\")\n",
    "\n",
    "# Evaluate\n",
    "test_accuracy = evaluate_agent(agent, train_pairs[:30], 30)  # Using first 30 as test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6b3245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
