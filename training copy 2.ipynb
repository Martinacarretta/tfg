{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cb7e8c3",
   "metadata": {},
   "source": [
    "# Notebook to experiment with training:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bc7211",
   "metadata": {},
   "source": [
    "## Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13a2d983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import torch\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "import wandb\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import torch.nn as nn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a06597a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glioblastoma(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 4} \n",
    "    # The metadata of the environment, e.g. {“render_modes”: [“rgb_array”, “human”], “render_fps”: 30}. \n",
    "    # For Jax or Torch, this can be indicated to users with “jax”=True or “torch”=True.\n",
    "\n",
    "    def __init__(self, image_path, mask_path, grid_size=4, tumor_threshold=0.01, render_mode=\"human\"): # cosntructor with the brain image, the mask and a size\n",
    "        super().__init__() # parent class\n",
    "        \n",
    "        self.image = np.load(image_path).astype(np.float32)\n",
    "        self.mask = np.load(mask_path).astype(np.uint8)\n",
    "        \n",
    "        img_min, img_max = self.image.min(), self.image.max()\n",
    "        if img_max > 1.0:  # only normalize if not already in [0, 1]\n",
    "            self.image = (self.image - img_min) / (img_max - img_min + 1e-8) #avoid division by 0\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.block_size = self.image.shape[0] // grid_size  # 240/4 = 60\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Define action and observation spaces\n",
    "        # Actions: 0 = stay, 1 = move down, 2 = move right\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        # Observations: grayscale patch (normalized 0-1)\n",
    "        # apparently Neural networks train better when inputs are scaled to small, \n",
    "        # consistent ranges rather than raw 0–255 values.\n",
    "        self.observation_space = spaces.Box( # Supports continuous (and discrete) vectors or matrices\n",
    "            low=0, high=1, # Data has been normalized\n",
    "            shape=(self.block_size, self.block_size), # shape of the observation\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.agent_pos = [0, 0] # INITIAL POSITION AT TOP LEFT\n",
    "        self.current_step = 0 # initialize counter\n",
    "        self.max_steps = 20  # like in the paper\n",
    "\n",
    "        self.tumor_threshold = tumor_threshold # 15% of the patch must be tumor to consider that the agent is inside the tumor region\n",
    "\n",
    "    def reset(self, seed=None, options=None): # new episode where we initialize the state. \n",
    "        super().reset(seed=seed) # parent\n",
    "        \n",
    "        # reset\n",
    "        self.agent_pos = [0, 0]  # top-left corner\n",
    "        self.current_step = 0\n",
    "        obs = self._get_obs()\n",
    "        info = {}\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "\n",
    "        prev_pos = self.agent_pos.copy() # for reward computation taking into consideration the transition changes\n",
    "        \n",
    "        # Apply action (respect grid boundaries)\n",
    "        if action == 1 and self.agent_pos[0] < self.grid_size - 1:\n",
    "            self.agent_pos[0] += 1  # move down\n",
    "        elif action == 2 and self.agent_pos[1] < self.grid_size - 1:\n",
    "            self.agent_pos[1] += 1  # move right\n",
    "        # else, the agent doesn't move so the observation \n",
    "        # and reward will be calculated from the same position\n",
    "        # no need to compute self.agent_po\n",
    "\n",
    "        reward = self._get_reward(action, prev_pos)\n",
    "                \n",
    "        obs = self._get_obs()\n",
    "\n",
    "        # Episode ends\n",
    "        terminated = self.current_step >= self.max_steps\n",
    "        truncated = False  # we don’t need truncation here\n",
    "        info = {}\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        r0 = self.agent_pos[0] * self.block_size # row start\n",
    "        c0 = self.agent_pos[1] * self.block_size # col start\n",
    "        \n",
    "        patch = self.image[r0:r0+self.block_size, c0:c0+self.block_size].astype(np.float32)\n",
    "\n",
    "        if patch.max() == 0: # DEBUGGING\n",
    "            print(\"Warning: extracted patch has max value 0 at position:\", self.agent_pos)\n",
    "        return patch\n",
    "\n",
    "    def _get_reward(self, action, prev_pos):        \n",
    "        # look position of the agent in the mask\n",
    "        r0 = self.agent_pos[0] * self.block_size\n",
    "        c0 = self.agent_pos[1] * self.block_size\n",
    "        patch_mask = self.mask[r0:r0+self.block_size, c0:c0+self.block_size]\n",
    "        \n",
    "        # Now that i have the patch where i was and the patch where i am, i can check if there is tumor in any of them\n",
    "        # tumor is labeled as 1 or 4 in the mask        \n",
    "        # label 2 is edema\n",
    "        \n",
    "        # first get a count of the tumor pixels in the patch. \n",
    "        tumor_count_curr = np.sum(np.isin(patch_mask, [1, 4]))\n",
    "        total = self.block_size * self.block_size # to compute the percentage\n",
    "        # Determine if patch has more than self.tumor_threshold of tumor\n",
    "        inside = (tumor_count_curr / total) >= self.tumor_threshold\n",
    "\n",
    "        if not inside and action == 0:  # outside and staying still\n",
    "            return -2.0\n",
    "        elif inside and action == 0:    # inside and staying still  \n",
    "            print(\"Agent stayed at tumor position\")\n",
    "            return 1.0\n",
    "        elif not inside and action != 0: # moved but still outside\n",
    "            return -0.5\n",
    "        elif inside and action != 0:     # moved into tumor\n",
    "            print(\"Agent moved into tumor at position:\", self.agent_pos)\n",
    "            return 1.0\n",
    "        return 0.0\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode != \"human\": # would be rgb_array or ansi\n",
    "            return  # Only render in human mode\n",
    "\n",
    "        # Create RGB visualization image\n",
    "        # not necessary since it's grayscale, but i want to draw the mask and position\n",
    "        vis_img = np.stack([self.image] * 3, axis=-1).astype(np.float32)\n",
    "\n",
    "        # Overlay tumor mask in red [..., 0] \n",
    "        tumor_overlay = np.zeros_like(vis_img) # do all blank but here we have 3 channels, mask is 2D\n",
    "        tumor_overlay[..., 0] = (self.mask > 0).astype(float) # red channel. set to float to avoid issues when blending in vis_img\n",
    "\n",
    "        # transparency overlay (crec que es el mateix valor que tinc a l'altra notebook)\n",
    "        alpha = 0.4\n",
    "        vis_img = (1 - alpha) * vis_img + alpha * tumor_overlay\n",
    "\n",
    "        # Plotting\n",
    "        fig, ax = plt.subplots(figsize=(3, 3))\n",
    "        ax.imshow(vis_img, cmap='gray', origin='upper')\n",
    "\n",
    "        # Draw grid lines\n",
    "        # alpha for transparency again\n",
    "        for i in range(1, self.grid_size):\n",
    "            ax.axhline(i * self.block_size, color='white', lw=1, alpha=0.5)\n",
    "            ax.axvline(i * self.block_size, color='white', lw=1, alpha=0.5)\n",
    "\n",
    "        # Draw agent position\n",
    "        r0 = self.agent_pos[0] * self.block_size\n",
    "        c0 = self.agent_pos[1] * self.block_size\n",
    "        rect = patches.Rectangle(\n",
    "            (c0, r0), # (x,y) bottom left corner\n",
    "            self.block_size, # width\n",
    "            self.block_size, # height\n",
    "            linewidth=2,\n",
    "            edgecolor='yellow',\n",
    "            facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        ax.set_title(f\"Agent at {self.agent_pos} | Step {self.current_step}\")\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def current_patch_overlap_with_lesion(self): # FALTAAA chat\n",
    "        \"\"\"\n",
    "        Returns the number of overlapping lesion pixels between the agent's\n",
    "        current patch and the ground-truth mask.\n",
    "        If > 0, the agent is correctly over the lesion (TP).\n",
    "        \"\"\"\n",
    "        # get current agent patch boundaries\n",
    "        row, col = self.agent_pos\n",
    "        patch_h = self.grid_size\n",
    "        patch_w = self.grid_size\n",
    "\n",
    "        y0 = row * patch_h\n",
    "        y1 = y0 + patch_h\n",
    "        x0 = col * patch_w\n",
    "        x1 = x0 + patch_w\n",
    "\n",
    "        # extract mask region under current patch\n",
    "        patch_mask = self.mask[y0:y1, x0:x1]\n",
    "\n",
    "        # count how many pixels of lesion (nonzero)\n",
    "        overlap = np.sum(patch_mask > 0)\n",
    "        return overlap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8d671b",
   "metadata": {},
   "source": [
    "# DQN:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956c9099",
   "metadata": {},
   "source": [
    "https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef6b41db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 60)\n",
      "3\n",
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "trial_image_path = f\"/home/martina/codi2/4year/tfg/training_set_npy/001_49.npy\"\n",
    "trial_mask_path = f\"/home/martina/codi2/4year/tfg/training_set_npy/001_49_mask.npy\"\n",
    "\n",
    "env = Glioblastoma(trial_image_path, trial_mask_path, grid_size=4)\n",
    "print(env.observation_space.shape)\n",
    "print(env.action_space.n)\n",
    "print(np.arange(env.action_space.n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b53a716",
   "metadata": {},
   "source": [
    "neural network that approximates the Q-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90c34bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, env, learning_rate=1e-3, device='cpu'):\n",
    "        super(DQN, self).__init__()\n",
    "        self.device = device\n",
    "        self.n_inputs = env.observation_space.shape[0] # 60\n",
    "        self.n_outputs = env.action_space.n # 3\n",
    "        self.actions = np.arange(env.action_space.n) # np.array([0, 1, 2])\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        input_channels = 1\n",
    "        height, width = env.observation_space.shape  # 60, 60   \n",
    "        \n",
    "        ### Construction of the neural network\n",
    "        ## features first and then fully connected layers\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        \n",
    "        # flatten \n",
    "        with  torch.no_grad(): # FALTA MIRAR Q ES AIXO\n",
    "            dummy_input = torch.zeros(1, input_channels, height, width) # batch size 1\n",
    "            n_flatten = self.features(dummy_input).view(1, -1).size(1)\n",
    "            \n",
    "        # nn.Linear (in features, out features)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_flatten, 512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, self.n_outputs)\n",
    "        )\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        ### Work with CUDA is allowed\n",
    "        if self.device == 'cuda':\n",
    "            self.to(self.device).cuda()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, 1, 60, 60)\n",
    "        features = self.features(x)\n",
    "        features_flat = features.view(x.size(0), -1)\n",
    "        q_values = self.fc(features_flat)\n",
    "        return q_values\n",
    "    \n",
    "    # e-greedy method\n",
    "    def get_action(self, state, epsilon=0.05):\n",
    "        if np.random.random() < epsilon:\n",
    "            # random action -- Exploration\n",
    "            action = np.random.choice(self.actions)  \n",
    "        else:\n",
    "            # Q-value based action -- Exploitation\n",
    "            qvals = self.get_qvals(state)  \n",
    "            if qvals.dim() == 2 and qvals.size(0) == 1:\n",
    "                action = torch.argmax(qvals, dim=-1).item()\n",
    "            else:\n",
    "                action = torch.argmax(qvals, dim=-1)[0].item()\n",
    "\n",
    "        return int(action)\n",
    "    \n",
    "    # forward pass through conv and fc layers\n",
    "    def get_qvals(self, state):\n",
    "        # Convert (60,60) → (1,1,60,60)\n",
    "        if isinstance(state, np.ndarray):\n",
    "            if state.ndim == 2:  # grayscale single image (60x60)\n",
    "                state = np.expand_dims(np.expand_dims(state, 0), 0) # (1,1,60,60)\n",
    "                \n",
    "            elif state.ndim == 3:  # batch or stacked images (batch, 60, 60)\n",
    "                if state.shape[0] != 1: #batch\n",
    "                    state = np.expand_dims(state, 1)\n",
    "                    \n",
    "        state_t = torch.FloatTensor(state).to(self.device)\n",
    "        qvals = self.forward(state_t)  # Use the forward method instead\n",
    "        return qvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3283f892",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self, env, dnnetwork, buffer, epsilon=0.1, eps_decay=0.99, epsilon_min=0.01, batch_size=32, gamma=0.99):\n",
    "        self.env = env\n",
    "        self.dnnetwork = dnnetwork # main network\n",
    "        self.target_network = deepcopy(dnnetwork) # prevents the target Q-values from changing with every single update\n",
    "        self.target_network.optimizer = None # paper said target net is only  weights, no optimizer\n",
    "        self.buffer = buffer # store experiences\n",
    "        self.epsilon = epsilon # initial epsilon for e-greedy\n",
    "        self.eps_decay = eps_decay # decay of epsilon after each episode to balance exploration and exploitation\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size # size of the mini-batch for training\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # block of the last X episodes to calculate the average reward \n",
    "        self.nblock = 100 \n",
    "        # average reward used to determine if the agent has learned to play\n",
    "        #self.reward_threshold = self.env.spec.reward_threshold \n",
    "        self.initialize()\n",
    "    \n",
    "    def initialize(self): # reset variables at the beginning of training\n",
    "        self.update_loss = []\n",
    "        self.training_rewards = []\n",
    "        self.mean_training_rewards = []\n",
    "        self.sync_eps = []\n",
    "        self.total_reward = 0\n",
    "        self.step_count = 0\n",
    "        self.state0 = self.env.reset()[0]\n",
    "        \n",
    "    ## Take new action\n",
    "    def take_step(self, eps, mode='train'):\n",
    "        if mode == 'explore':\n",
    "            # random action in burn-in and in the exploration phase (epsilon)\n",
    "            action = self.env.action_space.sample() \n",
    "        else:\n",
    "            # Action based on the Q-value (max Q-value)\n",
    "            action = self.dnnetwork.get_action(self.state0, eps)\n",
    "            self.step_count += 1\n",
    "            \n",
    "        # Execute action and get reward and new state\n",
    "        new_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "        done = terminated or truncated\n",
    "        self.total_reward += reward\n",
    "        # save experience in the buffer\n",
    "        self.buffer.append(self.state0, action, reward, done, new_state) \n",
    "        self.state0 = new_state.copy()\n",
    "        \n",
    "        if done:\n",
    "            self.state0 = self.env.reset()[0]\n",
    "        return done\n",
    "\n",
    "            \n",
    "    ## Training\n",
    "    def train(self, train_pairs, gamma=0.99, max_episodes=50000, \n",
    "              dnn_update_frequency=4,\n",
    "              dnn_sync_frequency=200):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Fill the buffer with N random experiences\n",
    "        print(\"Filling replay buffer...\")\n",
    "        while len(self.buffer.buffer) < self.batch_size:\n",
    "            # pick ONE random image for this burn-in episode\n",
    "            img_path, mask_path = random.choice(train_pairs)\n",
    "            self.env = Glioblastoma(img_path, mask_path, grid_size=self.env.grid_size, tumor_threshold=self.env.tumor_threshold)\n",
    "            self.state0, _ = self.env.reset()\n",
    "            #run short episode of 20 random steps on this image\n",
    "            for _ in range (self.env.max_steps):\n",
    "                self.take_step(self.epsilon, mode='explore')\n",
    "\n",
    "            \n",
    "        # Store metrics locally to plot\n",
    "        self.episode_rewards = []\n",
    "        self.mean_rewards = []\n",
    "        self.epsilon_values = []\n",
    "        self.loss_values = []\n",
    " \n",
    "        episode = 0\n",
    "        training = True\n",
    "        print(\"Training...\")\n",
    "        while training and episode < max_episodes:\n",
    "            img_path, mask_path = random.choice(train_pairs)\n",
    "            print(f\"[Episode {episode}] Using image: {os.path.basename(img_path)}\") # debugging\n",
    "\n",
    "            self.env = Glioblastoma(img_path, mask_path, grid_size=self.env.grid_size, tumor_threshold=self.env.tumor_threshold)\n",
    "            self.state0, _ = self.env.reset()\n",
    "            self.total_reward = 0\n",
    "            \n",
    "            for step in range(self.env.max_steps):\n",
    "                gamedone = self.take_step(self.epsilon, mode='train')\n",
    "               \n",
    "                # Upgrade main network\n",
    "                if self.step_count % dnn_update_frequency == 0:\n",
    "                    self.update()\n",
    "                # Synchronize the main network and the target network\n",
    "                if self.step_count % dnn_sync_frequency == 0:\n",
    "                    self.target_network.load_state_dict(\n",
    "                        self.dnnetwork.state_dict())\n",
    "                    self.sync_eps.append(episode)\n",
    "                    \n",
    "                if gamedone:\n",
    "                    episode += 1                   \n",
    "                    # Save the rewards\n",
    "                    self.training_rewards.append(self.total_reward)\n",
    "                    # Calculate the average reward for the last X episodes\n",
    "                    if len(self.training_rewards) >= self.nblock:\n",
    "                        mean_rewards = np.mean(self.training_rewards[-self.nblock:])\n",
    "                    else:\n",
    "                        mean_rewards = np.mean(self.training_rewards)  # Use all rewards if less than nblock\n",
    "                    \n",
    "                    self.mean_training_rewards.append(mean_rewards)\n",
    "\n",
    "                    print(\"Episode {:d} Mean Rewards {:.2f} Epsilon {} Loss {}\".format(episode, mean_rewards, self.epsilon, np.mean(self.update_loss)))\n",
    "                    \n",
    "                    wandb.log({\n",
    "                        'episode': episode,\n",
    "                        'mean_rewards': mean_rewards,\n",
    "                        'episode reward': self.total_reward,\n",
    "                        'epsilon': self.epsilon,\n",
    "                        'loss': np.mean(self.update_loss)\n",
    "                    }, step=episode)\n",
    "                    \n",
    "                    # Append metrics to lists for plotting\n",
    "                    self.episode_rewards.append(self.total_reward)\n",
    "                    self.mean_rewards.append(mean_rewards)\n",
    "                    self.epsilon_values.append(self.epsilon)\n",
    "                    self.loss_values.append(np.mean(self.update_loss))\n",
    "                    \n",
    "                    self.update_loss = []\n",
    "\n",
    "                    # Check if there are still episodes left\n",
    "                    if episode >= max_episodes:\n",
    "                        training = False\n",
    "                        print('\\nEpisode limit reached.')\n",
    "                        break\n",
    "                    \n",
    "                    # Update epsilon according to the fixed decay rate # SUBTRACTION FROM PPAPER\n",
    "                    self.epsilon = max(self.epsilon - self.eps_decay, self.epsilon_min) \n",
    "                    torch.save(self.dnnetwork.state_dict(), \"Glioblastoma\" + \".dat\")\n",
    "                    \n",
    "        # PLOTTING\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(10, 8))  # Create a 2x2 grid of subplots\n",
    "        axes = axes.ravel()  # Flatten the axes array for easier indexing\n",
    "\n",
    "        # Plot episode rewards\n",
    "        axes[0].plot(self.episode_rewards)\n",
    "        axes[0].set_xlabel('Episode')\n",
    "        axes[0].set_ylabel('Episode Reward')\n",
    "        axes[0].set_title('Episode Rewards Over Time')\n",
    "\n",
    "        # Plot mean rewards\n",
    "        axes[1].plot(self.mean_rewards)\n",
    "        axes[1].set_xlabel('Episode')\n",
    "        axes[1].set_ylabel('Mean Reward')\n",
    "        axes[1].set_title('Mean Rewards Over Time')\n",
    "\n",
    "        # Plot epsilon values\n",
    "        axes[2].plot(self.epsilon_values)\n",
    "        axes[2].set_xlabel('Episode')\n",
    "        axes[2].set_ylabel('Epsilon Values')\n",
    "        axes[2].set_title('Epsilon Values Over Time')\n",
    "\n",
    "        # Plot loss values\n",
    "        axes[3].plot(self.loss_values)\n",
    "        axes[3].set_xlabel('Episode')\n",
    "        axes[3].set_ylabel('Loss')\n",
    "        axes[3].set_title('Loss Over Time')\n",
    "\n",
    "        # Adjust layout (optional)\n",
    "        # fig.suptitle('Training Performance', fontsize=16)  # Add a main title\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust spacing between subplots and title\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # Loss calculation           \n",
    "    def calculate_loss(self, batch):\n",
    "        # Separate the variables of the experience and convert them to tensors\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "        device = self.dnnetwork.device\n",
    "\n",
    "        # Add channel dimension # FALTAA\n",
    "        states = torch.FloatTensor(states).unsqueeze(1).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).unsqueeze(1).to(device)\n",
    "\n",
    "        rewards_vals = torch.FloatTensor(rewards).to(device=device) \n",
    "        actions_vals = torch.LongTensor(np.array(actions)).reshape(-1,1).to(device=device)\n",
    "        dones_t = torch.BoolTensor(dones).to(device=device)\n",
    "        \n",
    "        # Obtain the Q values of the main network\n",
    "        qvals = torch.gather(self.dnnetwork.get_qvals(states), 1, actions_vals)\n",
    "        \n",
    "        # Obtain the target Q values.\n",
    "        # The detach() parameter prevents these values from updating the target network\n",
    "        qvals_next_all = self.target_network.get_qvals(next_states)  # Shape: [batch_size, n_actions]\n",
    "        qvals_next = torch.max(qvals_next_all, dim=1)[0].detach()    # Shape: [batch_size]\n",
    "\n",
    "        # 0 in terminal states\n",
    "        qvals_next[dones_t] = 0.0 \n",
    "        \n",
    "        print(\"qvals_next.shape\", qvals_next.shape, \"dones_t.shape\", dones_t.shape) # debugging\n",
    "        \n",
    "        # Calculate the Bellman equation\n",
    "        expected_qvals = (self.gamma * qvals_next) + rewards_vals\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = torch.nn.MSELoss()(qvals, expected_qvals.reshape(-1,1))\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def update(self):\n",
    "        if len(self.buffer.buffer) < self.batch_size:\n",
    "            return  # skip until we have enough experiences\n",
    "        \n",
    "        # Remove any gradient\n",
    "        self.dnnetwork.optimizer.zero_grad()  \n",
    "        # Select a subset from the buffer\n",
    "        batch = self.buffer.sample_batch(batch_size=self.batch_size) \n",
    "        \n",
    "        # \"\"\"Debugging block\"\"\"\n",
    "        # states, actions, rewards, dones, next_states = batch\n",
    "        # print(\"BATCH shapes\", states.shape, next_states.shape, actions.shape, rewards.shape, dones.shape)\n",
    "        # print(\"rewards stats: min,mean,max\", rewards.min(), rewards.mean(), rewards.max())\n",
    "        # # show a tiny sample\n",
    "        # print(\"sample rewards\", rewards[:10])\n",
    "        # print(\"state min/max example:\", states[0].min(), states[0].max())\n",
    "        # \"\"\"End debugging block\"\"\"\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = self.calculate_loss(batch) \n",
    "        # Difference to get the gradients\n",
    "        loss.backward() \n",
    "        \n",
    "        # add gradient clipping (FALTAAA)\n",
    "        torch.nn.utils.clip_grad_norm_(self.dnnetwork.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Apply the gradients to the neural network\n",
    "        self.dnnetwork.optimizer.step() \n",
    "        # Save loss values\n",
    "        if self.dnnetwork.device == 'cuda':\n",
    "            self.update_loss.append(loss.detach().cpu().numpy())\n",
    "        else:\n",
    "            self.update_loss.append(loss.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89a78476",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        self.buffer.append((np.array(state, dtype=np.float32), int(action), float(reward), bool(done), np.array(next_state, dtype=np.float32)))\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, dones, next_states = zip(*batch)\n",
    "        return (\n",
    "            np.array(states, dtype=np.float32),\n",
    "            np.array(actions, dtype=np.int64),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.bool_),\n",
    "            np.array(next_states, dtype=np.float32),\n",
    "        )\n",
    "\n",
    "    def burn_in_capacity(self):\n",
    "        return len(self.buffer) / self.capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89efd4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at 0.7, decaying 0.007776666666666666, will reach 0.0001 after 100 episodes\n"
     ]
    }
   ],
   "source": [
    "LR = 1e-4 #From paper\n",
    "MEMORY_SIZE = 15000 #From paper\n",
    "MAX_EPISODES = 100 #From paper\n",
    "\n",
    "\n",
    "EPSILON = 0.7 #From paper\n",
    "EPSILON_MIN = 1e-4 #From paper\n",
    "EPSILON_DECAY = (EPSILON - EPSILON_MIN) / 90 # EPSILON_DECAY = 1e-4 #From paper\n",
    "print(f\"Starting at {EPSILON}, decaying {EPSILON_DECAY}, will reach {EPSILON_MIN} after {MAX_EPISODES} episodes\")\n",
    "\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 128 #From paper\n",
    "BURN_IN = 1000\n",
    "DNN_UPD = 1\n",
    "DNN_SYNC = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18b21fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 30 training pairs out of 30 listed in CSV.\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"/home/martina/codi2/4year/tfg/training_set_npy\"\n",
    "csv_path = \"/home/martina/codi2/4year/tfg/training_dataset_slices.csv\"\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Construct image and mask filenames\n",
    "df[\"image_path\"] = df.apply(\n",
    "    lambda row: os.path.join(base_dir, f\"{row['Patient']:03d}_{row['SliceIndex']}.npy\"), axis=1\n",
    ")\n",
    "df[\"mask_path\"] = df.apply(\n",
    "    lambda row: os.path.join(base_dir, f\"{row['Patient']:03d}_{row['SliceIndex']}_mask.npy\"), axis=1\n",
    ")\n",
    "\n",
    "# Sanity check (optional)\n",
    "train_pairs = [\n",
    "    (img, mask)\n",
    "    for img, mask in zip(df[\"image_path\"], df[\"mask_path\"])\n",
    "    if os.path.exists(img) and os.path.exists(mask)\n",
    "]\n",
    "\n",
    "print(f\"✅ Found {len(train_pairs)} training pairs out of {len(df)} listed in CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce75291f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 136.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask unique values (counts): Counter({0.0: 30, 2.0: 29, 4.0: 27, 1.0: 26})\n",
      "Percentiles of %patches/ image that are 'inside': [0.0625 0.0625 0.0625 0.125  0.125  0.1875 0.25   0.25   0.25  ]\n",
      "Mean fraction of patches per image that are inside:  0.13541666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def dataset_patch_stats(train_pairs, grid_size=4, tumor_threshold=0.01):\n",
    "    patch_counts = []\n",
    "    label_counter = Counter()\n",
    "    perc_inside_per_image = []\n",
    "    for img_path, mask_path in tqdm(train_pairs):\n",
    "        mask = np.load(mask_path)\n",
    "        label_counter.update(np.unique(mask).tolist())\n",
    "        H, W = mask.shape\n",
    "        block = H // grid_size\n",
    "        inside_counts = 0\n",
    "        total_patches = grid_size*grid_size\n",
    "        for r in range(grid_size):\n",
    "            for c in range(grid_size):\n",
    "                patch = mask[r*block:(r+1)*block, c*block:(c+1)*block]\n",
    "                tumor_count = np.sum(np.isin(patch, [1,4]))\n",
    "                if (tumor_count / patch.size) >= tumor_threshold:\n",
    "                    inside_counts += 1\n",
    "        perc_inside_per_image.append(inside_counts / total_patches)\n",
    "    print(\"Mask unique values (counts):\", label_counter)\n",
    "    perc_inside_per_image = np.array(perc_inside_per_image)\n",
    "    print(\"Percentiles of %patches/ image that are 'inside':\", np.percentile(perc_inside_per_image, [0,1,5,25,50,75,95,99,100]))\n",
    "    print(\"Mean fraction of patches per image that are inside: \", perc_inside_per_image.mean())\n",
    "    return perc_inside_per_image\n",
    "\n",
    "perc = dataset_patch_stats(train_pairs, grid_size=4, tumor_threshold=0.00001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06deb4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: extracted patch has max value 0 at position: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "socket.send() raised exception.\n"
     ]
    }
   ],
   "source": [
    "net = DQN(Glioblastoma(*train_pairs[0], grid_size=4), learning_rate=LR, device='cpu')\n",
    "buffer = ReplayBuffer(capacity=MEMORY_SIZE)\n",
    "agent = DQNAgent(Glioblastoma(*train_pairs[0], grid_size=4), net, buffer,\n",
    "                 epsilon=EPSILON, eps_decay=EPSILON_DECAY, epsilon_min=EPSILON_MIN,\n",
    "                 batch_size=BATCH_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc753ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>episode reward</td><td>▄▂▅▁▆▂▅▇▃▇▆▄▅▄█▇▄▅▅▇▅▅▆▅▅▅▇▇▇▆▆▇▇▇▇▇▇▇▇▇</td></tr><tr><td>epsilon</td><td>█████▇▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>▁▁▁▁▂▂▂▃▃▃▄▅▅▄▅▆▆▆▄▅▆▆▆▆▆▅▄▅▅▆▅▅▅▅▆▆▅▆▆█</td></tr><tr><td>mean_rewards</td><td>▁▃▃▂▁▁▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇██████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>100</td></tr><tr><td>episode reward</td><td>-10</td></tr><tr><td>epsilon</td><td>0.0001</td></tr><tr><td>loss</td><td>2.66267</td></tr><tr><td>mean_rewards</td><td>-12.52</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sleepless-darkness-28</strong> at: <a href='https://wandb.ai/martinacarrettab/TFG_Glioblastoma/runs/gy13nm8a' target=\"_blank\">https://wandb.ai/martinacarrettab/TFG_Glioblastoma/runs/gy13nm8a</a><br> View project at: <a href='https://wandb.ai/martinacarrettab/TFG_Glioblastoma' target=\"_blank\">https://wandb.ai/martinacarrettab/TFG_Glioblastoma</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251030_232853-gy13nm8a/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/martina/codi2/4year/tfg/wandb/run-20251030_234857-cyaebz5u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/martinacarrettab/TFG_Glioblastoma/runs/cyaebz5u' target=\"_blank\">arcane-poltergeist-29</a></strong> to <a href='https://wandb.ai/martinacarrettab/TFG_Glioblastoma' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/martinacarrettab/TFG_Glioblastoma' target=\"_blank\">https://wandb.ai/martinacarrettab/TFG_Glioblastoma</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/martinacarrettab/TFG_Glioblastoma/runs/cyaebz5u' target=\"_blank\">https://wandb.ai/martinacarrettab/TFG_Glioblastoma/runs/cyaebz5u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/martinacarrettab/TFG_Glioblastoma/runs/cyaebz5u?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x74d781b1e980>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()\n",
    "wandb.Settings(quiet=True)\n",
    "wandb.init(project=\"TFG_Glioblastoma\", config={\n",
    "    \"lr\": LR,\n",
    "    \"MEMORY_SIZE\": MEMORY_SIZE,\n",
    "    \"MAX_EPISODES\": MAX_EPISODES,\n",
    "    \"EPSILON\": EPSILON,\n",
    "    \"EPSILON_DECAY\": EPSILON_DECAY,\n",
    "    \"EPSILON_MIN\": EPSILON_MIN,\n",
    "    \"GAMMA\": GAMMA,\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "    \"BURN_IN\": BURN_IN,\n",
    "    \"DNN_UPD\": DNN_UPD,\n",
    "    \"DNN_SYNC\": DNN_SYNC,\n",
    "    \"cnn\": {\n",
    "        \"filters\": [32, 64, 128],\n",
    "        \"kernel_size\": 3,\n",
    "        \"pool_size\": 2\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16003ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training starts at  2025-10-30 23:49:02.254986\n",
      "Filling replay buffer...\n",
      "Warning: extracted patch has max value 0 at position: [0, 0]\n",
      "Warning: extracted patch has max value 0 at position: [0, 0]\n",
      "Warning: extracted patch has max value 0 at position: [0, 1]\n",
      "Warning: extracted patch has max value 0 at position: [0, 1]\n",
      "Warning: extracted patch has max value 0 at position: [0, 2]\n",
      "Warning: extracted patch has max value 0 at position: [0, 2]\n",
      "Warning: extracted patch has max value 0 at position: [0, 3]\n",
      "Warning: extracted patch has max value 0 at position: [1, 3]\n",
      "Warning: extracted patch has max value 0 at position: [2, 3]\n",
      "Warning: extracted patch has max value 0 at position: [2, 3]\n",
      "Warning: extracted patch has max value 0 at position: [2, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [0, 0]\n",
      "Warning: extracted patch has max value 0 at position: [0, 0]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [0, 0]\n",
      "Warning: extracted patch has max value 0 at position: [0, 0]\n",
      "Warning: extracted patch has max value 0 at position: [1, 0]\n",
      "Warning: extracted patch has max value 0 at position: [2, 0]\n",
      "Warning: extracted patch has max value 0 at position: [2, 0]\n",
      "Warning: extracted patch has max value 0 at position: [2, 0]\n",
      "Warning: extracted patch has max value 0 at position: [3, 0]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [0, 0]\n",
      "Warning: extracted patch has max value 0 at position: [0, 0]\n",
      "Warning: extracted patch has max value 0 at position: [0, 0]\n",
      "Agent moved into tumor at position: [1, 2]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [0, 0]\n",
      "Warning: extracted patch has max value 0 at position: [0, 0]\n",
      "Agent moved into tumor at position: [2, 2]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [0, 0]\n",
      "Warning: extracted patch has max value 0 at position: [0, 0]\n",
      "Warning: extracted patch has max value 0 at position: [0, 1]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [0, 0]\n",
      "Warning: extracted patch has max value 0 at position: [0, 0]\n",
      "Warning: extracted patch has max value 0 at position: [0, 1]\n",
      "Agent moved into tumor at position: [1, 2]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "Warning: extracted patch has max value 0 at position: [0, 0]\n",
      "Training...\n",
      "[Episode 0] Using image: 027_61.npy\n",
      "Warning: extracted patch has max value 0 at position: [0, 0]\n",
      "Warning: extracted patch has max value 0 at position: [0, 0]\n",
      "qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])\n",
      "qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])\n",
      "qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])\n",
      "qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])\n",
      "qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])\n",
      "qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])\n",
      "qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])\n",
      "qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])\n",
      "qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])\n",
      "qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])\n",
      "qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])\n",
      "qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])\n",
      "qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])\n",
      "Warning: extracted patch has max value 0 at position: [3, 3]\n",
      "qvals_next.shape torch.Size([128]) dones_t.shape torch.Size([128])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>>> Training starts at \u001b[39m\u001b[38;5;124m\"\u001b[39m, datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow())\n\u001b[0;32m----> 2\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGAMMA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_EPISODES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdnn_update_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDNN_UPD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdnn_sync_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDNN_SYNC\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>>> Training completed at \u001b[39m\u001b[38;5;124m\"\u001b[39m, datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow())\n",
      "Cell \u001b[0;32mIn[17], line 94\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[0;34m(self, train_pairs, gamma, max_episodes, dnn_update_frequency, dnn_sync_frequency)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Upgrade main network\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_count \u001b[38;5;241m%\u001b[39m dnn_update_frequency \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Synchronize the main network and the target network\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_count \u001b[38;5;241m%\u001b[39m dnn_sync_frequency \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[17], line 231\u001b[0m, in \u001b[0;36mDQNAgent.update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_loss(batch) \n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Difference to get the gradients\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# add gradient clipping (FALTAAA)\u001b[39;00m\n\u001b[1;32m    234\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdnnetwork\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\">>> Training starts at \", datetime.datetime.now())\n",
    "agent.train(\n",
    "    train_pairs=train_pairs,\n",
    "    gamma=GAMMA,\n",
    "    max_episodes=MAX_EPISODES,\n",
    "    dnn_update_frequency=DNN_UPD,\n",
    "    dnn_sync_frequency=DNN_SYNC\n",
    ")\n",
    "print(\">>> Training completed at \", datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61533d8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionResetError",
     "evalue": "Connection lost",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinish\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:4105\u001b[0m, in \u001b[0;36mfinish\u001b[0;34m(exit_code, quiet)\u001b[0m\n\u001b[1;32m   4088\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Finish a run and upload any remaining data.\u001b[39;00m\n\u001b[1;32m   4089\u001b[0m \n\u001b[1;32m   4090\u001b[0m \u001b[38;5;124;03mMarks the completion of a W&B run and ensures all data is synced to the server.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4102\u001b[0m \u001b[38;5;124;03m    quiet: Deprecated. Configure logging verbosity using `wandb.Settings(quiet=...)`.\u001b[39;00m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mrun:\n\u001b[0;32m-> 4105\u001b[0m     \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexit_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:399\u001b[0m, in \u001b[0;36m_log_to_run.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m     run_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attach_id\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wb_logging\u001b[38;5;241m.\u001b[39mlog_to_run(run_id):\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:444\u001b[0m, in \u001b[0;36m_attach.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m         _is_attaching \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:2263\u001b[0m, in \u001b[0;36mRun.finish\u001b[0;34m(self, exit_code, quiet)\u001b[0m\n\u001b[1;32m   2254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m quiet \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2255\u001b[0m     deprecate\u001b[38;5;241m.\u001b[39mdeprecate(\n\u001b[1;32m   2256\u001b[0m         field_name\u001b[38;5;241m=\u001b[39mDeprecated\u001b[38;5;241m.\u001b[39mrun__finish_quiet,\n\u001b[1;32m   2257\u001b[0m         warning_message\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2261\u001b[0m         run\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2262\u001b[0m     )\n\u001b[0;32m-> 2263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:399\u001b[0m, in \u001b[0;36m_log_to_run.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m     run_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attach_id\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wb_logging\u001b[38;5;241m.\u001b[39mlog_to_run(run_id):\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:2276\u001b[0m, in \u001b[0;36mRun._finish\u001b[0;34m(self, exit_code)\u001b[0m\n\u001b[1;32m   2273\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wl\n\u001b[1;32m   2275\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinishing run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_path()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2276\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m telemetry\u001b[38;5;241m.\u001b[39mcontext(run\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tel:\n\u001b[1;32m   2277\u001b[0m     tel\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mfinish \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2279\u001b[0m \u001b[38;5;66;03m# Run hooks that need to happen before the last messages to the\u001b[39;00m\n\u001b[1;32m   2280\u001b[0m \u001b[38;5;66;03m# internal service, like Jupyter hooks.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/lib/telemetry.py:42\u001b[0m, in \u001b[0;36m_TelemetryObject.__exit__\u001b[0;34m(self, exctype, excinst, exctb)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_telemetry_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:750\u001b[0m, in \u001b[0;36mRun._telemetry_callback\u001b[0;34m(self, telem_obj)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_telemetry_obj\u001b[38;5;241m.\u001b[39mMergeFrom(telem_obj)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_telemetry_obj_dirty \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_telemetry_flush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:763\u001b[0m, in \u001b[0;36mRun._telemetry_flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m serialized \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_telemetry_obj_flushed:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 763\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_telemetry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_telemetry_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_telemetry_obj_flushed \u001b[38;5;241m=\u001b[39m serialized\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_telemetry_obj_dirty \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py:59\u001b[0m, in \u001b[0;36mInterfaceShared._publish_telemetry\u001b[0;34m(self, telem)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_publish_telemetry\u001b[39m(\u001b[38;5;28mself\u001b[39m, telem: tpb\u001b[38;5;241m.\u001b[39mTelemetryRecord) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_record(telemetry\u001b[38;5;241m=\u001b[39mtelem)\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py:43\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     41\u001b[0m request \u001b[38;5;241m=\u001b[39m spb\u001b[38;5;241m.\u001b[39mServerRequest()\n\u001b[1;32m     42\u001b[0m request\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_asyncer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/lib/asyncio_manager.py:136\u001b[0m, in \u001b[0;36mAsyncioManager.run\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    133\u001b[0m future \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schedule(fn, daemon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mCancelledError:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RunCancelledError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/lib/asyncio_manager.py:219\u001b[0m, in \u001b[0;36mAsyncioManager._wrap\u001b[0;34m(self, fn, daemon, name)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m (task \u001b[38;5;241m:=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mcurrent_task()):\n\u001b[1;32m    217\u001b[0m         task\u001b[38;5;241m.\u001b[39mset_name(name)\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn()\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m daemon:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/lib/service/service_client.py:38\u001b[0m, in \u001b[0;36mServiceClient.publish\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpublish\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: spb\u001b[38;5;241m.\u001b[39mServerRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send a request without waiting for a response.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_server_request(request)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/lib/service/service_client.py:64\u001b[0m, in \u001b[0;36mServiceClient._send_server_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     61\u001b[0m data \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mSerializeToString()\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writer\u001b[38;5;241m.\u001b[39mwrite(data)\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writer\u001b[38;5;241m.\u001b[39mdrain()\n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/streams.py:371\u001b[0m, in \u001b[0;36mStreamWriter.drain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mis_closing():\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;66;03m# Wait for protocol.connection_lost() call\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;66;03m# Raise connection closing error if any,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;66;03m# in a loop would never call connection_lost(), so it\u001b[39;00m\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;66;03m# would not see an error when the socket is closed.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m sleep(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 371\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol\u001b[38;5;241m.\u001b[39m_drain_helper()\n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/streams.py:167\u001b[0m, in \u001b[0;36mFlowControlMixin._drain_helper\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_drain_helper\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection_lost:\n\u001b[0;32m--> 167\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionResetError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConnection lost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paused:\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mConnectionResetError\u001b[0m: Connection lost"
     ]
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab631f23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
